_target_: agents.VectorQuantizedDDPG
# _convert_: all

mlp_dims: [256, 256]
# mlp_dims: [128, 128]
exploration_noise: 0.2
policy_noise: 0.2
noise_clip: 0.5
learning_rate: 3e-4
batch_size: 512
num_updates: 1000 # 1000 is 1 update per new data
# actor_update_freq: 2  # update actor less frequently than critic
nstep: 1
discount: 0.99
tau: 0.005
device: ${device}
name: "VQ-DDPG"

# vq_learning_rate: 3e-4
vq_learning_rate: 3e-5
vq_batch_size: 256
# vq_batch_size: 512
# vq_num_updates: 20000
vq_num_updates: 2000
# vq_num_updates: 1000
vq_patience: 100
vq_min_delta: 0.0
# levels: [8, 6, 5]  # target size 2^8, actual size 240
levels: [8, 6, 5]  # target size 2^8, actual size 240
# levels: [8, 2, 2]  # target size 2^8, actual size 240
# levels: [8]  # target size 2^8, actual size 240

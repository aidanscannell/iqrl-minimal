_target_: "agents.iQRL"
# Actor/critic stuff
mlp_dims: [512, 512]
# exploration_noise: 1.0
exploration_noise_start: 1.0
exploration_noise_end: 0.1
exploration_noise_num_steps: 50
policy_noise: 0.2
noise_clip: 0.3
learning_rate: 3e-4
batch_size: 256
utd_ratio: 1  # parameter update-to-data ratio
actor_update_freq: 2  # update actor less frequently than critic
nstep: 1
horizon: 5
discount: 0.99
tau: 0.005
device: ${device}
# Reset stuff
reset_type: "full" # "full" or "last-layer"
reset_strategy: "every-x-param-updates"  #  "latent-dist" or "every-x-param-updates"
reset_params_freq: null # reset params after this many param updates
reset_threshold: 0.01  # reset latent (z) when changed by more than this
memory_size: 10000  # mem size for calculating ||z_{mem} - e_{\phi}(x_mem)||
# ENC config
train_strategy: "interleaved"  # "interleaved" or "representation-first"
temporal_consistency: True # if True include dynamic model in enc
reconstruction_loss: False # if True include decoder
reward_loss: False # if True use reward prediction for representation learning
use_cosine_similarity_dynamics: False
use_cosine_similarity_reward: False
use_tar_enc: True # if True use tar enc to get latents for TD3
act_with_tar_enc: False # if True act with the tar enc
enc_mlp_dims: [256]
enc_learning_rate: 1e-4
enc_batch_size: 256
enc_utd_ratio: 1  # enc parameter update-to-data ratio
enc_patience: 100
enc_min_delta: 0.0
latent_dim: 512
project_latent: False
enc_tau: 0.005
enc_normalize: False
simplex_dim: 8
use_fsq: True
fsq_levels: [8, 8]
fsq_idx: 0
compile: False
# enc_reset_params_freq: int = 10000  # reset enc params after X param updates
name: iQRL_n=${agent.nstep}_h=${agent.horizon}_td3-targ=${agent.use_tar_enc}_act-targ=${agent.act_with_tar_enc}_cos=${agent.use_cosine_similarity_dynamics}_${agent.reset_strategy}-${agent.reset_type}_${agent.train_strategy}-${agent.reset_params_freq}_temp-${agent.temporal_consistency}_rec=${agent.reconstruction_loss}_utd=${agent.utd_ratio}_fsq=${agent.use_fsq}_${agent.fsq_levels}_idx=${agent.fsq_idx}_d=${agent.latent_dim}

defaults:
  - train_config
  # - env: cartpole_swingup
  # - agent: td3 # mppi or ddpg
  - override hydra/launcher: slurm # Use slurm (on cluster) for multirun
  - _self_

# env_id: "HalfCheetah-v4"
# env_id: "cartpole"
max_episode_steps: 1000
capture_train_video: False
capture_eval_video: True
action_repeat: 2
# dmc_task: "swingup"
# dmc_task: null

# Experiment config
# exp_name: "base"
buffer_size: 1000000
# learning_starts: 25000
learning_starts: 10000
total_timesteps: 1000000
logging_epoch_freq: 100
eval_every_steps: 2500
# eval_every_steps: 5000
seed: 42
device: "cuda"  # "cpu" or "cuda" etc
debug: False
# torch_deterministic: True

# W&B config
wandb_project_name: "world_models"
use_wandb: True
# use_wandb: False
wandb_group: ${env.env_id}
wandb_tags:
  - ${env.env_id}
monitor_gym: True
run_name: ${env.env_id}__${agent.name}__utd={agent.utd_ratio}__${seed}-${now:%Y-%m-%d_%H-%M-%S}
  # cfg.run_name = f"{cfg.env_id}__{cfg.exp_name}__{cfg.seed}__{int(time.time())}"  # TODO move to cfg

hydra:
  run:
    dir: output/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: true
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO # DEBUG

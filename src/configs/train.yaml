defaults:
#   - world_model: base
  - agent: td3 # mppi or ddpg
#   - env: inverted_pendulum
  # Use slurm on cluster or local?
  # - override hydra/launcher: submitit_local
  # - override hydra/launcher: local
  - override hydra/launcher: slurm
  # - override hydra/launcher: slurm_cpu
  - _self_


# observation_space_shape: ???
# action_space_shape: ???
# action_space_low: ???
# action_space_high: ???

env_id: "HalfCheetah-v4"
learning_starts: 25000
total_timesteps: 1000000

# Agent stuff
exploration_noise: 0.1
policy_noise: 0.2
policy_frequency: 2
noise_clip: 0.5
tau: 0.005  # target smoothing coefficient
gamma: 0.99  # discount factor
# Training config
batch_size: 64
learning_rate: 3e-4
buffer_size: 1000000

# Experiment config
exp_name: "base"
logging_epoch_freq: 100
seed: 42
device: "cpu"  # "cpu" or "gpu" etc
capture_train_video: False
capture_eval_video: True
debug: False
torch_deterministic: True

# W&B config
wandb_project_name: "world_models"
use_wandb: True
# use_wandb: False
wandb_group: ${env_id}
wandb_tags:
  - ${env_id}
monitor_gym: True
run_name: ${env_id}__${exp_name}__${seed}-${now:%Y-%m-%d_%H-%M-%S}
  # cfg.run_name = f"{cfg.env_id}__{cfg.exp_name}__{cfg.seed}__{int(time.time())}"  # TODO move to cfg

hydra:
  run:
    dir: output/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: true
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO # DEBUG

defaults:
  - base_train
#   - world_model: base
  - agent: td3 # mppi or ddpg
  - override hydra/launcher: slurm # Use slurm (on cluster) for multirun
  # - _self_

env_id: "HalfCheetah-v4"
max_episode_steps: 1000
frame_skip: 1
# frame_skip: 5
capture_train_video: False
capture_eval_video: True

# Experiment config
exp_name: "base"
buffer_size: 1000000
learning_starts: 25000
total_timesteps: 1000000
logging_epoch_freq: 100
eval_every_steps: 5000
seed: 42
device: "cpu"  # "cpu" or "gpu" etc
debug: False
torch_deterministic: True

# W&B config
wandb_project_name: "world_models"
use_wandb: True
# use_wandb: False
wandb_group: ${env_id}
wandb_tags:
  - ${env_id}
monitor_gym: True
run_name: ${env_id}__${exp_name}__${seed}-${now:%Y-%m-%d_%H-%M-%S}
  # cfg.run_name = f"{cfg.env_id}__{cfg.exp_name}__{cfg.seed}__{int(time.time())}"  # TODO move to cfg

hydra:
  run:
    dir: output/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: true
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO # DEBUG

agent:
  _target_: stable_baselines3.td3.TD3
  policy: "MlpPolicy"
  # env: ???
  learning_rate: 0.001
  buffer_size: 1000000
  learning_starts: 100
  batch_size: 100
  tau: 0.005
  gamma: 0.99
  # train_freq: (1, 'episode')
  # gradient_steps: -1
  # action_noise: None
  # replay_buffer_class: None
  # replay_buffer_kwargs: None
  # optimize_memory_usage: False
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5
  # stats_window_size: 100
  # tensorboard_log: ???
  # policy_kwargs: None
  verbose: 1
  seed: ${seed}
  device: "auto"
  # _init_setup_model: True



policy_type: "MlpPolicy"
total_timesteps: 25000
# env_id: "CartPole-v1"
env_id: "HalfCheetah-v4"
alg_name: "PPO"
exp_name: "PPO-sb"


seed: 42

# W&B config
wandb_project_name: "world_models"
use_wandb: True
# use_wandb: False
wandb_group: ${env_id}
wandb_tags:
  - ${env_id}
  - ${alg_name}
monitor_gym: True
run_name: ${env_id}__${exp_name}__${seed}-${now:%Y-%m-%d_%H-%M-%S}
  # cfg.run_name = f"{cfg.env_id}__{cfg.exp_name}__{cfg.seed}__{int(time.time())}"  # TODO move to cfg

hydra:
  run:
    dir: output/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  job:
    chdir: true
  sweep:
    dir: ${hydra.run.dir}
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO # DEBUG

%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
% \usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\usepackage[capitalize,nameinlink]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sample-efficient Reinforcement Learning with Implicitly Quantized Representations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUSTOM (our stuff)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newcommand{\our}{\textsc{vq-td3}\xspace}
% \newcommand{\our}{\textsc{iFSQ-RL}\xspace}
\newcommand{\our}{\textsc{iQRL}\xspace}

% Latin
\usepackage{xspace}
\newcommand{\eg}{\textit{e.g.\@}\xspace}
\newcommand{\ie}{\textit{i.e.\@}\xspace}
\newcommand{\cf}{\textit{cf.\@}\xspace}
\newcommand{\etc}{\textit{etc.\@}\xspace}
\newcommand{\etal}{\textit{et~al.\@}\xspace}

% Vectors
\usepackage{bm}
\def\vzero{{\bm{0}}}
\def\vone{{\bm{1}}}
\def\vmu{{\bm{\mu}}}
\def\vtheta{{\bm{\theta}}}
\def\va{{\bm{a}}}
\def\vb{{\bm{b}}}
\def\vc{{\bm{c}}}
\def\vd{{\bm{d}}}
\def\ve{{\bm{e}}}
\def\vf{{\bm{f}}}
\def\vg{{\bm{g}}}
\def\vh{{\bm{h}}}
\def\vi{{\bm{i}}}
\def\vj{{\bm{j}}}
\def\vk{{\bm{k}}}
\def\vl{{\bm{l}}}
\def\vm{{\bm{m}}}
\def\vn{{\bm{n}}}
\def\vo{{\bm{o}}}
\def\vp{{\bm{p}}}
\def\vq{{\bm{q}}}
\def\vr{{\bm{r}}}
\def\vs{{\bm{s}}}
\def\vt{{\bm{t}}}
\def\vu{{\bm{u}}}
\def\vv{{\bm{v}}}
\def\vw{{\bm{w}}}
\def\vx{{\bm{x}}}
\def\vy{{\bm{y}}}
\def\vz{{\bm{z}}}

% Elements of vectors
\def\evalpha{{\alpha}}
\def\evbeta{{\beta}}
\def\evepsilon{{\epsilon}}
\def\evlambda{{\lambda}}
\def\evomega{{\omega}}
\def\evmu{{\mu}}
\def\evpsi{{\psi}}
\def\evsigma{{\sigma}}
\def\evtheta{{\theta}}
\def\eva{{a}}
\def\evb{{b}}
\def\evc{{c}}
\def\evd{{d}}
\def\eve{{e}}
\def\evf{{f}}
\def\evg{{g}}
\def\evh{{h}}
\def\evi{{i}}
\def\evj{{j}}
\def\evk{{k}}
\def\evl{{l}}
\def\evm{{m}}
\def\evn{{n}}
\def\evo{{o}}
\def\evp{{p}}
\def\evq{{q}}
\def\evr{{r}}
\def\evs{{s}}
\def\evt{{t}}
\def\evu{{u}}
\def\evv{{v}}
\def\evw{{w}}
\def\evx{{x}}
\def\evy{{y}}
\def\evz{{z}}

% Matrix
\def\mA{{\bm{A}}}
\def\mB{{\bm{B}}}
\def\mC{{\bm{C}}}
\def\mD{{\bm{D}}}
\def\mE{{\bm{E}}}
\def\mF{{\bm{F}}}
\def\mG{{\bm{G}}}
\def\mH{{\bm{H}}}
\def\mI{{\bm{I}}}
\def\mJ{{\bm{J}}}
\def\mK{{\bm{K}}}
\def\mL{{\bm{L}}}
\def\mM{{\bm{M}}}
\def\mN{{\bm{N}}}
\def\mO{{\bm{O}}}
\def\mP{{\bm{P}}}
\def\mQ{{\bm{Q}}}
\def\mR{{\bm{R}}}
\def\mS{{\bm{S}}}
\def\mT{{\bm{T}}}
\def\mU{{\bm{U}}}
\def\mV{{\bm{V}}}
\def\mW{{\bm{W}}}
\def\mX{{\bm{X}}}
\def\mY{{\bm{Y}}}
\def\mZ{{\bm{Z}}}
\def\mBeta{{\bm{\beta}}}
\def\mPhi{{\bm{\Phi}}}
\def\mLambda{{\bm{\Lambda}}}
\def\mSigma{{\bm{\Sigma}}}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Ls}{\mathcal{L}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\emp}{\tilde{p}}
\newcommand{\lr}{\alpha}
\newcommand{\reg}{\lambda}
\newcommand{\rect}{\mathrm{rectifier}}
\newcommand{\softmax}{\mathrm{softmax}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\softplus}{\zeta}
\newcommand{\KL}{D_{\mathrm{KL}}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\standarderror}{\mathrm{SE}}
\newcommand{\Cov}{\mathrm{Cov}}
% Wolfram Mathworld says $L^2$ is for function spaces and $\ell^2$ is for vectors
% But then they seem to use $L^2$ for vectors throughout the site, and so does
% wikipedia.
\newcommand{\normlzero}{L^0}
\newcommand{\normlone}{L^1}
\newcommand{\normltwo}{L^2}
\newcommand{\normlp}{L^p}
\newcommand{\normmax}{L^\infty}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\usetikzlibrary{shapes.geometric,positioning,calc}

% Arno's cube hack
\newcommand{\cube}{%
\resizebox{1em}{.9em}{
\begin{tikzpicture}[baseline=-1ex,cube/.style={very thick,black},
            grid/.style={very thin,gray},
            axis/.style={->,black,thick}]
 \begin{scope}[every node/.append style={yslant=-0.5},yslant=-0.5]
 [cube/.style={very thick,black},
            axis/.style={->,blue,thick}]
   \shade[right color=gray!10, left color=black!50] (0,0) rectangle +(3,3);
   \node at (0.5,2.5) {};
   \node at (1.5,2.5) {};
   \node at (2.5,2.5) {};
   \node at (0.5,1.5) {};
   \node at (1.5,1.5) {};
   \node at (2.5,1.5) {};
   \node at (0.5,0.5) {};
   \node at (1.5,0.5) {};
   \node at (2.5,0.5) {};
   \draw (0,0) grid (3,3);
 \end{scope}
 
 \begin{scope}[every node/.append style={yslant=0.5},yslant=0.5]
   \shade[right color=gray!70,left color=gray!10] (3,-3) rectangle +(3,3);
   \node at (3.5,-0.5) {};
   \node at (4.5,-0.5) {};
   \node at (5.5,-0.5) {};
   \node at (3.5,-1.5) {};
   \node at (4.5,-1.5) {};
   \node at (5.5,-1.5) {};
   \node at (3.5,-2.5) {};
   \node at (4.5,-2.5) {};
   \node at (5.5,-2.5) {};
   \draw (3,-3) grid (6,0);
 \end{scope}

 \begin{scope}[every node/.append style={
     yslant=0.5,xslant=-1},yslant=0.5,xslant=-1
   ]
   \shade[bottom color=gray!10, top color=black!80] (6,3) rectangle +(-3,-3);
   \node at (3.5,2.5) {};
   \node at (3.5,1.5) {};
   \node at (3.5,0.5) {};
   \node at (4.5,2.5) {};
   \node at (4.5,1.5) {};
   \node at (4.5,0.5) {};
   \node at (5.5,2.5) {};
   \node at (5.5,1.5) {};
   \node at (5.5,0.5) {};
   \draw (3,0) grid (6,3);  
 \end{scope}
\end{tikzpicture}}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BEGIN DOCUMENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\twocolumn[
% \icmltitle{Submission and Formatting Instructions for \\
%            International Conference on Machine Learning (ICML 2024)}
% \icmltitle{Sample-efficient Reinforcement Learning with Latent Representations}
\icmltitle{Sample-efficient Reinforcement Learning \\ with Implicitly Quantized Representations}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Aidan Scannell}{aalto,fcai}
\icmlauthor{Arno Solin}{aalto}
\icmlauthor{Joni Pajarinen}{aalto}
% \icmlauthor{Firstname1 Lastname1}{equal,yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{aalto}{Aalto University, Finland}
\icmlaffiliation{fcai}{Finnish Center for Artificial Intelligence}
% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Aidan Scannell}{aidan.scannell@aalto.fi}
% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
  % Deep reinforcement learning (RL) has shown much promise for solving real-world continuous control problems.
  % Whilst deep reinforcement learning (RL) is a competitive approach for solving real-world continuous control problems, it
  % is limited by its sample-efficiency.
  % Recent works have shown that learning representations can improve sample efficiency in RL, whilst also
  % handling complex/high-dimensional observation spaces.
  Learning representations for reinforcement learning (RL) has shown much promise for continuous control.
  Not only can it handle complex observation spaces but it can also improve sample efficiency.
  A particularly promising approach is to use a self-supervised loss, known as the latent-state consistency loss.
  It requires an encoder which maps observations to latent states and a transition model which predicts
  next latent states.
  It then minimizes the distance between the next state predicted by the transition model and
  the next state predicted by the encoder.
  Typically it is combined with other loss terms such as reward or value prediction.
  This prevents representation collapse at the cost of learning a task-specific representation.
  In this work, we show that a careful consideration of the latent space enables us to {\em (i)} learn
  a task-agnostic representation and {\em (ii)} prevent representation collapse, whilst using only the latent-state
  consistency loss.
  We empirically show that our method prevents representation collapse as it maintains matrix rank.
  Our representation learning approach is {\em (i)} simple, {\em (ii)} can be combined with any model-free RL algorithm
  and, {\em (iii)} obtains state-of-the-art results in continuous control benchmarks from DeepMind Control Suite.
  % Moreover, it kk
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
\end{abstract}

\begin{figure}[t!]
\centering\scriptsize
%\resizebox{\columnwidth}{!}{%
%\begin{tikzpicture}
%
%  % Declare that we have a background layer (advanced)
%  \pgfdeclarelayer{background}
%  \pgfsetlayers{background,main}
%
%  % Helpers for small subscripts
%  \newcommand{\sub}[0]{\scalebox{.8}{\ensuremath t}}
%  \newcommand{\subs}[1]{\scalebox{.8}{\ensuremath t{+}#1}}
%
%  % Lengths (this tunes distance)
%  \newlength{\nodedist}\setlength{\nodedist}{2cm}
%
%  % Styles (this tunes colours etc.)
%  \tikzstyle{mynode}=[fill=blue!10!cyan,rounded corners=1pt,font=\tiny,inner sep=0]
%  \tikzstyle{trap}=[mynode,trapezium,minimum width=1.5em, minimum height=2em,
%                    trapezium left angle=-70, trapezium right angle=-70]
%  \tikzstyle{blob}=[mynode,circle,minimum width=2.4em,minimum height=2.4em]
%  \tikzstyle{polval}=[mynode,minimum width=6em,minimum height=2em,align=center,text=white,fill=black!60,inner sep=2pt]
%  \tikzstyle{arr}=[line width=1pt,black,->]
%  \tikzstyle{darr}=[line width=1pt,black,<->,densely dotted]
%
%  % *** FIRST HALF ***
%
%  % Draw nodes
%  \node[blob] (z0) at (0\nodedist,0) {$\vz_{\sub}$};
%  \node[blob] (z1) at (1\nodedist,0) {$\hat\vz_{\subs{1}}$};
%  \node[blob] (z2) at (2\nodedist,0) {$\hat\vz_{\subs{2}}$};
%
%  \node[trap] (e0) at (0\nodedist,\nodedist) {$e_\phi$};
%  \node[trap] (e1) at (1\nodedist,\nodedist) {$e_{\bar\phi}$};
%  \node[trap] (e2) at (2\nodedist,\nodedist) {$e_{\bar\phi}$};
%
%  \node[blob] (zh1) at ($(z1)!0.5!(e1)$) {$\bar\vz_{\subs{1}}$};
%  \node[blob] (zh2) at ($(z2)!0.5!(e2)$) {$\bar\vz_{\subs{2}}$};
%
%  \coordinate (m0) at ($(z0)!.5!(z1)$);
%  \node[blob] (a0) at ($(m0)!(zh1)!(m0)$) {$\va_{\sub}$};
%  \coordinate (m1) at ($(z1)!.5!(z2)$);
%  \node[blob] (a1) at ($(m1)!(zh2)!(m1)$) {$\va_{\subs{1}}$};
%
%  \node[mynode,above of=e0] (i0) {\includegraphics[width=4.7em]{figs/bg}};
%  \node[mynode,above of=e1] (i1) {\includegraphics[width=4.7em]{figs/bg}};
%  \node[mynode,above of=e2] (i2) {\includegraphics[width=4.7em]{figs/bg}};
%
%  \node[anchor=north] (o0) at (i0.north) {\color{white}$o_{\sub}$};
%  \node[anchor=north] (o1) at (i1.north) {\color{white}$o_{\subs{1}}$};
%  \node[anchor=north] (o2) at (i2.north) {\color{white}$o_{\subs{2}}$};
%
%  \node[above of=i1,align=center] (lab0) {\normalsize\bf 1.\ Learn representation};
%
%  % Draw arrows
%  \draw[arr] (i0) -- (e0);
%  \draw[arr] (i1) -- (e1);
%  \draw[arr] (i2) -- (e2);
%  \draw[arr] (z0) -- (z1);
%  \draw[arr] (z1) -- (z2);
%  \draw[arr] (e0) -- (z0);
%  \draw[arr] (e1) -- (zh1);
%  \draw[arr] (e2) -- (zh2);
%  \draw[arr] (a0) -- (z1);
%  \draw[arr] (a1) -- (z2);
%  \draw[darr] (zh1) -- (z1);
%  \draw[darr] (zh2) -- (z2);
%
%
%  % *** SECOND HALF ***
%
%  \node[blob] (z) at ($(e2) + (1.5\nodedist,0)$) {$\bar\vz_{\sub}$};
%  \node[polval] (policy) at ($(z) + (-.5\nodedist,-.5\nodedist)$) {{\bf Policy} \\ $\pi_\theta(\va_t \mid \bar\vz_t)$};
%  \node[polval] (value) at ($(z) + (.5\nodedist,-.5\nodedist)$) {{\bf Value} \\ $\mathcal{Q}_\psi(\bar\vz_t, \va_t)$};
%
%  \draw[arr] (z) -- (policy);
%  \draw[arr] (z) -- (value);
%
%  \node[align=center] at ($(z)!(lab0)!(z)$) {\normalsize\bf 2.\ Latent actor-critic\vphantom{p}};
%
%  % Draw backgrounds
%  \begin{pgfonlayer}{background}
%
%    \coordinate (mid) at ($(i2.east)!.5!(policy.west)$);
%
%    \draw[draw=none,fill=black!10,rounded corners=5pt] ($(i0.north west) + (-.25cm,1cm)$)
%      rectangle ($(mid)!(z2.north)!(mid) + (-3pt,-1cm)$);
%
%    \draw[draw=none,fill=black!05,rounded corners=5pt] ($(mid)!(i2.north east)!(mid) + (3pt,1cm)$)
%      rectangle ($(value.east)!(z2.north)!(value.east) + (.25cm,-1cm)$);
%
%  \end{pgfonlayer}
%
%\end{tikzpicture}}

\begin{tikzpicture}

  % Declare that we have a background layer (advanced)
  \pgfdeclarelayer{background}
  \pgfsetlayers{background,main}

  % Helpers for small subscripts
  \newcommand{\sub}[0]{\scalebox{.8}{\ensuremath t}}
  \newcommand{\subs}[1]{\scalebox{.8}{\ensuremath t{+}#1}}

  % Lengths (this tunes distance)
  \newlength{\nodedist}\setlength{\nodedist}{3cm}
  \newlength{\nodedistv}\setlength{\nodedistv}{2.5cm}

  % Styles (this tunes colours etc.)
  \tikzstyle{mynode}=[fill=white,draw=black!80,rounded corners=1pt,font=\tiny,inner sep=0,align=center]
  \tikzstyle{trap}=[mynode,trapezium,text width=6.5em, minimum height=2em,
                    trapezium left angle=-70, trapezium right angle=-70,inner sep=2pt]
  \tikzstyle{blob}=[mynode,circle,minimum width=2.4em,minimum height=2.4em]
  \tikzstyle{polval}=[mynode,minimum width=6em,minimum height=2em,align=center,text=white,fill=black!60,inner sep=2pt]
  \tikzstyle{arr}=[line width=1pt,black,->]
  \tikzstyle{darr}=[line width=1pt,black,<->,densely dotted]

  % Draw nodes
  \node[blob] (z0) at (0\nodedist,0) {$\vz_{\sub}$};
  \node[blob] (z1) at (1\nodedist,0) {$\hat\vz_{\subs{1}}$};
  \node[blob] (z2) at (2\nodedist,0) {$\hat\vz_{\subs{2}}$};

  \node[trap] (e0) at (0\nodedist,\nodedistv) {Encoder \\ $e_\phi$};
  \node[trap] (e1) at (1\nodedist,\nodedistv) {Momentum enc.\\$e_{\bar\phi}$};
  \node[trap] (e2) at (2\nodedist,\nodedistv) {Momentum enc.\\$e_{\bar\phi}$};

  \node[blob] (zh1) at ($(z1)!0.5!(e1)$) {$\bar\vz_{\subs{1}}$};
  \node[blob] (zh2) at ($(z2)!0.5!(e2)$) {$\bar\vz_{\subs{2}}$};

  \coordinate (m0) at ($(z0)!.5!(z1)$);
  \node[blob] (a0) at ($(m0)!(zh1)!(m0)$) {$\va_{\sub}$};
  \coordinate (m1) at ($(z1)!.5!(z2)$);
  \node[blob] (a1) at ($(m1)!(zh2)!(m1)$) {$\va_{\subs{1}}$};

  \node[mynode,minimum width=8em,minimum height=8em,outer sep=0,above of=e0,node distance=6em,path picture={\node at (path picture bounding box.center){\includegraphics[height=10em]{figs/humanoid-1}};}] (i0) {};

  \node[mynode,minimum width=8em,minimum height=8em,outer sep=0,above of=e1,node distance=6em,path picture={\node at (path picture bounding box.center){\includegraphics[height=10em]{figs/humanoid-3}};}] (i1) {};

  \node[mynode,minimum width=8em,minimum height=8em,outer sep=0,above of=e2,node distance=6em,path picture={\node at (path picture bounding box.center){\includegraphics[height=10em]{figs/humanoid-3}};}] (i2) {};

  \node[anchor=north] (o0) at (i0.north) {\color{white}$o_{\sub}$};
  \node[anchor=north] (o1) at (i1.north) {\color{white}$o_{\subs{1}}$};
  \node[anchor=north] (o2) at (i2.north) {\color{white}$o_{\subs{2}}$};

  % Draw arrows
  \draw[arr] (i0) -- (e0);
  \draw[arr] (i1) -- (e1);
  \draw[arr] (i2) -- (e2);
  \draw[arr] (z0) -- node[below,font=\tiny] {Dynamics} (z1);
  \draw[arr] (z1) -- node[below,font=\tiny] {Dynamics} (z2);
  \draw[arr] (e0) -- node[above,font=\tiny,rotate=90,yshift=6pt] {Normalization} (z0);
  \draw[arr] (e1) -- (zh1);
  \draw[arr] (e2) -- (zh2);
  \draw[arr] (a0) -- (z1);
  \draw[arr] (a1) -- (z2);
  \draw[darr] (zh1) -- (z1);
  \draw[darr] (zh2) -- (z2);

  % Add normalization
  \node[circle,fill=white,inner sep=0] at ($(e0)!.5!(z0)$) {\cube};
  \node[circle,fill=white,inner sep=0] at ($(e1)!.5!(zh1)$) {\cube};
  \node[circle,fill=white,inner sep=0] at ($(e2)!.5!(zh2)$) {\cube};

  % Minimization
  \node[blue!50,below of=z1,align=center,font=\scriptsize,xshift=4em] (min) {minimize: $\|\bar\vz_{\bullet}-\hat\vz_{\bullet}\|^{2}_{2}$};
  \draw[blue!50,line width=1pt,shorten <=1mm,shorten >=1mm,->] (min) to[bend right=30] ($(z1)!.5!(zh1)$);
  \draw[blue!50,line width=1pt,shorten <=1mm,shorten >=1mm,->] (min) to[bend left=30] ($(z2)!.5!(zh2)$);


\end{tikzpicture}
\caption{\textbf{Overview of \our's representation learning} \our learns a representation soley with the latent-state consistency loss, \ie a self-predictive loss, see \cref{eq:rep-loss}. We follow Finite Scalar Quantization (FSQ) \cite{mentzerFiniteScalarQuantization2023} and bound the domain of our latent representation, \ie we normalize it. Our normalization results in an implicit codebook whose size we can control.
  % Importantly, this prevents representation collapse without needing loss terms to minimize errors in reward and/or value predictions.
  % As such, our representations are task-agnostic.
  We denote our normalization scheme by \cube.}
\label{fig:diagram-self-predictive}
\vskip -0.2in
\end{figure}

\section{Introduction}
\label{intro}

Reinforcement learning (RL) \citep{sutton2018reinforcement} has shown much promise in games, animation and robotics.
However, applying RL in real-world environments is challenging. The problem is two-fold,
{\em (i)}, handling high dimensional/complex observation spaces is non-trivial and {\em (ii)}
deep RL typically requires millions of data points which can be unpractical (or costly), \ie it is very sample inefficient.

% Given such a latent space,
% Whilst these latent states are often lower dimensional this is not always
Many approaches have been proposed to improve sample efficiency in deep RL.
As \citet{laskinCURLContrastiveUnsupervised2020} highlighted, these can generally be divided into two streams of research:
{\em (i)} auxillary tasks on the agent's observations and {\em (ii)} world models that predict the future
\citep{haRecurrentWorldModels2018,hafnerLearning2019,hansenTemporalDifferenceLearning2022}.
In this work, we are interested in the first class of methods, which use auxillary tasks to improve sample efficiency.
In particular, we focus on representation learning methods which compress observations into latent states.
Nevertheless, our representation learning borrows ideas from world models as it uses multi-step predictions in a
latent-space dynamics model to learn temporally consist representations \citep{zhaoSimplifiedTemporalConsistency2023}.
% In particular, methods for compressing observations into latent states.%, aka representation learning.

Learning representations for RL has been investigated for decades
\citep{abelOptimalBehaviorApproximate2016,mannorDynamicAbstractionReinforcement2004,liUnifiedTheoryState2006,andreStateAbstractionProgrammable2002,deardenAbstractionApproximateDecisiontheoretic1997,singhReinforcementLearningSoft1994,higginsDefinitionDisentangledRepresentations2018,vanhoofStableReinforcementLearning2016,watterEmbedControlLocally2015,ghoshRepresentationsStableOffPolicy2020}.
However, these approaches are usually limited to simple environments.
Some approaches used unsupervised learning (\eg VAE \citep{kingmaAutoEncoding2014} with reconstruction loss) to acquire representations
\citep{finnDeepSpatialAutoencoders2016,higginsDARLAImprovingZeroShot2017,langeAutonomousReinforcementLearning2012,watterEmbedControlLocally2015}.
More recently, self-supervised learning (SSL) approaches (which do not reconstruct observations)
attempt to learn good features without labels \cite{anandUnsupervisedStateRepresentation2019}.
Alternative approaches leverage contrastive losses to learn representations \cite{laskinCURLContrastiveUnsupervised2020}.

To the best of our knowledge, all prior works that learn representations with objectives that depend on
predicting the reward and/or value in the latent space
\citep{zhangLearningInvariantRepresentations2020,zhaoSimplifiedTemporalConsistency2023,hansenTemporalDifferenceLearning2022,geladaDeepMDPLearningContinuous2019,rezaei-shoshtariContinuousMDPHomomorphisms2022}.
This is usually motivated by preventing representation collapse.
% use a latent-state consistency loss based upon the Cosine
% For example, \citet{zhaoSimplifiedTemporalConsistency2023} use a latent-state consistency loss based upon the Cosine
% similarity loss plus the MSE of reward predictions in the latent space.
% The cosine similarity loss provides training stability whilst the reward prediction prevents representation collapse.
However, learning representations using reward and/or value loss terms leads to the representations being task specific.


In this paper, we propose a simple representation learning technique that can be combined with any model-free RL method
(we use TD3 \citep{fujimotoAddressingFunctionApproximation2018}).
See \cref{fig:diagram-self-predictive} for an overview of our (extremely simple) representation learning method.
It is based soley on the latent-state consistency loss, \ie a commonly use self-supervised loss for RL.
In contrast to prior works, we normalize our latent representation using the technique from Finite Scalar Quantization
\citep{mentzerFiniteScalarQuantization2023}.
As a result, our latent space is bounded and associated with an \textit{implicit} codebook, whose size we can control.
% As a result, our latent space is associated with an \textit{implicit} codebook, whose size we can control.
Importantly, our method {\em (i)} prevents representation collapse (as it learns a high-rank representation),
{\em (ii)} demonstrates state-of-the-art sample efficiency in continuous control benchmarks,
{\em (iii)} is simple to implement, and {\em (iii)} learns a task-agnostic
representation which could be helpful in downstream tasks.


\section{Method}
\label{sec:method}

In this section, we detail our method, titled \textit{implicitly Quantized Reinforcement Learning} (\our).
\our is conceptually simple, it (1) learns a representation of the observation space and then,
(2) performs model-free RL (e.g. TD3) on this representation.
% It the first representation learning phase, it learns an encoder which maps observations $o \in \mathcal{O}$
% to a latent space $z \in \mathcal{Z}$.
% It first learns a representation of the observation space and then uses
See \cref{fig:diagram-self-predictive} and \cref{alg:main_alg}.

We consider Markov Decision Processes (MDPs, \citet{bellmanMarkovianDecisionProcess1957a}) $\mathcal{M} = (\mathcal{O}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$,
where an agent receives an observation $o_{t} \in \mathcal{O}$ at time step $t$ and performs an action $a_{t} \in \mathcal{A}$.
The agent then obtains a reward $r_{t} = \mathcal{R} (o_{t}, a_{t})$ and obtains the next observation
$o_{t+1} = P(\cdot \mid o_{t}, a_{t})$.
The discount factor is denoted $\gamma \in [0, 1]$.

\begin{algorithm}[tb]
   \caption{\our}
   \label{alg:main_alg}
   \renewcommand{\algorithmiccomment}[1]{\hfill\textcolor{gray}{\(\triangleright\) #1}}
\begin{algorithmic}
   \STATE {\bfseries Input:} Encoder $e_{\theta}$, transition $d_{\phi}$, critics $\{Q_{\psi_{1}}, Q_{\psi_{2}} \}$, policy $\pi_{\eta}$, learning rate $\alpha$, target network update rate $\tau$
   \FOR{$i$ {\bfseries to} $N_{\text{episodes}}$}
    \STATE Collect data in environment
    \STATE $\qquad \mathcal{D} \leftarrow \mathcal{D} \cup \{o_{t}, a_{t}, o_{t+1}, r_{t+1}\}^{T}_{t=0}$
    \FOR{$i=1$ {\bfseries to} $T$}
        \STATE Update representation
        \STATE $\qquad [\theta, \phi] \leftarrow [\theta, \phi] + \alpha \nabla \left( \mathcal{L}_{\text{rep}}(\theta, \phi; \mathcal{D}) \right)$  \COMMENT{\cref{eq:rep-loss}}
        \STATE Update critic
        \STATE \quad \quad $\psi \leftarrow \psi + \alpha \nabla \left( \mathcal{L}_{Q}(\psi; \mathcal{D}) \right)$ \COMMENT{\cref{eq:value-loss}}
        \IF{i \% 2 == 0}
          \STATE Update actor less frequently than critic
          \STATE $\quad \quad \eta \leftarrow \psi + \alpha \nabla \left( \mathcal{L}_{\pi}(\psi; \mathcal{D}) \right)$  \COMMENT{\cref{eq:policy-loss}}
          % \STATE $\bar{\eta} \leftarrow \tau \bar{\eta} + (1-\tau) \eta$
        \ENDIF
        \STATE Update target networks
        \STATE $\quad \quad [\bar{\theta}, \bar{\phi}, \bar{\psi}] \leftarrow \tau [\bar{\theta}, \bar{\phi}, \bar{\psi}] + (1-\tau) [{\theta}, {\phi}, {\psi}]$
    \ENDFOR
   \ENDFOR
\end{algorithmic}
\end{algorithm}

\our has four parameterized components.
\begin{align}
&\text{Encoder: } & z_{t} &= e_{\theta} (s_{t}) \label{eq:encoder} \\
&\text{Dynamics: } & \hat{z}_{t+1} &= d_{\phi} (z_{t}, a_{t}) \label{eq:transition} \\
&\text{Value: } & q_{t} &= Q_{\psi} (z_{t}, a_{t}) \label{eq:value} \\
&\text{Policy: } & a_{t} &\sim \pi_{\eta} (z_{t}) . \label{eq:policy}
\end{align}
Central to our method is the latent state $z_{t}$

\textbf{Bounding the latent space}
Typically the latent state $z_{t} \in \R^{d}$
However, in our work we bound the domain of the latent space, \ie $z_{t} \in [-L, L]^{d}$.

\textbf{Representation learning}
Our representation learning is based soley on the temporal consistency loss,
However, unlike previous works, we normalize the latent vector
Our representation learning consists of an encoder $e_{\theta}$ (which maps observations to
latent states) and a transition model $d_{\phi}$ which maps
Our representation learning adopts a self-supervised loss.
%
\begin{align} \label{eq:rep-loss}
  \mathcal{L}_{\text{rep}}(\theta, \phi; \tau)
 % &= \E_{s_{t} \sim \mathcal{D}} \left[ \sum_{h=0}^{H-1} \| \hat{z}_{t+1} - z_{t+1} \|_{2}^{2} \right] \\
%&= \E_{(s_{t}, a_{t}, s_{t+1}) \sim \mathcal{D}}
% &= \E_{\tau \sim \mathcal{D}}
% \left[ \sum_{h=0}^{H-1} \gamma^{h} \| d_{\phi}(e_{\theta}(s_{t}), a_{t}) - e_{\bar{\theta}}(s_{t+1}) \|_{2}^{2} \right]
&= \sum_{h=0}^{H-1} \gamma^{h} \| d_{\phi}(e_{\theta}(s_{t}), a_{t}) - e_{\bar{\theta}}(s_{t+1}) \|_{2}^{2}
\end{align}
%
\textbf{Finite Scalar Quantization}
We follow Finite Scalar Quantization (FSQ, \cite{mentzerFiniteScalarQuantization2023})
and bound each dimension of our latent space.
Given the output of our MLP encoder $z \in \R^{d}$,

where each entry in  $f : z \rightarrow \left[ L/2 \right] \mathrm{tanh}(z))$

% Thereby, we have zˆ ∈ C, where C is the implied codebook, given by the product of these per-channel
% codebook sets, with |C| = L
% d
% . The vectors in C can simply be enumerated leading to a bijection from
% any zˆ to an integer in {1, . . . , Ld}. Therefore, VQ can be replaced with FSQ in any neural networkrelated setup where VQ is commonly used, e.g., to train transformers, after appropriately adapting
% the output and input dimension of the layers before and after VQ, respectively. We generalize the
% above exposition to the case where the i-th channel is mapped to Li values and get |C| =
% Qd
% i=1 Li

\begin{table}[t]
\caption{FSQ levels $\mathcal{L}$ to approximate different codebook sizes $|\mathcal{C}|$}
\label{sample-table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Target size $|\mathcal{C}|$ & $2^{8}$ & $2^{10}$ & $2^{12}$ \\
\midrule
Proposed $\mathcal{L}$ & $[8,6,5]$ & $[8,5,5,5]$ & $[7,5,5,5,5]$ \\
% \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}



\textbf{Policy and value function learning}

\begin{align} \label{eq:value-loss}
  \mathcal{L}_{Q}(\psi ; \mathcal{D}) &= \E_{\tau \sim \mathcal{D}} \left[ (Q_{\psi_{k}}(e_{\theta}(s_{t}), a_{t}) - y)^{2}  \right], \quad \forall k \in 1, 2 \\
  y &= \sum_{n=0}^{N-1} \gamma^{n} r_{t+n} + \gamma^{n} \min_{k \in \{1,2\}} Q_{\bar{\psi}_{k}}(e_{\bar{\theta}}(s_{t+n}), a_{t+n})
\end{align}

\begin{align} \label{eq:policy-loss}
 \mathcal{L}_{\pi}(\eta; \mathcal{D}) = \E_{(o_{t}, a_{t}) \sim \mathcal{D}} \left[ \min_{k\in\{1,2\}} Q_{\psi_{k}}(e_{\theta}(o_{t}), a_{t}) \right]
\end{align}


Whilst our method resembles TCRL \citep{zhaoSimplifiedTemporalConsistency2023} it is important to note that
our transition model does not predict the reward.
As such, our method learns a task-agnostic representation.


\section{Experiments}
\label{sec:experiments}
In this section, we evaluate \our in a wide variety of continuous control tasks from the DeepMind Control (DMC) suite.
We aim to answer the following questions:
\begin{itemize}
    \item How does \our compare to state-of-the-art model-free and model-based algorithms?
    \item Is learning a representation with only latent-state consistency really better than including reward predictions and value predictions?
    \item Can \our's task-agnostic representation aid task-adaptation with its pre-trained representation?
\end{itemize}

% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1.0\textwidth]{./figs/utd_comparison.pdf}}
% \caption{\textbf{Normalizing the latent-space enables higher UTD} \our's (blue) sample efficiency improves when the UTD ratio is increased from $1 \rightarrow 8$. TCRL (which has an un-normalized latent space) does not see an improvement in sample efficiency when when the UTD ratio is increased. Results are for four DMC tasks. We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
% \label{fig:normalization_enables_higher_utd}
% \end{center}
% \vskip -0.2in
% \end{figure*}

% \textbf{Normalizing the latent-space enables higher UTD}
% We first show how using bounded latent states enables us to increase the UTD ratio to see improvements in sample efficiency.
% \cref{fig:normalization_enables_higher_utd} shows that as the UTD ratio is increased from $1$ to $8$ \our's sample efficiency
% continues to improve.
% In contrast, when increasing the UTD ratio for TCRL no improvements are observed.




\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{./figs/baselines_comparison.pdf}}
\caption{\textbf{Normalizing the latent-space improves sample efficiency} \our is significanly more sample efficient than the model-free baselines SAC (blue) and REDQ (purple). It performs similarly to TD-MPC (orange) whilst being significantly simpler and quicker to train. \our even outperforms TCRL, which is the most similar baseline. Results are for eight DMC tasks with UTD=1. We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
\label{fig:normalization_improves_sample_efficiency}
\end{center}
\vskip -0.2in
\end{figure*}

\textbf{Normalizing the latent-space improves sample efficiency}
In \cref{fig:normalization_improves_sample_efficiency},



\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{./figs/normalization-ablation.pdf}}
\caption{\textbf{Ablation of normalization}. \our XXXXXX We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
\label{fig:normalization-ablation}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{./figs/reward-ablation.pdf}}
\caption{\textbf{Reward prediction is not necessary for representation learning}. \our XXXXXX We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
\label{fig:reward-ablation}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{./figs/task-agnostic-ablation.pdf}}
\caption{\textbf{\our's task-agnostic representation helps for multi-task RL}. \our XXXXXX We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
\label{fig:multi-task-pretraining}
\end{center}
\vskip -0.2in
\end{figure*}


\textbf{\our's representation is helpful for multi-task RL}
In contrast to TCRL and TD-MPC, \our learns a representation that is task-agnostic.
As such, \our's task-agnostic representations can be used to speed-up learning in the incremental multi-task setting.
That is, leverage information learned from solving one task to aid learning another task in the same environment.
For example, pre-training our representation in walker-walk and then using this to speed up training in walker-walk.
In \cref{fig:multi-task-pretraining} we show that \our benefits from pre-training the representation in walker-walk
before training on walker-run.
In contrast, TCRL does not see this improvement.
This is because its representation loss contains a reward prediction term which makes its representation task-specific.
These results demonstrate that confirms task-agnostic representation


% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1.0\textwidth]{./figs/fsq_levels_comparison.pdf}}
% \caption{\textbf{Comparison of different FSQ levels $\mathcal{L}$}. Results are for four DMC tasks. We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
% \label{fig:fsq_levels_comparison}
% \end{center}
% \vskip -0.2in
% \end{figure*}


% \begin{itemize}
%     \item Show normalised latent leads to higher sample efficiency. Show results for NTC-TD3/TC-TD3/TD3/SAC/TCRL/TD-MPC with UTD=1.
%     \item Show normalised latent works with higher UTD. Compare NTC-TD3/TC-TD3/TCRL.
%     \item Show task-agnostic representation is good for incremental task learning. Compare NTC-TD3/TCRL-ours/TCRL
% \end{itemize}

\subsection{Ablations}

\textbf{Reward predict is not necessary for representation learning}
Next, we test if using reward prediction aids representation learning.
\cref{fig:reward-ablation} shows that

\textbf{Reconstruction loss has detrimental impact}
Recent work \citep{zhaoSimplifiedTemporalConsistency2023,hansenTemporalDifferenceLearning2022}
has shown that incorporating a reconstruction term into the representation loss can have a negative impact on performance.
To provide a thorough analysis of \our, we include results where we add a reconstruction term to our
representation loss \cref{eq:rep-loss}.
We perform reconstruction at each time step in the horizon.
\cref{fig:multi-task-pretraining} shows that in no environments does reconstruction aid learning.
Moreover, in the quadruped-run and dog-walk environments incorporating a reconstruction term has a detrimental impact on
overall performance.


% \begin{itemize}
%   \item Show un-normalised isn't as good.
%   \item Compare different size latent spaces? Different levels $[8,5]$ vs $[8,6,5]$ vs $[8,5,5,5]$ vs $[7,5,5,5,5]$
%   \item Target encoder vs online encoder for mapping states to latents for TD3.
%   \item Show cosine similarity slows training.
%   \item Show adding reconstruction loss hurts performance.
% \end{itemize}







\section{Conclusion}
\label{conclusion}

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{zotero-library}
\bibliographystyle{icml2024}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Hyperparameters}
\cref{tab:hyperparameters} lists all of the hyperparameters for training \our.
For further details please check the code which is available at XXXXXXX.

\begin{table}[t]
\caption{Hyperparameters for \our.}
\label{tab:hyperparameters}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lll}
\toprule
Hyperparameter & Value & Description \\
\midrule
\textbf{Training} & & \\
Max episode length & 1000 &  Max episode length \\
Num episodes & $500$ (easy) & Number of training episodes \\
             & $1000$ (medium) & \\
             & $15000$ (hard) & \\
Random episodes & $10$ & Number of random episodes at start \\
Action repeat & 2 & \\
Eval every episodes & $5$ & \\
Num eval episodes & $10$ & \\
\hline
\textbf{TD3} & & \\
Actor update freq & $2$  & update actor less frequently than critic \\
Batch size & $256$ & \\
Buffer size & $10^{6}$ & \\
Exploration noise & $\mathrm{Linear}(1.0,0.1,50)$ (easy) & \\
                  & $\mathrm{Linear}(1.0,0.1,150)$ (medium) & \\
                  & $\mathrm{Linear}(1.0,0.1,500)$ (hard) & \\
Learning rate & $3 \times 10^{-4}$ & \\
MLP dims & $[512, 512]$ & \\
Momentum coefficient ($\tau$) & $0.005$ & \\
Noise clip & $0.3$ & \\
N-step TD & $1$ & \\
Policy noise & $0.2$ & \\
UTD ratio & $1$ & \\
\hline
\textbf{Encoder} &  & \\
Discount $\gamma$ & $0.99$ & \\
Encoder learning rate & $10^{-4}$ & \\
Encoder MLP dims & $[256]$ & \\
Encoder momentum coefficient ($\tau$) & 0.005 & \\
FSQ levels & $[8, 5, 5, 5]$ & \\
Horizon $(H)$ & $5$ & Horizon for representation learning \\
Latent dimension ($d$) & $1024$ & \\
% \bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\section{Full results}


\begin{figure*}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1.0\textwidth]{./figs/reconstruction-loss-ablation.pdf}}
\caption{\textbf{Reconstruction loss has detrimental impact}. \our XXXXXX We plot the mean (solid line) and the $95\%$ confidence intervals (shaded) across 5 random seeds, where each seed averages over 10 evaluation episodes.}
\label{fig:multi-task-pretraining}
\end{center}
\vskip -0.2in
\end{figure*}

% \begin{figure*}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.97\textwidth]{./figs/all_mujoco_envs.pdf}}
% \caption{Comparison to SAC/TCRL with UTD=1.}
% \label{fig:main_plot}
% \end{center}
% \vskip -0.2in
% \end{figure*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.

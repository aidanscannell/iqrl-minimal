@article{2020NumPy-Array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {Fern{\'a}ndez del R{\'i}o}, Jaime and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  journal = {Nature},
  volume = {585},
  pages = {357--362},
  doi = {10.1038/s41586-020-2649-2}
}

@article{2020SciPy-NMeth,
  title = {{{SciPy}} 1.0: {{Fundamental}} Algorithms for Scientific Computing in Python},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and Haberland, Matt and Reddy, Tyler and Cournapeau, David and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and Bright, Jonathan and {van der Walt}, St{\'e}fan J. and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and Kern, Robert and Larson, Eric and Carey, C J and Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and VanderPlas, Jake and Laxalde, Denis and Perktold, Josef and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and Harris, Charles R. and Archibald, Anne M. and Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  year = {2020},
  journal = {Nature Methods},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  adsurl = {https://rdcu.be/b08Wh}
}

@inproceedings{abelOptimalBehaviorApproximate2016,
  title = {Near {{Optimal Behavior}} via {{Approximate State Abstraction}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Abel, David and Hershkowitz, David and Littman, Michael},
  year = {2016},
  month = jun,
  pages = {2915--2923},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-11-02},
  abstract = {The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Abel et al-2016 Near Optimal Behavior via Approximate State Abstraction/Abel et al_2016_Near Optimal Behavior via Approximate State Abstraction.pdf}
}

@book{abrahamManifolds1988,
  title = {Manifolds, {{Tensor Analysis}}, and {{Applications}}},
  author = {Abraham, Ralph and Marsden, J. E. and Ratiu, Tudor},
  year = {1988},
  series = {Applied {{Mathematical Sciences}}},
  edition = {2},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4612-1029-0},
  urldate = {2021-06-29},
  abstract = {The purpose of this book is to provide core material in nonlinear analysis for mathematicians, physicists, engineers, and mathematical biologists. The main goal is to provide a working knowledge of manifolds, dynamical systems, tensors, and differential forms. Some applications to Hamiltonian mechanics, fluid me\- chanics, electromagnetism, plasma dynamics and control thcory arc given in Chapter 8, using both invariant and index notation. The current edition of the book does not deal with Riemannian geometry in much detail, and it does not treat Lie groups, principal bundles, or Morse theory. Some of this is planned for a subsequent edition. Meanwhile, the authors will make available to interested readers supplementary chapters on Lie Groups and Differential Topology and invite comments on the book's contents and development. Throughout the text supplementary topics are given, marked with the symbols {\textasciitilde} and \{l:;J. This device enables the reader to skip various topics without disturbing the main flow of the text. Some of these provide additional background material intended for completeness, to minimize the necessity of consulting too many outside references. We treat finite and infinite-dimensional manifolds simultaneously. This is partly for efficiency of exposition. Without advanced applications, using manifolds of mappings, the study of infinite-dimensional manifolds can be hard to motivate.\vphantom\}},
  isbn = {978-0-387-96790-5},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PAPUN7FY/9780387967905.html}
}

@misc{abu-husseinUDPMUpsamplingDiffusion2023,
  title = {{{UDPM}}: {{Upsampling Diffusion Probabilistic Models}}},
  shorttitle = {{{UDPM}}},
  author = {{Abu-Hussein}, Shady and Giryes, Raja},
  year = {2023},
  month = may,
  number = {arXiv:2305.16269},
  eprint = {2305.16269},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.16269},
  urldate = {2023-11-27},
  abstract = {In recent years, Denoising Diffusion Probabilistic Models (DDPM) have caught significant attention. By composing a Markovian process that starts in the data domain and then gradually adds noise until reaching pure white noise, they achieve superior performance in learning data distributions. Yet, these models require a large number of diffusion steps to produce aesthetically pleasing samples, which is inefficient. In addition, unlike common generative adversarial networks, the latent space of diffusion models is not interpretable. In this work, we propose to generalize the denoising diffusion process into an Upsampling Diffusion Probabilistic Model (UDPM), in which we reduce the latent variable dimension in addition to the traditional noise level addition. As a result, we are able to sample images of size \$256{\textbackslash}times 256\$ with only 7 diffusion steps, which is less than two orders of magnitude compared to standard DDPMs. We formally develop the Markovian diffusion processes of the UDPM, and demonstrate its generation capabilities on the popular FFHQ, LSUN horses, ImageNet, and AFHQv2 datasets. Another favorable property of UDPM is that it is very easy to interpolate its latent space, which is not the case with standard diffusion models. Our code is available online {\textbackslash}url\{https://github.com/shadyabh/UDPM\}},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Abu-Hussein_Giryes-2023 UDPM/Abu-Hussein_Giryes_2023_UDPM.pdf;/Users/scannea1/Zotero/storage/P5FAK2EL/2305.html}
}

@inproceedings{achituveGPTreeGaussianProcess2021,
  title = {{{GP-Tree}}: {{A Gaussian Process Classifier}} for {{Few-Shot Incremental Learning}}},
  shorttitle = {{{GP-Tree}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Achituve, Idan and Navon, Aviv and Yemini, Yochai and Chechik, Gal and Fetaya, Ethan},
  year = {2021},
  month = jul,
  pages = {54--65},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-13},
  abstract = {Gaussian processes (GPs) are non-parametric, flexible, models that work well in many tasks. Combining GPs with deep learning methods via deep kernel learning (DKL) is especially compelling due to the strong representational power induced by the network. However, inference in GPs, whether with or without DKL, can be computationally challenging on large datasets. Here, we propose GP-Tree, a novel method for multi-class classification with Gaussian processes and DKL. We develop a tree-based hierarchical model in which each internal node of the tree fits a GP to the data using the P\{{\'o}\}lya-Gamma augmentation scheme. As a result, our method scales well with both the number of classes and data size. We demonstrate the effectiveness of our method against other Gaussian process training baselines, and we show how our general GP approach achieves improved accuracy on standard incremental few-shot learning benchmarks.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Achituve et al-2021 GP-Tree/Achituve et al_2021_GP-Tree.pdf}
}

@inproceedings{adamDualParameterizationSparse2021,
  title = {Dual {{Parameterization}} of {{Sparse Variational Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {ADAM, Vincent and Chang, Paul and Khan, Mohammad Emtiyaz E and Solin, Arno},
  year = {2021},
  volume = {34},
  pages = {11474--11486},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-09},
  abstract = {Sparse variational Gaussian process (SVGP) methods are a common choice for non-conjugate Gaussian process inference because of their computational benefits. In this paper, we improve their computational efficiency by using a dual parameterization where each data example is assigned dual parameters, similarly to site parameters used in expectation propagation. Our dual parameterization speeds-up inference using natural gradient descent, and provides a tighter evidence lower bound for hyperparameter learning. The approach has the same memory cost as the current SVGP methods, but it is faster and more accurate.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/ADAM et al-2021 Dual Parameterization of Sparse Variational Gaussian Processes/ADAM et al_2021_Dual Parameterization of Sparse Variational Gaussian Processes.pdf}
}

@inproceedings{agarwalModelbasedRLOptimistic2022,
  title = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}: {{Structural Conditions}} and {{Sample Complexity}}},
  shorttitle = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {35284--35297},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Agarwal_Zhang-2022 Model-based RL with Optimistic Posterior Sampling/Agarwal_Zhang_2022_Model-based RL with Optimistic Posterior Sampling.pdf}
}

@inproceedings{ajayConditionalGenerativeModeling2022,
  title = {Is {{Conditional Generative Modeling}} All You Need for {{Decision Making}}?},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Ajay, Anurag and Du, Yilun and Gupta, Abhi and Tenenbaum, Joshua B. and Jaakkola, Tommi S. and Agrawal, Pulkit},
  year = {2022},
  month = sep,
  urldate = {2023-09-30},
  abstract = {Recent improvements in conditional generative modeling have made it possible to generate high-quality images from language descriptions alone. We investigate whether these methods can directly address the problem of sequential decision-making. We view decision-making not through the lens of reinforcement learning (RL), but rather through conditional generative modeling. To our surprise, we find that our formulation leads to policies that can outperform existing offline RL approaches across standard benchmarks. By modeling a policy as a return-conditional generative model, we avoid the need for dynamic programming and subsequently eliminate many of the complexities that come with traditional offline RL. We further demonstrate the advantages of modeling policies as conditional generative models by considering two other conditioning variables: constraints and skills. Conditioning on a single constraint or skill during training leads to behaviors at test-time that can satisfy several constraints together or demonstrate a composition of skills. Our results illustrate that conditional generative modeling is a powerful tool for decision-making.},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ajay et al-2022 Is Conditional Generative Modeling all you need for Decision Making/Ajay et al_2022_Is Conditional Generative Modeling all you need for Decision Making.pdf}
}

@inproceedings{alayracFlamingoVisualLanguage2022,
  title = {Flamingo: A {{Visual Language Model}} for {{Few-Shot Learning}}},
  shorttitle = {Flamingo},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob L. and Borgeaud, Sebastian and Brock, Andy and Nematzadeh, Aida and Sharifzadeh, Sahand and Bi{\'n}kowski, Miko{\l}aj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Kar{\'e}n},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {23716--23736},
  urldate = {2023-10-02},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Alayrac et al-2022 Flamingo/Alayrac et al_2022_Flamingo.pdf}
}

@inproceedings{allen1983planning,
  title={Planning using a temporal world model},
  author={Allen, James F and Koomen, Johannes A},
  booktitle={Proceedings of the Eighth international joint conference on Artificial intelligence-Volume 2},
  pages={741--747},
  year={1983}
}

@article{almubarakSafety2021,
  title = {Safety {{Embedded Differential Dynamic Programming}} Using {{Discrete Barrier States}}},
  author = {Almubarak, Hassan and Stachowicz, Kyle and Sadegh, Nader and Theodorou, Evangelos A.},
  year = {2021},
  month = may,
  journal = {arXiv:2105.14608 [cs, eess]},
  eprint = {2105.14608},
  primaryclass = {cs, eess},
  urldate = {2021-06-30},
  abstract = {Certified safe control is a growing challenge in robotics, especially when performance and safety objectives are desired to be concurrently achieved. In this work, we extend the barrier state (BaS) concept, recently proposed for stabilization of continuous time systems, to enforce safety for discrete time systems by creating a discrete barrier state (DBaS). The constructed DBaS is embedded into the discrete model of the safety-critical system in order to integrate safety objectives into performance objectives. We subsequently use the proposed technique to implement a safety embedded stabilizing control for nonlinear discrete systems. Furthermore, we employ the DBaS method to develop a safety embedded differential dynamic programming (DDP) technique to plan and execute safe optimal trajectories. The proposed algorithm is leveraged on a differential wheeled robot and on a quadrotor to safely perform several tasks including reaching, tracking and safe multi-quadrotor movement. The DBaS-based DDP (DBaS-DDP) is compared to the penalty method used in constrained DDP problems where it is shown that the DBaS-DDP consistently outperforms the penalty method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/3JZHF338/Almubarak et al. - 2021 - Safety Embedded Differential Dynamic Programming u.pdf;/Users/scannea1/Zotero/storage/QN5N9CF7/2105.html}
}

@article{almubarakSafety2021a,
  title = {Safety {{Embedded Control}} of {{Nonlinear Systems}} via {{Barrier States}}},
  author = {Almubarak, Hassan and Sadegh, Nader and Theodorou, Evangelos A.},
  year = {2021},
  month = mar,
  journal = {arXiv:2102.10253 [cs, eess]},
  eprint = {2102.10253},
  primaryclass = {cs, eess},
  urldate = {2021-06-30},
  abstract = {In many safety critical control systems, possibly opposing safety restrictions and control performance objectives may arise. To confront such a conflict, this paper proposes a methodology that embeds safety into stability of control systems. The development enforces safety by means of barrier functions used in optimization which are used to construct {\textbackslash}textit\{barrier states\} (BaS) that are {\textbackslash}textit\{embedded\} in the control system's model. As a result, as long as the equilibrium point of interest of the closed loop system is asymptotically stable, the generated trajectories are guaranteed to be safe. Consequently, a conflict between control objectives and safety constraints is substantially avoided. To show the efficacy of the proposed technique, we employ the simple pole placement method on a linear control system to generate a safely stabilizing controller. Optimal control is subsequently employed to fulfill safety, stability and performance objectives by solving the associated Hamilton-Jacobi-Bellman (HJB) which minimizes a cost functional that can involve the barrier states. Following this further, we exploit optimal control on a second dimensional pendulum on a cart model that is desired to avoid low velocities regions where the system may exhibit some controllability loss and on two simple mobile robots that are sent to opposite targets with an obstacle on the way which may potentially result in a collision.},
  archiveprefix = {arxiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control},
  note = {Comment: Updates: Corrected typos. Revised arguments in Section III and Section IV},
  file = {/Users/scannea1/Zotero/storage/3L49BDYL/Almubarak et al. - 2021 - Safety Embedded Control of Nonlinear Systems via B.pdf;/Users/scannea1/Zotero/storage/NXP4GVNR/2102.html}
}

@book{altmanConstrained1999,
  title = {Constrained {{Markov Decision Processes}}: {{Stochastic Modeling}}},
  shorttitle = {Constrained {{Markov Decision Processes}}},
  author = {Altman, Eitan},
  year = {1999},
  publisher = {{Routledge}},
  doi = {10.1201/9781315140223},
  abstract = {This book provides a unified approach for the study of constrained Markov decision processes with a finite state space and unbounded costs. Unlike the single controller case considered in many other books, the author considers a single controller with several objectives, such as minimizing delays and loss, probabilities, and maximization of throughputs. It is desirable to design a controller that minimizes one cost objective, subject to inequality constraints on other cost objectives. This framework describes dynamic decision problems arising frequently in many engineering fields. A thorough overview of these applications is presented in the introduction. The book is then divided into three sections that build upon each other.}
}

@article{amesControl2017,
  title = {Control {{Barrier Function Based Quadratic Programs}} for {{Safety Critical Systems}}},
  author = {Ames, Aaron D. and Xu, Xiangru and Grizzle, Jessy W. and Tabuada, Paulo},
  year = {2017},
  month = aug,
  journal = {IEEE Transactions on Automatic Control},
  volume = {62},
  number = {8},
  pages = {3861--3876},
  issn = {1558-2523},
  doi = {10.1109/TAC.2016.2638961},
  abstract = {Safety critical systems involve the tight coupling between potentially conflicting control objectives and safety constraints. As a means of creating a formal framework for controlling systems of this form, and with a view toward automotive applications, this paper develops a methodology that allows safety conditions-expressed as control barrier functions-to be unified with performance objectives-expressed as control Lyapunov functions-in the context of real-time optimization-based controllers. Safety conditions are specified in terms of forward invariance of a set, and are verified via two novel generalizations of barrier functions; in each case, the existence of a barrier function satisfying Lyapunov-like conditions implies forward invariance of the set, and the relationship between these two classes of barrier functions is characterized. In addition, each of these formulations yields a notion of control barrier function (CBF), providing inequality constraints in the control input that, when satisfied, again imply forward invariance of the set. Through these constructions, CBFs can naturally be unified with control Lyapunov functions (CLFs) in the context of a quadratic program (QP); this allows for the achievement of control objectives (represented by CLFs) subject to conditions on the admissible states of the system (represented by CBFs). The mediation of safety and performance through a QP is demonstrated on adaptive cruise control and lane keeping, two automotive control problems that present both safety and performance considerations coupled with actuator bounds.},
  keywords = {Automotive engineering,Barrier function,control Lyapunov function,Cruise control,Electrical engineering,Electronic mail,Lyapunov methods,nonlinear control,quadratic program,safety,Safety,set invariance},
  file = {/Users/scannea1/Zotero/storage/9NUGNXTJ/Ames et al. - 2017 - Control Barrier Function Based Quadratic Programs .pdf}
}

@inproceedings{amesControl2019,
  title = {Control {{Barrier Functions}}: {{Theory}} and {{Applications}}},
  shorttitle = {Control {{Barrier Functions}}},
  booktitle = {2019 18th {{European Control Conference}} ({{ECC}})},
  author = {Ames, Aaron D. and Coogan, Samuel and Egerstedt, Magnus and Notomista, Gennaro and Sreenath, Koushil and Tabuada, Paulo},
  year = {2019},
  month = jun,
  pages = {3420--3431},
  doi = {10.23919/ECC.2019.8796030},
  abstract = {This paper provides an introduction and overview of recent work on control barrier functions and their use to verify and enforce safety properties in the context of (optimization based) safety-critical controllers. We survey the main technical results and discuss applications to several domains including robotic systems.},
  file = {/Users/scannea1/Zotero/storage/HMGX6K3G/Ames et al. - 2019 - Control Barrier Functions Theory and Applications.pdf;/Users/scannea1/Zotero/storage/9BY4GJA5/8796030.html}
}

@inproceedings{amosModelBasedStochasticValue2021,
  title = {On the {{Model-Based Stochastic Value Gradient}} for {{Continuous Reinforcement Learning}}},
  booktitle = {Proceedings of the 3rd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Amos, Brandon and Stanton, Samuel and Yarats, Denis and Wilson, Andrew Gordon},
  year = {2021},
  month = may,
  pages = {6--20},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-22},
  abstract = {Model-based reinforcement learning approaches add explicit domain knowledge to agents in hopes of improving the sample-efficiency in comparison to model-free agents. However, in practice model-based methods are unable to achieve the same asymptotic performance on challenging continuous control tasks due to the complexity of learning and controlling an explicit world model. In this paper we investigate the stochastic value gradient (SVG),which is a well-known family of methods for controlling continuous systems which includes model-based approaches that distill a model-based value expansion into a model-free policy. We consider a variant of the model-based SVG that scales to larger systems and uses 1) an entropy regularization to help with exploration,2) a learned deterministic world model to improve the short-horizon value estimate, and 3) a learned model-free value estimate after the model's rollout. This SVG variation captures the model-free soft actor-critic method as an instance when the model rollout horizon is zero,and otherwise uses short-horizon model rollouts to improve the value estimate for the policy update. We surpass the asymptotic performance of other model-based methods on the proprioceptive MuJoCo locomotion tasks from the OpenAI gym,including a humanoid. We notably achieve these results with a simple deterministic world model without requiring an ensemble.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Amos et al-2021 On the Model-Based Stochastic Value Gradient for Continuous Reinforcement/Amos et al_2021_On the Model-Based Stochastic Value Gradient for Continuous Reinforcement.pdf}
}

@inproceedings{amosModelBasedStochasticValue2021a,
  title = {On the {{Model-Based Stochastic Value Gradient}} for {{Continuous Reinforcement Learning}}},
  booktitle = {Proceedings of the 3rd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Amos, Brandon and Stanton, Samuel and Yarats, Denis and Wilson, Andrew Gordon},
  year = {2021},
  month = may,
  pages = {6--20},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-05},
  abstract = {Model-based reinforcement learning approaches add explicit domain knowledge to agents in hopes of improving the sample-efficiency in comparison to model-free agents. However, in practice model-based methods are unable to achieve the same asymptotic performance on challenging continuous control tasks due to the complexity of learning and controlling an explicit world model. In this paper we investigate the stochastic value gradient (SVG),which is a well-known family of methods for controlling continuous systems which includes model-based approaches that distill a model-based value expansion into a model-free policy. We consider a variant of the model-based SVG that scales to larger systems and uses 1) an entropy regularization to help with exploration,2) a learned deterministic world model to improve the short-horizon value estimate, and 3) a learned model-free value estimate after the model's rollout. This SVG variation captures the model-free soft actor-critic method as an instance when the model rollout horizon is zero,and otherwise uses short-horizon model rollouts to improve the value estimate for the policy update. We surpass the asymptotic performance of other model-based methods on the proprioceptive MuJoCo locomotion tasks from the OpenAI gym,including a humanoid. We notably achieve these results with a simple deterministic world model without requiring an ensemble.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Amos et al-2021 On the Model-Based Stochastic Value Gradient for Continuous Reinforcement/Amos et al_2021_On the Model-Based Stochastic Value Gradient for Continuous Reinforcement2.pdf}
}

@inproceedings{anandUnsupervisedStateRepresentation2019,
  title = {Unsupervised {{State Representation Learning}} in {{Atari}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Anand, Ankesh and Racah, Evan and Ozair, Sherjil and Bengio, Yoshua and C{\^o}t{\'e}, Marc-Alexandre and Hjelm, R Devon},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-02},
  abstract = {State representation learning, or the ability to capture latent generative factors of an environment is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations in an unsupervised manner without supervision from rewards is an open problem. We introduce a method that tries to learn better state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Anand et al-2019 Unsupervised State Representation Learning in Atari/Anand et al_2019_Unsupervised State Representation Learning in Atari.pdf}
}

@article{andersonNonCentral1946,
  title = {The {{Non-Central Wishart Distribution}} and {{Certain Problems}} of {{Multivariate Statistics}}},
  author = {Anderson, T. W.},
  year = {1946},
  journal = {The Annals of Mathematical Statistics},
  volume = {17},
  number = {4},
  eprint = {2236082},
  eprinttype = {jstor},
  pages = {409--431},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  urldate = {2021-02-23},
  abstract = {The non-central Wishart distribution is the joint distribution of the sums of squares and cross-products of the deviations from the sample means when the observations arise from a set of normal multivariate populations with constant covariance matrix but expected values that vary from observation to observation. The characteristic function for this distribution is obtained from the distribution of the observations (Theorem 1). By using the characteristic functions it is shown that the convolution of several non-central Wishart distributions is another non-central Wishart distribution (Theorem 2). A simple integral representation of the distribution in the general case is given (Theorem 3). The integrand is a function of the roots of a determinantal equation involving the matrix of sums of squares and cross-products of deviations of observations and the matrix of sums of squares and cross-products of deviations of corresponding expected values. The knowledge of the non-central Wishart distribution is applied to two general problems of multivariate normal statistics. The moments of the generalized variance, which is the determinant of sums of squares and cross-products multiplied by a constant, are given for the cases of the expected values of the variates lying on a line (Theorem 4) and lying on a plane (Theorem 5). The likelihood ratio criterion for testing linear hypotheses can be expressed as the ratio of two determinants or as a symmetric function of the roots of a determinantal equation. In either case there is involved a matrix having a Wishart distribution and another matrix independently distributed such that the sum of these two matrices has a non-central Wishart distribution. When the null hypothesis is not true the moments of this criterion are given in the non-central planar case (Theorem 6).},
  file = {/Users/scannea1/Zotero/storage/Y35MHNGC/Anderson - 1946 - The Non-Central Wishart Distribution and Certain P.pdf}
}

@inproceedings{andreStateAbstractionProgrammable2002,
  title = {State Abstraction for Programmable Reinforcement Learning Agents},
  booktitle = {Eighteenth National Conference on {{Artificial}} Intelligence},
  author = {Andre, David and Russell, Stuart J.},
  year = {2002},
  month = jul,
  pages = {119--125},
  publisher = {{American Association for Artificial Intelligence}},
  address = {{USA}},
  urldate = {2023-11-02},
  abstract = {Safe state abstraction in reinforcement learning allows an agent to ignore aspects of its current state that are irrelevant to its current decision, and therefore speeds up dynamic programming and learning. This paper explores safe state abstraction in hierarchical reinforcement learning, where learned behaviors must conform to a given partial, hierarchical program. Unlike previous approaches to this problem, our methods yield significant state abstraction while maintaining \emph{hierarchical optimality}, i.e., optimality among all policies consistent with the partial program. We show how to achieve this for a partial programming language that is essentially Lisp augmented with nondeterministic constructs. We demonstrate our methods on two variants of Dietterich's taxi domain, showing how state abstraction and hierarchical optimality result in faster learning of better policies and enable the transfer of learned skills from one problem to another.},
  isbn = {978-0-262-51129-2}
}

@inproceedings{andrychowiczHindsightExperienceReplay2017,
  title = {Hindsight {{Experience Replay}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Pieter Abbeel, OpenAI and Zaremba, Wojciech},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-09-20},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Andrychowicz et al-2017 Hindsight Experience Replay/Andrychowicz et al_2017_Hindsight Experience Replay.pdf}
}

@article{aoyamaReceding2021,
  title = {Receding {{Horizon Differential Dynamic Programming Under Parametric Uncertainty}}},
  author = {Aoyama, Yuichiro and Saravanos, Augustinos D. and Theodorou, Evangelos A.},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.10836 [math]},
  eprint = {2104.10836},
  primaryclass = {math},
  urldate = {2021-06-25},
  abstract = {Generalized Polynomial Chaos (gPC) theory has been widely used for representing parametric uncertainty in a system, thanks to its ability to propagate uncertainty evolution. In an optimal control context, gPC can be combined with several optimization techniques to achieve a control policy that handles effectively this type of uncertainty. Such a suitable method is Differential Dynamic Programming (DDP), leading to an algorithm that inherits the scalability to high-dimensional systems and fast convergence nature of the latter. In this paper, we expand this combination aiming to acquire probabilistic guarantees on the satisfaction of constraints. In particular, we exploit the ability of gPC to express higher order moments of the uncertainty distribution - without any Gaussianity assumption - and we incorporate chance constraints that lead to expressions involving the state variance. Furthermore, we demonstrate that by implementing our algorithm in a receding horizon fashion, we compute control policies that effectively reduce the accumulation of uncertainty on the trajectory. The applicability of our method is verified through simulation results on a differential wheeled robot and a quadrotor that perform obstacle avoidance tasks.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Optimization and Control},
  note = {Comment: 7 pages, 3 figures, for CDC 2021},
  file = {/Users/scannea1/Zotero/storage/CXB6FM5C/Aoyama et al. - 2021 - Receding Horizon Differential Dynamic Programming .pdf;/Users/scannea1/Zotero/storage/5XLXZG2Z/2104.html}
}

@inproceedings{arcariDualStochasticMPC2020,
  title = {Dual {{Stochastic MPC}} for {{Systems}} with {{Parametric}} and {{Structural Uncertainty}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Arcari, Elena and Hewing, Lukas and Schlichting, Max and Zeilinger, Melanie},
  year = {2020},
  month = jul,
  pages = {894--903},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-11-29},
  abstract = {Designing controllers for systems affected by model uncertainty can prove to be a challenge, especially when seeking the optimal compromise between the conflicting goals of identification and control. This trade-off is explicitly taken into account in the dual control problem, for which the exact solution is provided by stochastic dynamic programming. Due to its computational intractability, we propose a sampling-based approximation for systems affected by both parametric and structural model uncertainty. The approach proposed in this paper separates the prediction horizon in a dual and an exploitation part. The dual part is formulated as a scenario tree that actively discriminates among a set of potential models while learning unknown parameters. In the exploitation part,  achieved information is fixed for each scenario, and open-loop control sequences are computed for the remainder of the horizon. As a result, we solve one optimization problem over a collection of control sequences for the entire horizon, explicitly considering the knowledge gained in each scenario, leading to a dual model predictive control formulation.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Arcari et al-2020 Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty/Arcari et al_2020_Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty.pdf}
}

@inproceedings{arcariDualStochasticMPC2020a,
  title = {Dual {{Stochastic MPC}} for {{Systems}} with {{Parametric}} and {{Structural Uncertainty}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Arcari, Elena and Hewing, Lukas and Schlichting, Max and Zeilinger, Melanie},
  year = {2020},
  month = jul,
  pages = {894--903},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-02-22},
  abstract = {Designing controllers for systems affected by model uncertainty can prove to be a challenge, especially when seeking the optimal compromise between the conflicting goals of identification and control. This trade-off is explicitly taken into account in the dual control problem, for which the exact solution is provided by stochastic dynamic programming. Due to its computational intractability, we propose a sampling-based approximation for systems affected by both parametric and structural model uncertainty. The approach proposed in this paper separates the prediction horizon in a dual and an exploitation part. The dual part is formulated as a scenario tree that actively discriminates among a set of potential models while learning unknown parameters. In the exploitation part,  achieved information is fixed for each scenario, and open-loop control sequences are computed for the remainder of the horizon. As a result, we solve one optimization problem over a collection of control sequences for the entire horizon, explicitly considering the knowledge gained in each scenario, leading to a dual model predictive control formulation.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Arcari et al-2020 Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty/Arcari et al_2020_Dual Stochastic MPC for Systems with Parametric and Structural Uncertainty2.pdf}
}

@article{ariafarADMMBO2019,
  title = {{{ADMMBO}}: {{Bayesian Optimization}} with {{Unknown Constraints}} Using {{ADMM}}},
  shorttitle = {{{ADMMBO}}},
  author = {Ariafar, Setareh and {Coll-Font}, Jaume and Brooks, Dana and Dy, Jennifer},
  year = {2019},
  month = jan,
  journal = {Journal of machine learning research : JMLR},
  volume = {20},
  abstract = {There exist many problems in science and engineering that involve optimization of an unknown or partially unknown objective function. Recently, Bayesian Optimization (BO) has emerged as a powerful tool for solving optimization problems whose objective functions are only available as a black box and are expensive to evaluate. Many practical problems, however, involve optimization of an unknown objective function subject to unknown constraints. This is an important yet challenging problem for which, unlike optimizing an unknown function, existing methods face several limitations. In this paper, we present a novel constrained Bayesian optimization framework to optimize an unknown objective function subject to unknown constraints. We introduce an equivalent optimization by augmenting the objective function with constraints, introducing auxiliary variables for each constraint, and forcing the new variables to be equal to the main variable. Building on the Alternating Direction Method of Multipliers (ADMM) algorithm, we propose ADMM-Bayesian Optimization (ADMMBO) to solve the problem in an iterative fashion. Our framework leads to multiple unconstrained subproblems with unknown objective functions, which we then solve via BO. Our method resolves several challenges of state-of-the-art techniques: it can start from infeasible points, is insensitive to initialization, can efficiently handle 'decoupled problems' and has a concrete stopping criterion. Extensive experiments on a number of challenging BO benchmark problems show that our proposed approach outperforms the state-of-the-art methods in terms of the speed of obtaining a feasible solution and convergence to the global optimum as well as minimizing the number of total evaluations of unknown objective and constraints functions.}
}

@inproceedings{arvanitidisLatent2018a,
  title = {Latent {{Space Oddity}}: On the {{Curvature}} of {{Deep Generative Models}}},
  shorttitle = {Latent {{Space Oddity}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, S{\o}ren},
  year = {2018},
  month = feb,
  urldate = {2021-03-08},
  abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear "generator" function that maps latent points into the input...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8Z7PQAKV/Arvanitidis et al. - 2018 - Latent Space Oddity on the Curvature of Deep Gene.pdf;/Users/scannea1/Zotero/storage/UJFPWJWH/forum.html}
}

@misc{asConstrainedPolicyOptimization2022,
  title = {Constrained {{Policy Optimization}} via {{Bayesian World Models}}},
  author = {As, Yarden and Usmanova, Ilnura and Curi, Sebastian and Krause, Andreas},
  year = {2022},
  month = feb,
  number = {arXiv:2201.09802},
  eprint = {2201.09802},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.09802},
  urldate = {2022-09-12},
  abstract = {Improving sample-efficiency and safety are crucial challenges when deploying reinforcement learning in high-stakes real world applications. We propose LAMBDA, a novel model-based approach for policy optimization in safety critical tasks modeled via constrained Markov decision processes. Our approach utilizes Bayesian world models, and harnesses the resulting uncertainty to maximize optimistic upper bounds on the task objective, as well as pessimistic upper bounds on the safety constraints. We demonstrate LAMBDA's state of the art performance on the Safety-Gym benchmark suite in terms of sample efficiency and constraint violation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/As et al-2022 Constrained Policy Optimization via Bayesian World Models/As et al_2022_Constrained Policy Optimization via Bayesian World Models.pdf;/Users/scannea1/Zotero/storage/SYXWY3QE/2201.html}
}

@inproceedings{assranSelfSupervisedLearningImages2023,
  title = {Self-{{Supervised Learning From Images With}} a {{Joint-Embedding Predictive Architecture}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Assran, Mahmoud and Duval, Quentin and Misra, Ishan and Bojanowski, Piotr and Vincent, Pascal and Rabbat, Michael and LeCun, Yann and Ballas, Nicolas},
  year = {2023},
  pages = {15619--15629},
  urldate = {2023-10-02},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Assran et al-2023 Self-Supervised Learning From Images With a Joint-Embedding Predictive/Assran et al_2023_Self-Supervised Learning From Images With a Joint-Embedding Predictive.pdf}
}

@article{aswaniProvably2013,
  title = {Provably Safe and Robust Learning-Based Model Predictive Control},
  author = {Aswani, Anil and Gonzalez, Humberto and Sastry, S. Shankar and Tomlin, Claire},
  year = {2013},
  month = may,
  journal = {Automatica},
  volume = {49},
  number = {5},
  pages = {1216--1226},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2013.02.003},
  urldate = {2021-06-25},
  abstract = {Controller design faces a trade-off between robustness and performance, and the reliability of linear controllers has caused many practitioners to focus on the former. However, there is renewed interest in improving system performance to deal with growing energy constraints. This paper describes a learning-based model predictive control (LBMPC) scheme that provides deterministic guarantees on robustness, while statistical identification tools are used to identify richer models of the system in order to improve performance; the benefits of this framework are that it handles state and input constraints, optimizes system performance with respect to a cost function, and can be designed to use a wide variety of parametric or nonparametric statistical tools. The main insight of LBMPC is that safety and performance can be decoupled under reasonable conditions in an optimization framework by maintaining two models of the system. The first is an approximate model with bounds on its uncertainty, and the second model is updated by statistical methods. LBMPC improves performance by choosing inputs that minimize a cost subject to the learned dynamics, and it ensures safety and robustness by checking whether these same inputs keep the approximate model stable when it is subject to uncertainty. Furthermore, we show that if the system is sufficiently excited, then the LBMPC control action probabilistically converges to that of an MPC computed using the true dynamics.},
  langid = {english},
  keywords = {Learning control,Predictive control,Robustness,Safety analysis,Statistics},
  file = {/Users/scannea1/Zotero/storage/NM75P263/Aswani et al. - 2013 - Provably safe and robust learning-based model pred.pdf;/Users/scannea1/Zotero/storage/L22HPPJE/S0005109813000678.html}
}

@inproceedings{atkesonComparison1997,
  title = {A Comparison of Direct and Model-Based Reinforcement Learning},
  booktitle = {Proceedings of {{International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Atkeson, C.G. and Santamaria, J.C.},
  year = {1997},
  month = apr,
  volume = {4},
  pages = {3557-3564 vol.4},
  doi = {10.1109/ROBOT.1997.606886},
  abstract = {This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.},
  keywords = {Computational modeling,Control system synthesis,Control systems,Educational institutions,Force control,Jacobian matrices,Learning,Robots,State-space methods,Training data},
  file = {/Users/scannea1/Zotero/storage/ANHXHIXS/Atkeson and Santamaria - 1997 - A comparison of direct and model-based reinforceme.pdf;/Users/scannea1/Zotero/storage/P4GM3WQN/606886.html}
}

@article{auerUsing2002,
  title = {Using {{Confidence Bounds}} for {{Exploitation-Exploration Trade-offs}}},
  author = {Auer, Peter},
  year = {2002},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Nov},
  pages = {397--422},
  issn = {ISSN 1533-7928},
  urldate = {2022-05-02},
  abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
  file = {/Users/scannea1/Zotero/storage/T7P7KJVN/Auer - 2002 - Using Confidence Bounds for Exploitation-Explorati.pdf}
}

@inproceedings{baiGaussian2022,
  title = {Gaussian {{Mixture Variational Autoencoder}} with {{Contrastive Learning}} for {{Multi-Label Classification}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Bai, Junwen and Kong, Shufeng and Gomes, Carla P.},
  year = {2022},
  month = jun,
  pages = {1383--1398},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {Multi-label classification (MLC) is a prediction task where each sample can have more than one label. We propose a novel contrastive learning boosted multi-label prediction model based on a Gaussian mixture variational autoencoder (C-GMVAE), which learns a multimodal prior space and employs a contrastive loss. Many existing methods introduce extra complex neural modules like graph neural networks to capture the label correlations, in addition to the prediction modules. We find that by using contrastive learning in the supervised setting, we can exploit label information effectively in a data-driven manner, and learn meaningful feature and label embeddings which capture the label correlations and enhance the predictive power. Our method also adopts the idea of learning and aligning latent spaces for both features and labels. In contrast to previous works based on a unimodal prior, C-GMVAE imposes a Gaussian mixture structure on the latent space, to alleviate the posterior collapse and over-regularization issues. C-GMVAE outperforms existing methods on multiple public datasets and can often match other models' full performance with only 50\% of the training data. Furthermore, we show that the learnt embeddings provide insights into the interpretation of label-label interactions.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/6FD5XP9Q/Bai et al. - 2022 - Gaussian Mixture Variational Autoencoder with Cont.pdf}
}

@article{barcelosDual2021,
  title = {Dual {{Online Stein Variational Inference}} for {{Control}} and {{Dynamics}}},
  author = {Barcelos, Lucas and Lambert, Alexander and Oliveira, Rafael and Borges, Paulo and Boots, Byron and Ramos, Fabio},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.12890 [cs]},
  eprint = {2103.12890},
  primaryclass = {cs},
  urldate = {2021-06-30},
  abstract = {Model predictive control (MPC) schemes have a proven track record for delivering aggressive and robust performance in many challenging control tasks, coping with nonlinear system dynamics, constraints, and observational noise. Despite their success, these methods often rely on simple control distributions, which can limit their performance in highly uncertain and complex environments. MPC frameworks must be able to accommodate changing distributions over system parameters, based on the most recent measurements. In this paper, we devise an implicit variational inference algorithm able to estimate distributions over model parameters and control inputs on-the-fly. The method incorporates Stein Variational gradient descent to approximate the target distributions as a collection of particles, and performs updates based on a Bayesian formulation. This enables the approximation of complex multi-modal posterior distributions, typically occurring in challenging and realistic robot navigation tasks. We demonstrate our approach on both simulated and real-world experiments requiring real-time execution in the face of dynamically changing environments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  note = {Comment: Corresponding author: lucas.barcelos@sydney.edu.au},
  file = {/Users/scannea1/Zotero/storage/XEYAVZZ7/Barcelos et al. - 2021 - Dual Online Stein Variational Inference for Contro.pdf;/Users/scannea1/Zotero/storage/RYZDEYSS/2103.html}
}

@inproceedings{barth-maronDistributedDistributionalDeterministic2018,
  title = {Distributed {{Distributional Deterministic Policy Gradients}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {{Barth-Maron}, Gabriel and Hoffman, Matthew W. and Budden, David and Dabney, Will and Horgan, Dan and Tb, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  year = {2018},
  month = feb,
  urldate = {2023-10-09},
  abstract = {This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of N-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Barth-Maron et al-2018 Distributed Distributional Deterministic Policy Gradients/Barth-Maron et al_2018_Distributed Distributional Deterministic Policy Gradients.pdf}
}

@article{basye1992decision,
  title={A decision-theoretic approach to planning, perception, and control},
  author={Basye, Kenneth and Dean, Thomas and Kirman, Jak and Lejter, Moises},
  journal={IEEE Expert},
  volume={7},
  number={4},
  pages={58--65},
  year={1992},
  publisher={IEEE}
}

@inproceedings{bauerUnderstanding2016,
  title = {Understanding {{Probabilistic Sparse Gaussian Process Approximations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bauer, Matthias and {van der Wilk}, Mark and Rasmussen, Carl Edward},
  year = {2016},
  volume = {29},
  pages = {1533--1541},
  urldate = {2021-02-02},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PQCYYMZR/Bauer et al. - 2016 - Understanding Probabilistic Sparse Gaussian Proces.pdf;/Users/scannea1/Zotero/storage/FWFIKGWQ/7250eb93b3c18cc9daa29cf58af7a004-Abstract.html}
}

@inproceedings{bechtleCurious2020,
  title = {Curious {{iLQR}}: {{Resolving Uncertainty}} in {{Model-based RL}}},
  shorttitle = {Curious {{iLQR}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Bechtle, Sarah and Lin, Yixin and Rai, Akshara and Righetti, Ludovic and Meier, Franziska},
  year = {2020},
  month = may,
  pages = {162--171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-30},
  abstract = {Curiosity as a means to explore during reinforcement learning problems has recently become very popular. However, very little progress has been made in utilizing curiosity for learning control. In ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JVSF3DF5/Bechtle et al. - 2020 - Curious iLQR Resolving Uncertainty in Model-based.pdf;/Users/scannea1/Zotero/storage/57HXZE89/bechtle20a.html}
}

@article{becker2022on,
  title = {On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning},
  author = {Becker, Philipp and Neumann, Gerhard},
  year = {2022},
  journal = {Transactions on Machine Learning Research}
}

@article{becker2022on,
  title = {On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning},
  author = {Becker, Philipp and Neumann, Gerhard},
  year = {2022},
  journal = {Transactions on Machine Learning Research},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Becker_Neumann-2022 On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning/Becker_Neumann_2022_On Uncertainty in Deep State Space Models for Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/66KFGUAK/forum.html}
}

@misc{beckSurveyMetaReinforcementLearning2023,
  title = {A {{Survey}} of {{Meta-Reinforcement Learning}}},
  author = {Beck, Jacob and Vuorio, Risto and Liu, Evan Zheran and Xiong, Zheng and Zintgraf, Luisa and Finn, Chelsea and Whiteson, Shimon},
  year = {2023},
  month = jan,
  number = {arXiv:2301.08028},
  eprint = {2301.08028},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.08028},
  urldate = {2023-09-30},
  abstract = {While deep reinforcement learning (RL) has fueled multiple high-profile successes in machine learning, it is held back from more widespread adoption by its often poor data efficiency and the limited generality of the policies it produces. A promising approach for alleviating these limitations is to cast the development of better RL algorithms as a machine learning problem itself in a process called meta-RL. Meta-RL is most commonly studied in a problem setting where, given a distribution of tasks, the goal is to learn a policy that is capable of adapting to any new task from the task distribution with as little data as possible. In this survey, we describe the meta-RL problem setting in detail as well as its major variations. We discuss how, at a high level, meta-RL research can be clustered based on the presence of a task distribution and the learning budget available for each individual task. Using these clusters, we then survey meta-RL algorithms and applications. We conclude by presenting the open problems on the path to making meta-RL part of the standard toolbox for a deep RL practitioner.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Beck et al-2023 A Survey of Meta-Reinforcement Learning/Beck et al_2023_A Survey of Meta-Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/AGAL7SQY/2301.html}
}

@article{bellemareAutonomous2020,
  title = {Autonomous Navigation of Stratospheric Balloons Using Reinforcement Learning},
  author = {Bellemare, Marc G. and Candido, Salvatore and Castro, Pablo Samuel and Gong, Jun and Machado, Marlos C. and Moitra, Subhodeep and Ponda, Sameera S. and Wang, Ziyu},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7836},
  pages = {77--82},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2939-8},
  urldate = {2020-12-15},
  abstract = {Efficiently navigating a superpressure balloon in the stratosphere1 requires the integration of a multitude of cues, such as wind speed and solar elevation, and the process is complicated by forecast errors and sparse wind measurements. Coupled with the need to make decisions in real time, these factors rule out the use of conventional control techniques2,3. Here we describe the use of reinforcement learning4,5 to create a high-performing flight controller. Our algorithm uses data augmentation6,7 and a self-correcting design to overcome the key technical challenge of reinforcement learning from imperfect data, which has proved to be a major obstacle to its application to physical systems8. We deployed our controller to station Loon superpressure balloons at multiple locations across the globe, including a 39-day controlled experiment over the Pacific Ocean. Analyses show that the controller outperforms Loon's previous algorithm and is robust to the natural diversity in stratospheric winds. These results demonstrate that reinforcement learning is an effective solution to real-world autonomous control problems in which neither conventional methods nor human intervention suffice, offering clues about what may be needed to create artificially intelligent agents that continuously interact with real, dynamic environments.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/698C2AZ7/Bellemare et al. - 2020 - Autonomous navigation of stratospheric balloons us.pdf;/Users/scannea1/Zotero/storage/5CXZNZ75/s41586-020-2939-8.html}
}

@inproceedings{bellemareDistributionalPerspectiveReinforcement2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  pages = {449--458},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-20},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Bellemare et al-2017 A Distributional Perspective on Reinforcement Learning/Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/RWJRM5RZ/Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf}
}

@article{bellmanDynamic1956,
  title = {Dynamic {{Programming}}},
  author = {Bellman, Richard},
  year = {1956},
  journal = {Princeton University Press},
  doi = {10.1126/science.153.3731.34},
  urldate = {2022-04-28}
}

@article{bellmanMarkovianDecisionProcess1957a,
  title = {A {{Markovian Decision Process}}},
  author = {Bellman, Richard},
  year = {1957},
  journal = {Journal of Mathematics and Mechanics},
  volume = {6},
  number = {5},
  eprint = {24900506},
  eprinttype = {jstor},
  pages = {679--684},
  publisher = {{Indiana University Mathematics Department}},
  issn = {0095-9057},
  urldate = {2023-05-13},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Bellman-1957 A Markovian Decision Process/Bellman_1957_A Markovian Decision Process2.pdf}
}

@inproceedings{belznerBayesian2017b,
  title = {Bayesian Verification under Model Uncertainty},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Software Engineering}} for {{Smart Cyber-Physical Systems}}},
  author = {Belzner, Lenz and Gabor, Thomas},
  year = {2017},
  month = may,
  series = {{{SEsCPS}} '17},
  pages = {10--13},
  publisher = {{IEEE Press}},
  address = {{Buenos Aires, Argentina}},
  urldate = {2020-12-13},
  abstract = {Machine learning enables systems to build and update domain models based on runtime observations. In this paper, we study statistical model checking and runtime verification for systems with this ability. Two challenges arise: (1) Models built from limited runtime data yield uncertainty to be dealt with. (2) There is no definition of satisfaction w.r.t. uncertain hypotheses. We propose such a definition of subjective satisfaction based on recently introduced satisfaction functions. We also propose the BV algorithm as a Bayesian solution to runtime verification of subjective satisfaction under model uncertainty. BV provides user-definable stochastic bounds for type I and II errors. We discuss empirical results of a toy experiment.},
  isbn = {978-1-5386-4043-2},
  file = {/Users/scannea1/Zotero/storage/32XVLZ4U/Belzner and Gabor - 2017 - Bayesian Verification under Model Uncertainty.pdf}
}

@inproceedings{berkenkampSafe2015,
  title = {Safe and Robust Learning Control with {{Gaussian}} Processes},
  booktitle = {2015 {{European Control Conference}} ({{ECC}})},
  author = {Berkenkamp, Felix and Schoellig, Angela P.},
  year = {2015},
  month = jul,
  pages = {2496--2501},
  doi = {10.1109/ECC.2015.7330913},
  abstract = {This paper introduces a learning-based robust control algorithm that provides robust stability and performance guarantees during learning. The approach uses Gaussian process (GP) regression based on data gathered during operation to update an initial model of the system and to gradually decrease the uncertainty related to this model. Embedding this data-based update scheme in a robust control framework guarantees stability during the learning process. Traditional robust control approaches have not considered online adaptation of the model and its uncertainty before. As a result, their controllers do not improve performance during operation. Typical machine learning algorithms that have achieved similar high-performance behavior by adapting the model and controller online do not provide the guarantees presented in this paper. In particular, this paper considers a stabilization task, linearizes the nonlinear, GP-based model around a desired operating point, and solves a convex optimization problem to obtain a linear robust controller. The resulting performance improvements due to the learning-based controller are demonstrated in experiments on a quadrotor vehicle.},
  keywords = {Adaptation models,Data models,Robust control,Robustness,Stability analysis,Uncertainty,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/BBDEMG6I/Berkenkamp and Schoellig - 2015 - Safe and robust learning control with Gaussian pro.pdf}
}

@inproceedings{berkenkampSafe2016,
  title = {Safe Controller Optimization for Quadrotors with {{Gaussian}} Processes},
  booktitle = {International {{Conference}} on {{Robotics}} and {{Automation}}},
  author = {Berkenkamp, F. and Schoellig, A. P. and Krause, A.},
  year = {2016},
  month = may,
  pages = {491--496},
  publisher = {{IEEE}},
  doi = {10.1109/ICRA.2016.7487170},
  abstract = {One of the most fundamental problems when designing controllers for dynamic systems is the tuning of the controller parameters. Typically, a model of the system is used to obtain an initial controller, but ultimately the controller parameters must be tuned manually on the real system to achieve the best performance. To avoid this manual tuning step, methods from machine learning, such as Bayesian optimization, have been used. However, as these methods evaluate different controller parameters on the real system, safety-critical system failures may happen. In this paper, we overcome this problem by applying, for the first time, a recently developed safe optimization algorithm, SafeOpt, to the problem of automatic controller parameter tuning. Given an initial, low-performance controller, SafeOpt automatically optimizes the parameters of a control law while guaranteeing safety. It models the underlying performance measure as a Gaussian process and only explores new controller parameters whose performance lies above a safe performance threshold with high probability. Experimental results on a quadrotor vehicle indicate that the proposed method enables fast, automatic, and safe optimization of controller parameters without human intervention.},
  keywords = {aircraft control,automatic controller parameter tuning,Bayes methods,Bayesian optimization,Computational modeling,control system synthesis,controller design,dynamic systems,Gaussian process,Gaussian processes,helicopters,learning systems,low-performance controller,machine learning,Noise measurement,optimisation,Optimization,parameter estimation,performance threshold,probability,quadcopter,quadrotor vehicle,safe controller optimization,safe optimization algorithm,safe-exploration,SafeOpt,Safety,safety-critical system failure,Tuning,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/ZNFMVIUZ/Berkenkamp et al. - 2016 - Safe controller optimization for quadrotors with G.pdf;/Users/scannea1/Zotero/storage/P55WHW8T/7487170.html}
}

@inproceedings{berkenkampSafe2017,
  title = {Safe {{Model-based Reinforcement Learning}} with {{Stability Guarantees}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  year = {2017},
  volume = {30},
  pages = {908--918},
  urldate = {2020-11-26},
  langid = {english},
  keywords = {reinforcement-learning,safe-exploration},
  file = {/Users/scannea1/Zotero/storage/AQTLYZZW/Berkenkamp et al. - 2017 - Safe Model-based Reinforcement Learning with Stabi.pdf;/Users/scannea1/Zotero/storage/UXGE9IY2/766ebcd59621e305170616ba3d3dac32-Abstract.html}
}

@phdthesis{berkenkampSafe2019,
  type = {Doctoral {{Thesis}}},
  title = {Safe {{Exploration}} in {{Reinforcement Learning}}: {{Theory}} and {{Applications}} in {{Robotics}}},
  shorttitle = {Safe {{Exploration}} in {{Reinforcement Learning}}},
  author = {Berkenkamp, Felix},
  year = {2019},
  doi = {10.3929/ethz-b-000370833},
  urldate = {2020-11-26},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  langid = {english},
  school = {ETH Zurich},
  keywords = {quadcopter,reinforcement-learning,robotics,safe-exploration},
  annotation = {Accepted: 2019-10-16T10:53:26Z},
  file = {/Users/scannea1/Zotero/storage/WYNQYBJ3/Berkenkamp - 2019 - Safe Exploration in Reinforcement Learning Theory.pdf;/Users/scannea1/Zotero/storage/7XZQV7AR/370833.html;/Users/scannea1/Zotero/storage/J9XZZSNF/370833.html}
}

@article{betroBayesianMethodsGlobal1991,
  title = {Bayesian Methods in Global Optimization},
  author = {Betr{\`o}, Bruno},
  year = {1991},
  month = mar,
  journal = {Journal of Global Optimization},
  volume = {1},
  number = {1},
  pages = {1--14},
  issn = {1573-2916},
  doi = {10.1007/BF00120661},
  urldate = {2023-04-03},
  abstract = {This paper reviews methods which have been proposed for solving global optimization problems in the framework of the Bayesian paradigm.},
  langid = {english},
  keywords = {Bayesian inference,decision theory,multistart method,stochastic processes,stopping rules}
}

@article{bettsSurvey1998,
  title = {Survey of {{Numerical Methods}} for {{Trajectory Optimization}}},
  author = {Betts, John T.},
  year = {1998},
  month = mar,
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {21},
  number = {2},
  pages = {193--207},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/2.4231},
  urldate = {2021-02-08},
  file = {/Users/scannea1/Zotero/storage/AVKV9FTT/Betts - 1998 - Survey of Numerical Methods for Trajectory Optimiz.pdf;/Users/scannea1/Zotero/storage/UX6QF98W/2.html}
}

@inproceedings{bharadhwajInformationPrioritizationEmpowerment2021,
  title = {Information {{Prioritization}} through {{Empowerment}} in {{Visual Model-based RL}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Bharadhwaj, Homanga and Babaeizadeh, Mohammad and Erhan, Dumitru and Levine, Sergey},
  year = {2021},
  month = oct,
  urldate = {2023-09-30},
  abstract = {Model-based reinforcement learning (RL) algorithms designed for handling complex visual observations typically learn some sort of latent state representation, either explicitly or implicitly. Standard methods of this sort do not distinguish between functionally relevant aspects of the state and irrelevant distractors, instead aiming to represent all available information equally. We propose a modified objective for model-based RL that, in combination with mutual information maximization, allows us to learn representations and dynamics for visual model-based RL without reconstruction in a way that explicitly prioritizes functionally relevant factors. The key principle behind our design is to integrate a term inspired by variational empowerment into a state-space learning model based on mutual information. This term prioritizes information that is correlated with action, thus ensuring that functionally relevant factors are captured first. Furthermore, the same empowerment term also promotes faster exploration during the RL process, especially for sparse-reward tasks where the reward signal is insufficient to drive exploration in the early stages of learning. We evaluate the approach on a suite of vision-based robot control tasks with natural video backgrounds, and show that the proposed prioritized information objective outperforms state-of-the-art model based RL approaches by an average of 20{\textbackslash}\% in terms of episodic returns at 1M environment interactions with 30{\textbackslash}\% higher sample efficiency at 100k interactions.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Bharadhwaj et al-2021 Information Prioritization through Empowerment in Visual Model-based RL/Bharadhwaj et al_2021_Information Prioritization through Empowerment in Visual Model-based RL.pdf}
}

@inproceedings{bhardwajDifferentiable2020,
  title = {Differentiable {{Gaussian Process Motion Planning}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bhardwaj, Mohak and Boots, Byron and Mukadam, Mustafa},
  year = {2020},
  month = may,
  pages = {10598--10604},
  issn = {2577-087X},
  doi = {10.1109/ICRA40945.2020.9197260},
  abstract = {Modern trajectory optimization based approaches to motion planning are fast, easy to implement, and effective on a wide range of robotics tasks. However, trajectory optimization algorithms have parameters that are typically set in advance (and rarely discussed in detail). Setting these parameters properly can have a significant impact on the practical performance of the algorithm, sometimes making the difference between finding a feasible plan or failing at the task entirely. We propose a method for leveraging past experience to learn how to automatically adapt the parameters of Gaussian Process Motion Planning (GPMP) algorithms. Specifically, we propose a differentiable extension to the GPMP2 algorithm, so that it can be trained end-to-end from data. We perform several experiments that validate our algorithm and illustrate the benefits of our proposed learning-based approach to motion planning.},
  keywords = {Artificial intelligence,Automation,Conferences,Gaussian processes,Planning,Robots,Trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/Z83QZZAC/Bhardwaj et al. - 2020 - Differentiable Gaussian Process Motion Planning.pdf;/Users/scannea1/Zotero/storage/CB972CMQ/9197260.html}
}

@inproceedings{bingMetaReinforcementLearningLanguage2023,
  title = {Meta-{{Reinforcement Learning}} via {{Language Instructions}}},
  booktitle = {2023 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Bing, Zhenshan and Koch, Alexander and Yao, Xiangtong and Huang, Kai and Knoll, Alois},
  year = {2023},
  month = may,
  pages = {5985--5991},
  doi = {10.1109/ICRA48891.2023.10160626},
  urldate = {2023-10-02},
  abstract = {Although deep reinforcement learning has recently been very successful at learning complex behaviors, it requires a tremendous amount of data to learn a task. One of the fundamental reasons causing this limitation lies in the nature of the trial-and-error learning paradigm of reinforcement learning, where the agent communicates with the environment and pro-gresses in the learning only relying on the reward signal. This is implicit and rather insufficient to learn a task well. On the con-trary, humans are usually taught new skills via natural language instructions. Utilizing language instructions for robotic motion control to improve the adaptability is a recently emerged topic and challenging. In this paper, we present a meta-RL algorithm that addresses the challenge of learning skills with language instructions in multiple manipulation tasks. On the one hand, our algorithm utilizes the language instructions to shape its in-terpretation of the task, on the other hand, it still learns to solve task in a trial-and-error process. We evaluate our algorithm on the robotic manipulation benchmark (Meta-World) and it significantly outperforms state-of-the-art methods in terms of training and testing task success rates. Codes are available at https://tumi6robot.wixsite.com/million.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Bing et al-2023 Meta-Reinforcement Learning via Language Instructions/Bing et al_2023_Meta-Reinforcement Learning via Language Instructions.pdf;/Users/scannea1/Zotero/storage/VU9UE6PD/10160626.html}
}

@inproceedings{blundellWeight2015,
  title = {Weight {{Uncertainty}} in {{Neural Network}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = jun,
  pages = {1613--1622},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2022-07-15},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/WJBLL6WY/Blundell et al. - 2015 - Weight Uncertainty in Neural Network.pdf}
}

@inproceedings{boedeckerApproximate2014,
  title = {Approximate Real-Time Optimal Control Based on Sparse {{Gaussian}} Process Models},
  booktitle = {2014 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {Boedecker, Joschka and Springenberg, Jost Tobias and W{\"u}lfing, Jan and Riedmiller, Martin},
  year = {2014},
  month = dec,
  pages = {1--8},
  issn = {2325-1867},
  doi = {10.1109/ADPRL.2014.7010608},
  abstract = {In this paper we present a fully automated approach to (approximate) optimal control of non-linear systems. Our algorithm jointly learns a non-parametric model of the system dynamics - based on Gaussian Process Regression (GPR) - and performs receding horizon control using an adapted iterative LQR formulation. This results in an extremely data-efficient learning algorithm that can operate under real-time constraints. When combined with an exploration strategy based on GPR variance, our algorithm successfully learns to control two benchmark problems in simulation (two-link manipulator, cart-pole) as well as to swing-up and balance a real cart-pole system. For all considered problems learning from scratch, that is without prior knowledge provided by an expert, succeeds in less than 10 episodes of interaction with the system.},
  keywords = {Approximation algorithms,Approximation methods,Computational modeling,Optimal control,Optimization,Predictive models,Trajectory},
  file = {/Users/scannea1/Zotero/storage/DYM9RM8Y/Boedecker et al. - 2014 - Approximate real-time optimal control based on spa.pdf;/Users/scannea1/Zotero/storage/VCPKHUJ7/7010608.html}
}

@misc{bommasaniOpportunitiesRisksFoundation2022,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2023-11-23},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  note = {Comment: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI). Report page with citation guidelines: https://crfm.stanford.edu/report.html},
  file = {/Users/scannea1/Zotero/storage/94BIPLYH/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf;/Users/scannea1/Zotero/storage/RM29U5NP/2108.html}
}

@inproceedings{boneyRegularizing2019,
  title = {Regularizing {{Trajectory Optimization}} with {{Denoising Autoencoders}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Boney, Rinu and Di Palo, Norman and Berglund, Mathias and Ilin, Alexander and Kannala, Juho and Rasmus, Antti and Valpola, Harri},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-12-11},
  file = {/Users/scannea1/Zotero/storage/AS2KZHRW/Boney et al. - 2019 - Regularizing Trajectory Optimization with Denoisin.pdf}
}

@article{bonyPrincipe1969,
  title = {Principe Du Maximum, In{\'e}galit{\'e} de {{Harnack}} et Unicit{\'e} Du Probl{\`e}me de {{Cauchy}} Pour Les Op{\'e}rateurs Elliptiques D{\'e}g{\'e}n{\'e}r{\'e}s},
  author = {Bony, Jean-Michel},
  year = {1969},
  journal = {Annales de l'Institut Fourier},
  volume = {19},
  number = {1},
  pages = {277--304},
  doi = {10.5802/aif.319},
  urldate = {2021-06-29},
  file = {/Users/scannea1/Zotero/storage/XNR2DYNE/Bony - 1969 - Principe du maximum, ingalit de Harnack et unici.pdf;/Users/scannea1/Zotero/storage/FLLNVGFL/item.html}
}

@inproceedings{botevPracticalGaussNewtonOptimisation2017,
  title = {Practical {{Gauss-Newton Optimisation}} for {{Deep Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Botev, Aleksandar and Ritter, Hippolyt and Barber, David},
  year = {2017},
  month = jul,
  pages = {557--565},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-13},
  abstract = {We present an efficient block-diagonal approximation to the Gauss-Newton matrix for feedforward neural networks. Our resulting algorithm is competitive against state-of-the-art first-order optimisation methods, with sometimes significant improvement in optimisation performance. Unlike first-order methods, for which hyperparameter tuning of the optimisation parameters is often a laborious process, our approach can provide good performance even when used with default settings. A side result of our work is that for piecewise linear transfer functions, the network objective function can have no differentiable local maxima, which may partially explain why such transfer functions facilitate effective optimisation.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Botev et al-2017 Practical Gauss-Newton Optimisation for Deep Learning/Botev et al_2017_Practical Gauss-Newton Optimisation for Deep Learning.pdf;/Users/scannea1/Zotero/storage/J3VSA4Y7/Botev et al. - 2017 - Practical Gauss-Newton Optimisation for Deep Learn.pdf}
}

@article{brezisCharacterization1970,
  title = {On a Characterization of Flow-Invariant Sets},
  author = {Brezis, Haim},
  year = {1970},
  month = mar,
  journal = {Communications on Pure and Applied Mathematics},
  volume = {23},
  number = {2},
  pages = {261--263},
  issn = {00103640, 10970312},
  doi = {10.1002/cpa.3160230211},
  urldate = {2021-06-29},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/FL6V3IAF/on-a-characterization-of-flow-invariant-sets-FOPl7HAHdR.html}
}

@inproceedings{brohanRT1RoboticsTransformer2023,
  title = {{{RT-1}}: {{Robotics Transformer}} for {{Real-World Control}} at {{Scale}}},
  shorttitle = {{{RT-1}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Dabis, Joseph and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ibarz, Julian and Ichter, Brian and Irpan, Alex and Jackson, Tomas and Jesmonth, Sally and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Kuang-Huei and Levine, Sergey and Lu, Yao and Malla, Utsav and Manjunath, Deeksha and Mordatch, Igor and Nachum, Ofir and Parada, Carolina and Peralta, Jodilyn and Perez, Emily and Pertsch, Karl and Quiambao, Jornell and Rao, Kanishka and Ryoo, Michael S. and Salazar, Grecia and Sanketi, Pannag R. and Sayed, Kevin and Singh, Jaspiar and Sontakke, Sumedh and Stone, Austin and Tan, Clayton and Tran, Huong and Vanhoucke, Vincent and Vega, Steve and Vuong, Quan H. and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = jul,
  volume = {19},
  urldate = {2023-10-25},
  isbn = {978-0-9923747-9-2},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Brohan et al-2023 RT-1/Brohan et al_2023_RT-1.pdf}
}

@misc{brohanRT2VisionLanguageActionModels2023,
  title = {{{RT-2}}: {{Vision-Language-Action Models Transfer Web Knowledge}} to {{Robotic Control}}},
  shorttitle = {{{RT-2}}},
  author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
  year = {2023},
  month = jul,
  number = {arXiv:2307.15818},
  eprint = {2307.15818},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models (VLA) and instantiate an example of such a model, which we call RT-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables RT-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows RT-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
  archiveprefix = {arxiv},
  keywords = {\_tablet,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Website: https://robotics-transformer.github.io/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Brohan et al-2023 RT-2/Brohan et al_2023_RT-2.pdf;/Users/scannea1/Zotero/storage/33TXTFE2/2307.html}
}

@inproceedings{buckmanSampleEfficientReinforcementLearning2018,
  title = {Sample-{{Efficient Reinforcement Learning}} with {{Stochastic Ensemble Value Expansion}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-05},
  abstract = {There is growing interest in combining model-free and model-based approaches in reinforcement learning with the goal of achieving the high performance of model-free algorithms with low sample complexity. This is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths, STEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Buckman et al-2018 Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion/Buckman et al_2018_Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion.pdf}
}

@inproceedings{buisson-fenetActively2020,
  title = {Actively {{Learning Gaussian Process Dynamics}}},
  booktitle = {2nd {{Annual Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {{Buisson-Fenet}, Mona and Solowjow, Friedrich and Trimpe, Sebastian},
  year = {2020},
  volume = {120},
  pages = {1--11},
  publisher = {{Proceedings of Machine Learning Research}},
  abstract = {Despite the availability of ever more data enabled through modern sensor and computer technology, it still remains an open problem to learn dynamical systems in a sample-efficient way. We propose active learning strategies that leverage information-theoretical properties arising naturally during Gaussian process regression while respecting constraints on the sampling process imposed by the system dynamics. Sample points are selected in regions with high uncertainty, leading to exploratory behavior and data-efficient training of the model. All results are validated in an extensive numerical benchmark.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3LQUYRKQ/Buisson-Fenet et al. - Actively Learning Gaussian Process Dynamics.pdf}
}

@inproceedings{burtRates2019,
  title = {Rates of {{Convergence}} for {{Sparse Variational Gaussian Process Regression}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Burt, David and Rasmussen, Carl Edward and Wilk, Mark Van Der},
  year = {2019},
  month = may,
  pages = {862--871},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-04-08},
  abstract = {Excellent variational approximations to Gaussian process posteriors have been developed which avoid the ({$N$}3)O(N3){\textbackslash}mathcal\{O\}{\textbackslash}left(N\^3{\textbackslash}right) scaling with dataset size {$N$}NN. They reduce the computational cost to ({$NM$}2)O(NM2){\textbackslash}mathcal\{O\}{\textbackslash}left(NM\^2{\textbackslash}right), with {$M\llN$}M{$\ll$}NM{\textbackslash}ll N the number of inducing variables, which summarise the process. While the computational cost seems to be linear in {$N$}NN, the true complexity of the algorithm depends on how {$M$}MM must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing {$M$}MM more slowly than {$N$}NN. A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, {$M$}=(log{$DN$})M=O(logDN)M={\textbackslash}mathcal\{O\}({\textbackslash}log\^D N) suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase {$M$}MM in continual learning scenarios.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/IVLK3YJI/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf;/Users/scannea1/Zotero/storage/VLQ6WE7S/Burt et al. - 2019 - Rates of Convergence for Sparse Variational Gaussi.pdf}
}

@inproceedings{caponeLocalized2020,
  title = {Localized Active Learning of {{Gaussian}} Process State Space Models},
  booktitle = {2nd {{Annual Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Capone, Alexandre and Noske, Gerrit and Umlauft, Jonas and Beckers, Thomas and Lederer, Armin and Hirche, Sandra},
  year = {2020},
  volume = {120:1-12},
  eprint = {2005.02191},
  publisher = {{Proceedings of Machine Learning Research}},
  urldate = {2021-08-05},
  abstract = {The performance of learning-based control techniques crucially depends on how effectively the system is explored. While most exploration techniques aim to achieve a globally accurate model, such approaches are generally unsuited for systems with unbounded state spaces. Furthermore, a globally accurate model is not required to achieve good performance in many common control applications, e.g., local stabilization tasks. In this paper, we propose an active learning strategy for Gaussian process state space models that aims to obtain an accurate model on a bounded subset of the state-action space. Our approach aims to maximize the mutual information of the exploration trajectories with respect to a discretization of the region of interest. By employing model predictive control, the proposed technique integrates information collected during exploration and adaptively improves its exploration strategy. To enable computational tractability, we decouple the choice of most informative data points from the model predictive control optimization step. This yields two optimization problems that can be solved in parallel. We apply the proposed method to explore the state space of various dynamical systems and compare our approach to a commonly used entropy-based exploration strategy. In all experiments, our method yields a better model within the region of interest than the entropy-based method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  note = {Comment: Submitted to Learning for Dynamics and Control (L4DC)},
  file = {/Users/scannea1/Zotero/storage/HWFMG3WZ/Capone et al. - 2020 - Localized active learning of Gaussian process stat.pdf;/Users/scannea1/Zotero/storage/J8WW4JNZ/2005.html}
}

@book{carmoRiemannian1992,
  title = {Riemannian {{Geometry}}},
  author = {do Carmo, Manfredo},
  year = {1992},
  series = {Mathematics: {{Theory}} \& {{Applications}}},
  publisher = {{Birkh{\"a}user Basel}},
  urldate = {2021-02-18},
  abstract = {Riemannian Geometry is an expanded edition of a highly acclaimed and successful textbook (originally published in Portuguese) for first-year graduate students in mathematics and physics. The author's treatment goes very directly to the basic language of Riemannian geometry and immediately presents some of its most fundamental theorems. It is elementary, assuming only a modest background from readers, making it suitable for a wide variety of students and course structures. Its selection of topics has been deemed "superb" by teachers who have used the text. A significant feature of the book is its powerful and revealing structure, beginning simply with the definition of a differentiable manifold and ending with one of the most important results in Riemannian geometry, a proof of the Sphere Theorem. The text abounds with basic definitions and theorems, examples, applications, and numerous exercises to test the student's understanding and extend knowledge and insight into the subject. Instructors and students alike will find the work to be a significant contribution to this highly applicable and stimulating subject.},
  isbn = {978-0-8176-3490-2},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VAALLD2J/9780817634902.html}
}

@inproceedings{changFantasizingDualGPs2022,
  title = {Fantasizing with {{Dual GPs}} in {{Bayesian Optimization}} and {{Active Learning}}},
  booktitle = {{{NeurIPS Workshop}} on {{Gaussian Processes}}, {{Spatiotemporal Modeling}}, and {{Decision-making Systems}}},
  author = {Chang, Paul E. and Verma, Prakhar and John, S. T. and Picheny, Victor and Moss, Henry and Solin, Arno},
  year = {2022},
  month = nov,
  eprint = {2211.01053},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01053},
  urldate = {2023-03-09},
  abstract = {Gaussian processes (GPs) are the main surrogate functions used for sequential modelling such as Bayesian Optimization and Active Learning. Their drawbacks are poor scaling with data and the need to run an optimization loop when using a non-Gaussian likelihood. In this paper, we focus on `fantasizing' batch acquisition functions that need the ability to condition on new fantasized data computationally efficiently. By using a sparse Dual GP parameterization, we gain linear scaling with batch size as well as one-step updates for non-Gaussian likelihoods, thus extending sparse models to greedy batch fantasizing acquisition functions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: In the 2022 NeurIPS Workshop on Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chang et al-2022 Fantasizing with Dual GPs in Bayesian Optimization and Active Learning/Chang et al_2022_Fantasizing with Dual GPs in Bayesian Optimization and Active Learning.pdf;/Users/scannea1/Zotero/storage/RKY854SE/2211.html}
}

@inproceedings{changMemoryBasedDualGaussian2023,
  title = {Memory-{{Based Dual Gaussian Processes}} for {{Sequential Learning}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Chang, Paul Edmund and Verma, Prakhar and John, S. T. and Solin, Arno and Khan, Mohammad Emtiyaz},
  year = {2023},
  month = jul,
  pages = {4035--4054},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-13},
  abstract = {Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chang et al-2023 Memory-Based Dual Gaussian Processes for Sequential Learning/Chang et al_2023_Memory-Based Dual Gaussian Processes for Sequential Learning.pdf}
}

@inproceedings{chenCalibratingTransformersSparse2022,
  title = {Calibrating {{Transformers}} via {{Sparse Gaussian Processes}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Chen, Wenlong and Li, Yingzhen},
  year = {2022},
  month = sep,
  urldate = {2023-09-30},
  abstract = {Transformer models have achieved profound success in prediction tasks in a wide range of applications in natural language processing, speech recognition and computer vision. Extending Transformer's success to safety-critical domains requires calibrated uncertainty estimation which remains under-explored. To address this, we propose Sparse Gaussian Process attention (SGPA), which performs Bayesian inference directly in the output space of multi-head attention blocks (MHAs) in transformer to calibrate its uncertainty. It replaces the scaled dot-product operation with a valid symmetric kernel and uses sparse Gaussian processes (SGP) techniques to approximate the posterior processes of MHA outputs. Empirically, on a suite of prediction tasks on text, images and graphs, SGPA-based Transformers achieve competitive predictive accuracy, while noticeably improving both in-distribution calibration and out-of-distribution robustness and detection.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chen_Li-2022 Calibrating Transformers via Sparse Gaussian Processes/Chen_Li_2022_Calibrating Transformers via Sparse Gaussian Processes.pdf}
}

@inproceedings{chenGenAugRetargetingBehaviors2023,
  title = {{{GenAug}}: {{Retargeting}} Behaviors to Unseen Situations via {{Generative Augmentation}}},
  shorttitle = {{{GenAug}}},
  booktitle = {Robotics: {{Science}} and {{Systems XIX}}},
  author = {Chen, Qiuyu and Kiami, Shosuke C. and Gupta, Abhishek and Kumar, Vikash},
  year = {2023},
  month = jul,
  volume = {19},
  urldate = {2023-11-21},
  isbn = {978-0-9923747-9-2},
  file = {/Users/scannea1/Zotero/storage/FX7T727Y/Chen et al. - 2023 - GenAug Retargeting behaviors to unseen situations.pdf}
}

@inproceedings{chengEndtoEnd2019,
  title = {End-to-{{End Safe Reinforcement Learning}} through {{Barrier Functions}} for {{Safety-Critical Continuous Control Tasks}}},
  booktitle = {{{AAAI}}},
  author = {Cheng, Richard and Orosz, G. and Murray, R. and Burdick, J.},
  year = {2019},
  doi = {10.1609/aaai.v33i01.33013387},
  abstract = {Reinforcement Learning (RL) algorithms have found limited success beyond simulated applications, and one main reason is the absence of safety guarantees during the learning process. Real world systems would realistically fail or break before an optimal controller can be learned. To address this issue, we propose a controller architecture that combines (1) a model-free RL-based controller with (2) model-based controllers utilizing control barrier functions (CBFs) and (3) on-line learning of the unknown system dynamics, in order to ensure safety during learning. Our general framework leverages the success of RL algorithms to learn high-performance controllers, while the CBF-based controllers both guarantee safety and guide the learning process by constraining the set of explorable polices. We utilize Gaussian Processes (GPs) to model the system dynamics and its uncertainties.  Our novel controller synthesis algorithm, RL-CBF, guarantees safety with high probability during the learning process, regardless of the RL algorithm used, and demonstrates greater policy exploration efficiency. We test our algorithm on (1) control of an inverted pendulum and (2) autonomous car-following with wireless vehicle-to-vehicle communication, and show that our algorithm attains much greater sample efficiency in learning than other state-of-the-art algorithms and maintains safety during the entire learning process.},
  file = {/Users/scannea1/Zotero/storage/AAV4CC6T/Cheng et al. - 2019 - End-to-End Safe Reinforcement Learning through Bar.pdf}
}

@inproceedings{chenRandomizedEnsembledDouble2021,
  title = {Randomized {{Ensembled Double Q-Learning}}: {{Learning Fast Without}} a {{Model}}},
  shorttitle = {Randomized {{Ensembled Double Q-Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Chen, Xinyue and Wang, Che and Zhou, Zijian and Ross, Keith},
  year = {2021},
  eprint = {2101.05982},
  primaryclass = {cs},
  urldate = {2022-09-22},
  abstract = {Using a high Update-To-Data (UTD) ratio, model-based methods have recently achieved much higher sample efficiency than previous model-free methods for continuous-action DRL benchmarks. In this paper, we introduce a simple model-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show that its performance is just as good as, if not better than, a state-of-the-art model-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this performance using fewer parameters than the model-based method, and with less wall-clock run time. REDQ has three carefully integrated ingredients which allow it to achieve its high performance: (i) a UTD ratio {$>>$} 1; (ii) an ensemble of Q functions; (iii) in-target minimization across a random subset of Q functions from the ensemble. Through carefully designed experiments, we provide a detailed analysis of REDQ and related model-free algorithms. To our knowledge, REDQ is the first successful model-free DRL algorithm for continuous-action spaces using a UTD ratio {$>>$} 1.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at ICLR 2021},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chen et al-2021 Randomized Ensembled Double Q-Learning/Chen et al_2021_Randomized Ensembled Double Q-Learning.pdf;/Users/scannea1/Zotero/storage/FAXTXBT3/2101.html}
}

@misc{chenTransDreamerReinforcementLearning2022,
  title = {{{TransDreamer}}: {{Reinforcement Learning}} with {{Transformer World Models}}},
  shorttitle = {{{TransDreamer}}},
  author = {Chen, Chang and Wu, Yi-Fu and Yoon, Jaesik and Ahn, Sungjin},
  year = {2022},
  month = feb,
  number = {arXiv:2202.09481},
  eprint = {2202.09481},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.09481},
  urldate = {2023-09-30},
  abstract = {The Dreamer agent provides various benefits of Model-Based Reinforcement Learning (MBRL) such as sample efficiency, reusable knowledge, and safe planning. However, its world model and policy networks inherit the limitations of recurrent neural networks and thus an important question is how an MBRL framework can benefit from the recent advances of transformers and what the challenges are in doing so. In this paper, we propose a transformer-based MBRL agent, called TransDreamer. We first introduce the Transformer State-Space Model, a world model that leverages a transformer for dynamics predictions. We then share this world model with a transformer-based policy network and obtain stability in training a transformer-based RL agent. In experiments, we apply the proposed model to 2D visual RL and 3D first-person visual RL tasks both requiring long-range memory access for memory-based reasoning. We show that the proposed model outperforms Dreamer in these complex tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Deep RL Workshop NeurIPS 2021},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Chen et al-2022 TransDreamer/Chen et al_2022_TransDreamer.pdf;/Users/scannea1/Zotero/storage/688ZLJPD/2202.html}
}

@inproceedings{choiEnvironmentAgnosticRepresentation2023,
  title = {Environment {{Agnostic Representation}} for {{Visual Reinforcement Learning}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Choi, Hyesong and Lee, Hunsang and Jeong, Seongwon and Min, Dongbo},
  year = {2023},
  pages = {263--273},
  urldate = {2023-10-03},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Choi et al-2023 Environment Agnostic Representation for Visual Reinforcement Learning/Choi et al_2023_Environment Agnostic Representation for Visual Reinforcement Learning.pdf}
}

@inproceedings{chuaDeepReinforcementLearning2018,
  title = {Deep {{Reinforcement Learning}} in a {{Handful}} of {{Trials}} Using {{Probabilistic Dynamics Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  year = {2018},
  volume = {31},
  urldate = {2021-07-02},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/K5RIDAIK/Chua et al. - 2018 - Deep Reinforcement Learning in a Handful of Trials.pdf;/Users/scannea1/Zotero/storage/GPK99BVP/3de568f8597b94bda53149c7d7f5958c-Abstract.html}
}

@inproceedings{cohenHealing2020,
  title = {Healing {{Products}} of {{Gaussian Process Experts}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Cohen, Samuel and Mbuvha, Rendani and Marwala, Tshilidzi and Deisenroth, Marc},
  year = {2020},
  month = nov,
  pages = {2068--2077},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-26},
  abstract = {Gaussian processes (GPs) are nonparametric Bayesian models that have been applied to regression and classification problems. One of the approaches to alleviate their cubic training cost is the use of local GP experts trained on subsets of the data. In particular, product-of-expert models combine the predictive distributions of local experts through a tractable product operation. While these expert models allow for massively distributed computation, their predictions typically suffer from erratic behaviour of the mean or uncalibrated uncertainty quantification. By calibrating predictions via a tempered softmax weighting, we provide a solution to these problems for multiple product-of-expert models, including the generalised product of experts and the robust Bayesian committee machine. Furthermore, we leverage the optimal transport literature and propose a new product-of-expert model that combines predictions of local experts by computing their Wasserstein barycenter, which can be applied to both regression and classification.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/TYJE597B/Cohen et al. - 2020 - Healing Products of Gaussian Process Experts.pdf;/Users/scannea1/Zotero/storage/Z28D4TVC/Cohen et al. - 2020 - Healing Products of Gaussian Process Experts.pdf}
}

@book{coverElements2006,
  title = {Elements of Information Theory},
  author = {Cover, M., Thomas and Joy, A., Thomas},
  year = {2006},
  publisher = {{John Wiley \& Sons}}
}

@article{cowen-riversSAMBA2022,
  title = {{{SAMBA}}: Safe Model-Based~\& Active Reinforcement Learning},
  shorttitle = {{{SAMBA}}},
  author = {{Cowen-Rivers}, Alexander I. and Palenicek, Daniel and Moens, Vincent and Abdullah, Mohammed Amin and Sootla, Aivar and Wang, Jun and {Bou-Ammar}, Haitham},
  year = {2022},
  month = jan,
  journal = {Machine Learning},
  issn = {1573-0565},
  doi = {10.1007/s10994-021-06103-6},
  urldate = {2022-01-11},
  abstract = {In this paper, we propose SAMBA, a novel framework for safe reinforcement learning that combines aspects from probabilistic modelling, information theory, and statistics. Our method builds upon PILCO to enable active exploration using novel acquisition functions for out-of-sample Gaussian process evaluation optimised through a multi-objective problem that supports conditional-value-at-risk constraints. We evaluate our algorithm on a variety of safe dynamical system benchmarks involving both low and high-dimensional state representations. Our results show orders of magnitude reductions in samples and violations compared to state-of-the-art methods. Lastly, we provide intuition as to the effectiveness of the framework by a detailed analysis of our acquisition functions and safety constraints.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4K3U6I8V/Cowen-Rivers et al. - 2022 - SAMBA safe model-based& active reinforcement lea.pdf}
}

@inproceedings{coxStatisticalMethodGlobal1992,
  title = {A Statistical Method for Global Optimization},
  booktitle = {[{{Proceedings}}] 1992 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}}},
  author = {Cox, D.D. and John, S.},
  year = {1992},
  month = oct,
  pages = {1241-1246 vol.2},
  doi = {10.1109/ICSMC.1992.271617},
  abstract = {An algorithm for finding global optima using statistical prediction is presented. Assuming a random function model, lower confidence bounds on predicted values are used for sequential selection of evaluation points and as a convergence criterion. Comparison with published results for several test functions indicates that the procedure is very efficient in finding the global optimum of a multimodal function, and in terminating with relatively few evaluations.{$<>$}},
  keywords = {Concurrent computing,Convergence,Linear algebra,Optimization methods,Predictive models,Search methods,Statistical analysis,Statistics,Stochastic processes,Testing},
  file = {/Users/scannea1/Zotero/storage/PJ4QE9TW/271617.html}
}

@article{csatoSparseOnlineGaussian2002,
  title = {Sparse On-Line Gaussian Processes},
  author = {Csat{\'o}, Lehel and Opper, Manfred},
  year = {2002},
  month = mar,
  journal = {Neural Computation},
  volume = {14},
  number = {3},
  pages = {641--668},
  issn = {0899-7667},
  doi = {10.1162/089976602317250933},
  abstract = {We develop an approach for sparse representations of gaussian process (GP) models (which are Bayesian types of kernel machines) in order to overcome their limitations for large data sets. The method is based on a combination of a Bayesian on-line algorithm, together with a sequential construction of a relevant subsample of the data that fully specifies the prediction of the GP model. By using an appealing parameterization and projection techniques in a reproducing kernel Hilbert space, recursions for the effective parameters and a sparse gaussian approximation of the posterior process are obtained. This allows for both a propagation of predictions and Bayesian error measures. The significance and robustness of our approach are demonstrated on a variety of experiments.},
  langid = {english},
  pmid = {11860686},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Csat_Opper-2002 Sparse on-line gaussian processes/Csato_Opper_2002_Sparse on-line gaussian processes.pdf}
}

@inproceedings{curiCombining2021,
  title = {Combining {{Pessimism}} with {{Optimism}} for {{Robust}} and {{Efficient Model-Based Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Curi, Sebastian and Bogunovic, Ilija and Krause, Andreas},
  year = {2021},
  month = jul,
  pages = {2254--2264},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {In real-world tasks, reinforcement learning (RL) agents frequently encounter situations that are not present during training time. To ensure reliable performance, the RL agents need to exhibit robustness to such worst-case situations. The robust-RL framework addresses this challenge via a minimax optimization between an agent and an adversary. Previous robust RL algorithms are either sample inefficient, lack robustness guarantees, or do not scale to large problems. We propose the Robust Hallucinated Upper-Confidence RL (RH-UCRL) algorithm to provably solve this problem while attaining near-optimal sample complexity guarantees. RH-UCRL is a model-based reinforcement learning (MBRL) algorithm that effectively distinguishes between epistemic and aleatoric uncertainty and efficiently explores both the agent and the adversary decision spaces during policy learning. We scale RH-UCRL to complex tasks via neural networks ensemble models as well as neural network policies. Experimentally we demonstrate that RH-UCRL outperforms other robust deep RL algorithms in a variety of adversarial environments.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/56JHPA7V/Curi et al. - 2021 - Combining Pessimism with Optimism for Robust and E.pdf;/Users/scannea1/Zotero/storage/DWGYF72A/Curi et al. - 2021 - Combining Pessimism with Optimism for Robust and E.pdf}
}

@inproceedings{curiEfficient2020,
  title = {Efficient {{Model-Based Reinforcement Learning}} through {{Optimistic Policy Search}} and {{Planning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Curi, Sebastian and Berkenkamp, Felix and Krause, Andreas},
  year = {2020},
  urldate = {2020-11-26},
  keywords = {model-based-reinforcement-learning,planning,policy-search},
  file = {/Users/scannea1/Zotero/storage/ID2FAF6Y/Mukherjee and Fine - 1995 - Asymptotics of Gradient-based Neural Network Train.pdf;/Users/scannea1/Zotero/storage/3WGIBLZ7/a36b598abb934e4528412e5a2127b931-Paper.html}
}

@inproceedings{curiStructured2020,
  title = {Structured {{Variational Inference}} in {{Partially Observable Unstable Gaussian Process State Space Models}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Curi, Sebastian and Melchior, Silvan and Berkenkamp, Felix and Krause, Andreas},
  year = {2020},
  month = jul,
  pages = {147--157},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2020-11-26},
  abstract = {We propose a new variational inference algorithm for learning in Gaussian Process State-Space Models (GPSSMs). Our algorithm enables learning of unstable and partially observable systems, where pre...},
  langid = {english},
  keywords = {gaussian-process,state-space-model,variational-inference},
  file = {/Users/scannea1/Zotero/storage/DAEIJZM2/Curi et al. - 2020 - Structured Variational Inference in Partially Obse.pdf;/Users/scannea1/Zotero/storage/9W4SGF7V/curi20a.html}
}

@inproceedings{cutlerEfficient2015,
  title = {Efficient Reinforcement Learning for Robots Using Informative Simulated Priors},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Cutler, M. and How, J. P.},
  year = {2015},
  month = may,
  pages = {2605--2612},
  publisher = {{IEEE}},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2015.7139550},
  abstract = {Autonomous learning through interaction with the physical world is a promising approach to designing controllers and decision-making policies for robots. Unfortunately, learning on robots is often difficult due to the large number of samples needed for many learning algorithms. Simulators are one way to decrease the samples needed from the robot by incorporating prior knowledge of the dynamics into the learning algorithm. In this paper we present a novel method for transferring data from a simulator to a robot, using simulated data as a prior for real-world learning. A Bayesian nonparametric prior is learned from a potentially black-box simulator. The mean of this function is used as a prior for the Probabilistic Inference for Learning Control (PILCO) algorithm. The simulated prior improves the convergence rate and performance of PILCO by directing the policy search in areas of the state-space that have not yet been observed by the robot. Simulated and hardware results show the benefits of using the prior knowledge in the learning framework.},
  keywords = {autonomous learning,Bayes methods,Bayesian nonparametric prior,black-box simulator,controller design,convergence rate,Data models,decision-making policy,Gaussian processes,Hardware,Heuristic algorithms,informative simulated priors,learning (artificial intelligence),learning systems,Mathematical model,nonparametric statistics,PILCO algorithm,Prediction algorithms,probabilistic inference for learning control algorithm,reinforcement learning algorithm,robots,Robots},
  file = {/Users/scannea1/Zotero/storage/8TMVDYFN/Cutler and How - 2015 - Efficient reinforcement learning for robots using .pdf;/Users/scannea1/Zotero/storage/8868YSWR/7139550.html}
}

@phdthesis{damianouDeep2015,
  title = {Deep {{Gaussian Processes}} and {{Variational Propagation}} of {{Uncertainty}}},
  author = {Damianou, Andreas},
  year = {2015},
  month = jul,
  urldate = {2021-09-24},
  abstract = {Uncertainty propagation across components of complex probabilistic models is vital for improving regularisation. Unfortunately, for many interesting models based on non-linear Gaussian processes (GPs), straightforward propagation of uncertainty is computationally and mathematically intractable. This thesis is concerned with solving this problem through developing novel variational inference approaches.  From a modelling perspective, a key contribution of the thesis is the development of deep Gaussian processes (deep GPs). Deep GPs generalise several interesting GP-based models and, hence, motivate the development of uncertainty propagation techniques. In a deep GP, each layer is modelled as the output of a multivariate GP, whose inputs are governed by another GP. The resulting model is no longer a GP but, instead, can learn much more complex interactions between data. In contrast to other deep models, all the uncertainty in parameters and latent variables is marginalised out and both supervised and unsupervised learning is handled. Two important special cases of a deep GP can equivalently be seen as its building components and, historically, were developed as such. Firstly, the variational GP-LVM is concerned with propagating uncertainty in Gaussian process latent variable models. Any observed inputs (e.g. temporal) can also be used to correlate the latent space posteriors. Secondly, this thesis develops manifold relevance determination (MRD) which considers a common latent space for multiple views. An adapted variational framework allows for strong model regularisation, resulting in rich latent space representations to be learned. The developed models are also equipped with algorithms that maximise the information communicated between their different stages using uncertainty propagation, to achieve improved learning when partially observed values are present.  The developed methods are demonstrated in experiments with simulated and real data. The results show that the developed variational methodologies improve practical applicability by enabling automatic capacity control in the models, even when data are scarce.},
  copyright = {cc\_by\_nc\_nd},
  langid = {english},
  school = {University of Sheffield},
  file = {/Users/scannea1/Zotero/storage/6UA38AQH/Damianou - 2015 - Deep Gaussian Processes and Variational Propagatio.pdf;/Users/scannea1/Zotero/storage/GFC4KFMI/Damianou_Thesis.pdf;/Users/scannea1/Zotero/storage/KMT7ZCPG/9968.html}
}

@inproceedings{dasguptaAnalysis2005,
  title = {Analysis of a Greedy Active Learning Strategy},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dasgupta, Sanjoy},
  year = {2005},
  volume = {17},
  publisher = {{MIT Press}},
  urldate = {2021-08-12},
  file = {/Users/scannea1/Zotero/storage/V7PKLZJS/Dasgupta - 2005 - Analysis of a greedy active learning strategy.pdf}
}

@inproceedings{daxbergerLaplace2021,
  title = {Laplace {{Redux}} - {{Effortless Bayesian Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Daxberger, Erik and Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Bauer, Matthias and Hennig, Philipp},
  year = {2021},
  volume = {34},
  pages = {20089--20103},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-14},
  abstract = {Bayesian formulations of deep learning have been shown to have compelling theoretical properties and offer practical functional benefits, such as improved predictive uncertainty quantification and model selection. The Laplace approximation (LA) is a classic, and arguably the simplest family of approximations for the intractable posteriors of deep neural networks. Yet, despite its simplicity, the LA is not as popular as alternatives like variational Bayes or deep ensembles. This may be due to assumptions that the LA is expensive due to the involved Hessian computation, that it is difficult to implement, or that it yields inferior results. In this work we show that these are misconceptions: we (i) review the range of variants of the LA including versions with minimal cost overhead; (ii) introduce "laplace", an easy-to-use software library for PyTorch offering user-friendly access to all major flavors of the LA; and (iii) demonstrate through extensive experiments that the LA is competitive with more popular alternatives in terms of performance, while excelling in terms of computational cost. We hope that this work will serve as a catalyst to a wider adoption of the LA in practical deep learning, including in domains where Bayesian approaches are not typically considered at the moment.},
  file = {/Users/scannea1/Zotero/storage/YYKB5MPQ/Daxberger et al. - 2021 - Laplace Redux - Effortless Bayesian Deep Learning.pdf}
}

@article{deardenAbstractionApproximateDecisiontheoretic1997,
  title = {Abstraction and Approximate Decision-Theoretic Planning},
  author = {Dearden, Richard and Boutilier, Craig},
  year = {1997},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {89},
  number = {1},
  pages = {219--283},
  issn = {0004-3702},
  doi = {10.1016/S0004-3702(96)00023-9},
  urldate = {2023-11-02},
  abstract = {Markov decision processes (MDPs) have recently been proposed as useful conceptual models for understanding decision-theoretic planning. However, the utility of the associated computational methods remains open to question: most algorithms for computing optimal policies require explicit enumeration of the state space of the planning problem. We propose an abstraction technique for MDPs that allows approximately optimal solutions to be computed quickly. Abstractions are generated automatically, using an intensional representation of the planning problem (probabilistic strips rules) to determine the most relevant problem features and optimally solving a reduced problem based on these relevant features. The key features of our method are: abstractions can be generated quickly; the abstract solution can be applied directly to the original problem; and the loss of optimality can be bounded. We also describe methods by which the abstract solution can be viewed as a set of default reactions that can be improved incrementally, and used as a heuristic for search-based planning or other MDP methods. Finally, we discuss certain difficulties that point toward other forms of aggregation for MDPs.},
  keywords = {Abstraction,Approximation,Decision theory,Execution,Heuristics,Markov decision processes,Planning,Search},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Dearden_Boutilier-1997 Abstraction and approximate decision-theoretic planning/Dearden_Boutilier_1997_Abstraction and approximate decision-theoretic planning.pdf}
}

@inproceedings{deisenrothApproximate2008,
  title = {Approximate Dynamic Programming with {{Gaussian}} Processes},
  booktitle = {American {{Control Conference}}},
  author = {Deisenroth, Marc P. and Peters, Jan and Rasmussen, Carl E.},
  year = {2008},
  month = jun,
  pages = {4480--4485},
  publisher = {{IEEE}},
  issn = {2378-5861},
  doi = {10.1109/ACC.2008.4587201},
  abstract = {In general, it is difficult to determine an optimal closed-loop policy in nonlinear control problems with continuous-valued state and control domains. Hence, approximations are often inevitable. The standard method of discretizing states and controls suffers from the curse of dimensionality and strongly depends on the chosen temporal sampling rate. In this paper, we introduce Gaussian process dynamic programming (GPDP) and determine an approximate globally optimal closed-loop policy. In GPDP, value functions in the Bellman recursion of the dynamic programming algorithm are modeled using Gaussian processes. GPDP returns an optimal state- feedback for a finite set of states. Based on these outcomes, we learn a possibly discontinuous closed-loop policy on the entire state space by switching between two independently trained Gaussian processes. A binary classifier selects one Gaussian process to predict the optimal control signal. We show that GPDP is able to yield an almost optimal solution to an LQ problem using few sample points. Moreover, we successfully apply GPDP to the underpowered pendulum swing up, a complex nonlinear control problem.},
  keywords = {Bayesian methods,Control systems,Dynamic programming,Function approximation,Gaussian processes,Machine learning,Nonlinear dynamical systems,Optimal control,Sampling methods,State-space methods},
  file = {/Users/scannea1/Zotero/storage/YT5V4IBN/Deisenroth et al. - 2008 - Approximate dynamic programming with Gaussian proc.pdf;/Users/scannea1/Zotero/storage/EM9NQUE4/4587201.html}
}

@phdthesis{deisenrothEfficient2010,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc},
  year = {2010},
  month = nov,
  urldate = {2021-09-23},
  langid = {american},
  file = {/Users/scannea1/Zotero/storage/8K7J9N2Q/Deisenroth - 2019 - Efficient Reinforcement Learning using Gaussian Pr.pdf}
}

@phdthesis{deisenrothEfficient2010a,
  title = {Efficient {{Reinforcement Learning}} Using {{Gaussian Processes}}},
  author = {Deisenroth, Marc Peter},
  year = {2010},
  langid = {english},
  keywords = {gaussian-processes,model-based-rl},
  file = {/Users/scannea1/Zotero/storage/8XMSD6YK/Deisenroth - Efficient Reinforcement Learning using Gaussian Pr.pdf}
}

@article{deisenrothGaussian2009,
  title = {Gaussian Process Dynamic Programming},
  author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward and Peters, Jan},
  year = {2009},
  month = mar,
  journal = {Neurocomputing},
  series = {Advances in {{Machine Learning}} and {{Computational Intelligence}}},
  volume = {72},
  number = {7},
  pages = {1508--1524},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2008.12.019},
  urldate = {2021-06-18},
  abstract = {Reinforcement learning (RL) and optimal control of systems with continuous states and actions require approximation techniques in most interesting cases. In this article, we introduce Gaussian process dynamic programming (GPDP), an approximate value function-based RL algorithm. We consider both a classic optimal control problem, where problem-specific prior knowledge is available, and a classic RL problem, where only very general priors can be used. For the classic optimal control problem, GPDP models the unknown value functions with Gaussian processes and generalizes dynamic programming to continuous-valued states and actions. For the RL problem, GPDP starts from a given initial state and explores the state space using Bayesian active learning. To design a fast learner, available data have to be used efficiently. Hence, we propose to learn probabilistic models of the a priori unknown transition dynamics and the value functions on the fly. In both cases, we successfully apply the resulting continuous-valued controllers to the under-actuated pendulum swing up and analyze the performances of the suggested algorithms. It turns out that GPDP uses data very efficiently and can be applied to problems, where classic dynamic programming would be cumbersome.},
  langid = {english},
  keywords = {Bayesian active learning,Dynamic programming,Gaussian processes,Optimal control,Policy learning,Reinforcement learning},
  file = {/Users/scannea1/Zotero/storage/WSCK8AP2/Deisenroth et al. - 2009 - Gaussian process dynamic programming.pdf;/Users/scannea1/Zotero/storage/LC9VDCYH/S0925231209000162.html}
}

@article{deisenrothGaussian2015,
  title = {Gaussian {{Processes}} for {{Data-Efficient Learning}} in {{Robotics}} and {{Control}}},
  author = {Deisenroth, M. P. and Fox, D. and Rasmussen, C. E.},
  year = {2015},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {2},
  pages = {408--423},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2013.218},
  abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this paper, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
  keywords = {Approximation methods,Bayesian inference,Computational modeling,control,Data models,Gaussian processes,Policy search,Predictive models,Probabilistic logic,reinforcement learning,robotics,Robots,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/EKCHX8MG/Deisenroth et al. - 2015 - Gaussian Processes for Data-Efficient Learning in .pdf;/Users/scannea1/Zotero/storage/ETJFH6IF/6654139.html}
}

@inproceedings{deisenrothPILCO2011,
  title = {{{PILCO}}: {{A Model-Based}} and {{Data-Efficient Approach}} to {{Policy Search}}.},
  shorttitle = {{{PILCO}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Deisenroth, Marc and Rasmussen, Carl},
  year = {2011},
  month = jan,
  volume = {28},
  pages = {465--472},
  abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
  file = {/Users/scannea1/Zotero/storage/9Y6HDXE3/Deisenroth and Rasmussen - 2011 - PILCO A Model-Based and Data-Efficient Approach t.pdf}
}

@inproceedings{dengAcceleratedLinearizedLaplace2022,
  title = {Accelerated {{Linearized Laplace Approximation}} for {{Bayesian Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Deng, Zhijie and Zhou, Feng and Zhu, Jun},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {2695--2708},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Deng et al-2022 Accelerated Linearized Laplace Approximation for Bayesian Deep Learning/Deng et al_2022_Accelerated Linearized Laplace Approximation for Bayesian Deep Learning.pdf}
}

@inproceedings{dengAcceleratedLinearizedLaplace2022a,
  title = {Accelerated {{Linearized Laplace Approximation}} for {{Bayesian Deep Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Deng, Zhijie and Zhou, Feng and Zhu, Jun},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {2695--2708},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Deng et al-2022 Accelerated Linearized Laplace Approximation for Bayesian Deep Learning/Deng et al_2022_Accelerated Linearized Laplace Approximation for Bayesian Deep Learning2.pdf}
}

@inproceedings{depewegLearning2017,
  title = {Learning and Policy Search in Stochastic Dynamical Systems with {{Bayesian}} Neural Networks},
  booktitle = {5th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2017 - {{Conference Track Proceedings}}},
  author = {Depeweg, S. and {Hern{\'a}ndez-Lobato}, J. M. and {Doshi-Velez}, F. and Udluft, S.},
  year = {2017},
  urldate = {2022-04-29},
  file = {/Users/scannea1/Zotero/storage/XDBN68FW/1195629.html}
}

@inproceedings{doerrOptimizing2017,
  title = {Optimizing {{Long-term Predictions}} for {{Model-based Policy Search}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Doerr, Andreas and Daniel, {\relax Ch}ristian and {Nguyen-Tuong}, Duy and Marco, Alonso and Schaal, Stefan and Marc, Toussaint and Trimpe, Sebastian},
  year = {2017},
  month = oct,
  pages = {227--238},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-18},
  abstract = {We propose a novel long-term optimization criterion to improve the robustness of model-based reinforcement learning in real-world scenarios. Learning a dynamics model to derive a solution promises...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/T23MBV4T/Doerr et al. - 2017 - Optimizing Long-term Predictions for Model-based P.pdf;/Users/scannea1/Zotero/storage/TUM2S5EH/doerr17a.html}
}

@inproceedings{doerrProbabilistic2018,
  title = {Probabilistic {{Recurrent State-Space Models}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Doerr, A. and Daniel, C. and Schiegg, Martin and {Nguyen-Tuong}, D. and Schaal, S. and Toussaint, Marie-Eve and Trimpe, Sebastian},
  year = {2018},
  abstract = {State-space models (SSMs) are a highly expressive model class for learning patterns in time series data and for system identification. Deterministic versions of SSMs (e.g. LSTMs) proved extremely successful in modeling complex time series data. Fully probabilistic SSMs, however, are often found hard to train, even for smaller problems. To overcome this limitation, we propose a novel model formulation and a scalable training algorithm based on doubly stochastic variational inference and Gaussian processes. In contrast to existing work, the proposed variational approximation allows one to fully capture the latent state temporal correlations. These correlations are the key to robust training. The effectiveness of the proposed PR-SSM is evaluated on a set of real-world benchmark datasets in comparison to state-of-the-art probabilistic model learning methods. Scalability and robustness are demonstrated on a high dimensional problem.},
  keywords = {gaussian-processes,state-space-model,variational-inference},
  file = {/Users/scannea1/Zotero/storage/JZL52M9S/Doerr et al. - 2018 - Probabilistic Recurrent State-Space Models.pdf}
}

@inproceedings{dongMotion2016,
  title = {Motion {{Planning}} as {{Probabilistic Inference}} Using {{Gaussian Processes}} and {{Factor Graphs}}},
  booktitle = {Robotics: {{Science}} and {{Systems}}},
  author = {Dong, Jing and Mukadam, Mustafa and Dellaert, F. and Boots, Byron},
  year = {2016},
  doi = {10.15607/RSS.2016.XII.001},
  abstract = {With the increased use of high degree-of-freedom robots that must perform tasks in real-time, there is a need for fast algorithms for motion planning. In this work, we view motion planning from a probabilistic perspective. We consider smooth continuous-time trajectories as samples from a Gaussian process (GP) and formulate the planning problem as probabilistic inference. We use factor graphs and numerical optimization to perform inference quickly, and we show how GP interpolation can further increase the speed of the algorithm. Our framework also allows us to incrementally update the solution of the planning problem to contend with changing conditions. We benchmark our algorithm against several recent trajectory optimization algorithms on planning problems in multiple environments. Our evaluation reveals that our approach is several times faster than previous algorithms while retaining robustness. Finally, we demonstrate the incremental version of our algorithm on replanning problems, and show that it often can find successful solutions in a fraction of the time required to replan from scratch.},
  file = {/Users/scannea1/Zotero/storage/TYZB7HUM/Dong et al. - 2016 - Motion Planning as Probabilistic Inference using G.pdf}
}

@inproceedings{dorkaDynamicUpdatetoDataRatio2022,
  title = {Dynamic {{Update-to-Data Ratio}}: {{Minimizing World Model Overfitting}}},
  shorttitle = {Dynamic {{Update-to-Data Ratio}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Dorka, Nicolai and Welschehold, Tim and Burgard, Wolfram},
  year = {2022},
  month = sep,
  urldate = {2023-11-05},
  abstract = {Early stopping based on the validation set performance is a popular approach to find the right balance between under- and overfitting in the context of supervised learning. However, in reinforcement learning, even for supervised sub-problems such as world model learning, early stopping is not applicable as the dataset is continually evolving. As a solution, we propose a new general method that dynamically adjusts the update to data (UTD) ratio during training based on under- and overfitting detection on a small subset of the continuously collected experience not used for training. We apply our method to DreamerV2, a state-of-the-art model-based reinforcement learning algorithm, and evaluate it on the DeepMind Control Suite and the Atari 100k benchmark. The results demonstrate that one can better balance under- and overestimation by adjusting the UTD ratio with our approach compared to the default setting in DreamerV2 and that it is competitive with an extensive hyperparameter search which is not feasible for many applications. Our method eliminates the need to set the UTD hyperparameter by hand and even leads to a higher robustness with regard to other learning-related hyperparameters further reducing the amount of necessary tuning.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Dorka et al-2022 Dynamic Update-to-Data Ratio/Dorka et al_2022_Dynamic Update-to-Data Ratio.pdf}
}

@inproceedings{doroSampleEfficientReinforcementLearning2022,
  title = {Sample-{{Efficient Reinforcement Learning}} by {{Breaking}} the {{Replay Ratio Barrier}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {D'Oro, Pierluca and Schwarzer, Max and Nikishin, Evgenii and Bacon, Pierre-Luc and Bellemare, Marc G. and Courville, Aaron},
  year = {2022},
  month = sep,
  urldate = {2023-10-18},
  abstract = {Increasing the replay ratio, the number of updates of an agent's parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms. In this work, we show that fully or partially resetting the parameters of deep reinforcement learning agents causes better replay ratio scaling capabilities to emerge. We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks. We then provide an analysis of the design choices required for favorable replay ratio scaling to be possible and discuss inherent limits and tradeoffs.},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/D'Oro et al-2022 Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier/D'Oro et al_2022_Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier.pdf}
}

@inproceedings{driessPaLMEEmbodiedMultimodal2023,
  title = {{{PaLM-E}}: {{An Embodied Multimodal Language Model}}},
  shorttitle = {{{PaLM-E}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
  year = {2023},
  month = jul,
  pages = {8469--8488},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-25},
  abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multimodal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Driess et al-2023 PaLM-E/Driess et al_2023_PaLM-E.pdf}
}

@inproceedings{driessReinforcementLearningNeural2022,
  title = {Reinforcement {{Learning}} with {{Neural Radiance Fields}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Driess, Danny and Schubert, Ingmar and Florence, Pete and Li, Yunzhu and Toussaint, Marc},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {16931--16945},
  urldate = {2023-10-07},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Driess et al-2022 Reinforcement Learning with Neural Radiance Fields/Driess et al_2022_Reinforcement Learning with Neural Radiance Fields.pdf}
}

@article{duffieOverview1997a,
  title = {An {{Overview}} of {{Value}} at {{Risk}}},
  author = {Duffie, Darrell and Pan, Jun},
  year = {1997},
  month = feb,
  journal = {The Journal of Derivatives},
  volume = {4},
  number = {3},
  pages = {7--49},
  publisher = {{Institutional Investor Journals Umbrella}},
  issn = {1074-1240, 2168-8524},
  doi = {10.3905/jod.1997.407971},
  urldate = {2022-05-02},
  chapter = {Primary Article},
  copyright = {{\textcopyright} 1997 Pageant Media Ltd},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/PUPCSZR8/7.html}
}

@book{eduardof.Model2007,
  title = {Model {{Predictive Control}}},
  author = {Eduardo F., Camacho and Carlos, Bordons},
  year = {2007},
  publisher = {{Springer}}
}

@inproceedings{eeckmanSigmoid1988,
  title = {The {{Sigmoid Nonlinearity}} in {{Prepyriform Cortex}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Eeckman, Frank},
  editor = {Anderson, D.},
  year = {1988},
  publisher = {{American Institute of Physics}},
  urldate = {2021-05-12},
  file = {/Users/scannea1/Zotero/storage/HXJWREHX/60a70bb05b08d6cd95deb3bdb750dce8-Abstract.html}
}

@inproceedings{eleftheriadisIdentification2017,
  title = {Identification of {{Gaussian Process State Space Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eleftheriadis, Stefanos and Nicholson, Tom and Deisenroth, Marc and Hensman, James},
  year = {2017},
  volume = {30},
  pages = {5309--5319},
  urldate = {2020-11-26},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/5YAC3Z5R/Eleftheriadis et al. - 2017 - Identification of Gaussian Process State Space Mod.pdf;/Users/scannea1/Zotero/storage/EKVSHK87/1006ff12c465532f8c574aeaa4461b16-Abstract.html;/Users/scannea1/Zotero/storage/Y3DYE9YC/1006ff12c465532f8c574aeaa4461b16-Abstract.html}
}

@inproceedings{ertinMaximum2003,
  title = {Maximum {{Mutual Information Principle}} for {{Dynamic Sensor Query Problems}}},
  booktitle = {Information {{Processing}} in {{Sensor Networks}}},
  author = {Ertin, Emre and Fisher, John W. and Potter, Lee C.},
  editor = {Zhao, Feng and Guibas, Leonidas},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {405--416},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36978-3_27},
  abstract = {In this paper we study a dynamic sensor selection method for Bayesian filtering problems. In particular we consider the distributed Bayesian Filtering strategy given in [1] and show that the principle of mutual information maximization follows naturally from the expected uncertainty minimization criterion in a Bayesian filtering framework. This equivalence results in a computationally feasible approach to state estimation in sensor networks. We illustrate the application of the proposed dynamic sensor selection method to both discrete and linear Gaussian models for distributed tracking as well as to stationary target localization using acoustic arrays.},
  isbn = {978-3-540-36978-3},
  langid = {english},
  keywords = {Mutual Information,Sensor Measurement,Sensor Network,Sensor Node,State Space Model},
  file = {/Users/scannea1/Zotero/storage/R5MFG3MW/Ertin et al. - 2003 - Maximum Mutual Information Principle for Dynamic S.pdf}
}

@misc{eschenhagenMixturesLaplaceApproximations2021,
  title = {Mixtures of {{Laplace Approximations}} for {{Improved Post-Hoc Uncertainty}} in {{Deep Learning}}},
  author = {Eschenhagen, Runa and Daxberger, Erik and Hennig, Philipp and Kristiadi, Agustinus},
  year = {2021},
  month = nov,
  number = {arXiv:2111.03577},
  eprint = {2111.03577},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.03577},
  urldate = {2022-09-14},
  abstract = {Deep neural networks are prone to overconfident predictions on outliers. Bayesian neural networks and deep ensembles have both been shown to mitigate this problem to some extent. In this work, we aim to combine the benefits of the two approaches by proposing to predict with a Gaussian mixture model posterior that consists of a weighted sum of Laplace approximations of independently trained deep neural networks. The method can be used post hoc with any set of pre-trained networks and only requires a small computational and memory overhead compared to regular ensembles. We theoretically validate that our approach mitigates overconfidence "far away" from the training data and empirically compare against state-of-the-art baselines on standard uncertainty quantification benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Bayesian Deep Learning Workshop, NeurIPS 2021},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Eschenhagen et al-2021 Mixtures of Laplace Approximations for Improved Post-Hoc Uncertainty in Deep/Eschenhagen et al_2021_Mixtures of Laplace Approximations for Improved Post-Hoc Uncertainty in Deep.pdf;/Users/scannea1/Zotero/storage/VBCB9Q32/2111.html}
}

@inproceedings{eysenbachMismatchedNoMore2023,
  title = {Mismatched {{No More}}: {{Joint Model-Policy Optimization}} for {{Model-Based RL}}},
  shorttitle = {Mismatched {{No More}}},
  booktitle = {Deep {{RL Workshop NeurIPS}} 2021},
  author = {Eysenbach, Benjamin and Khazatsky, Alexander and Levine, Sergey and Salakhutdinov, Ruslan},
  year = {2023},
  month = may,
  urldate = {2023-06-22},
  abstract = {Many model-based reinforcement learning (RL) methods follow a similar template: fit a model to previously observed data, and then use data from that model for RL or planning. However, models that achieve better training performance (e.g., lower MSE) are not necessarily better for control: an RL agent may seek out the small fraction of states where an accurate model makes mistakes, or it might act in ways that do not expose the errors of an inaccurate model. As noted in prior work, there is an objective mismatch: models are useful if they yield good policies, but they are trained to maximize their accuracy, rather than the performance of the policies that result from them. In this work we propose a model-learning objective that directly optimizes a model to be useful for model-based RL. This objective, which depends on samples from the learned model, is a (global) lower bound on the expected return in the real environment. We jointly optimize the policy and model using this one objective, thus mending the objective mismatch in prior work. The resulting algorithm (MnM) is conceptually similar to a GAN: a classifier distinguishes between real and fake transitions, the model is updated to produce transitions that look realistic, and the policy is updated to avoid states where the model predictions are unrealistic. Our theory justifies the intuition that the best dynamics for learning a good policy are not necessarily the correct dynamics.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Eysenbach et al-2023 Mismatched No More/Eysenbach et al_2023_Mismatched No More.pdf}
}

@inproceedings{eysenbachRobust2021,
  title = {Robust {{Predictable Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Eysenbach, Ben and Salakhutdinov, Russ R and Levine, Sergey},
  year = {2021},
  volume = {34},
  pages = {27813--27825},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-11},
  file = {/Users/scannea1/Zotero/storage/AJVL6SIE/Eysenbach et al. - 2021 - Robust Predictable Control.pdf}
}

@article{faesslerDifferential2018,
  title = {Differential {{Flatness}} of {{Quadrotor Dynamics Subject}} to {{Rotor Drag}} for {{Accurate Tracking}} of {{High-Speed Trajectories}}},
  author = {Faessler, Matthias and Franchi, Antonio and Scaramuzza, Davide},
  year = {2018},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {2},
  pages = {620--626},
  issn = {2377-3766},
  doi = {10.1109/LRA.2017.2776353},
  abstract = {In this letter, we prove that the dynamical model of a quadrotor subject to linear rotor drag effects is differentially flat in its position and heading. We use this property to compute feedforward control terms directly from a reference trajectory to be tracked. The obtained feedforward terms are then used in a cascaded, nonlinear feedback control law that enables accurate agile flight with quadrotors. Compared to the state-of-the-art control methods, which treat the rotor drag as an unknown disturbance, our method reduces the trajectory tracking error significantly. Finally, we present a method based on a gradient-free optimization to identify the rotor drag coefficients, which are required to compute the feedforward control terms. The new theoretical results are thoroughly validated trough extensive comparative experiments.},
  keywords = {Acceleration,Aerial systems,Aerodynamics,Computational modeling,differential flatness,Drag,dynamics,mechanics and control,quadrotor control,Rotors,Trajectory,Trajectory tracking},
  file = {/Users/scannea1/Zotero/storage/4JIJS2IU/Faessler et al. - 2018 - Differential Flatness of Quadrotor Dynamics Subjec.pdf}
}

@inproceedings{fahrooDirect2000,
  title = {Direct Trajectory Optimization by a {{Chebyshev}} Pseudospectral Method},
  booktitle = {Proceedings of the 2000 {{American Control Conference}}},
  author = {Fahroo, F. and Ross, I. M.},
  year = {2000},
  month = jun,
  volume = {6},
  pages = {3860--3864},
  issn = {0743-1619},
  doi = {10.1109/ACC.2000.876945},
  abstract = {A Chebyshev pseudospectral method is presented in this paper for directly solving a generic optimal control problem with state and control constraints. This method employs Nth degree Lagrange polynomial approximations for the state and control variables with the values of these variables at the Chebyshev-Gauss-Lobatto (CGL) points as the expansion coefficients. This process yields a nonlinear programming problem (NLP) with the state and control values at the CGL points as unknown NLP parameters. Numerical examples demonstrate this method yields more accurate results than those obtained from the traditional collocation methods.},
  keywords = {CGL points,Chebyshev approximation,Chebyshev pseudospectral method,Chebyshev-Gauss-Lobatto points,control constraints,Cost function,Differential equations,direct trajectory optimization,Gaussian processes,high-degree Lagrange polynomial approximations,Lagrangian functions,Mathematics,Nonlinear equations,nonlinear programming,nonlinear programming problem,optimal control,Optimal control,optimal control problem,Optimization methods,path planning,Polynomials,spectral analysis,state constraints,unknown NLP parameters},
  file = {/Users/scannea1/Zotero/storage/9RXR2VPM/Fahroo and Ross - 2000 - Direct trajectory optimization by a Chebyshev pseu.pdf;/Users/scannea1/Zotero/storage/CQN2YN69/876945.html}
}

@misc{FCAIteammeetingpresentationfeb2023,
  title = {{{FCAI-team-meeting-presentation-feb-2023}}},
  journal = {Google Docs},
  urldate = {2023-02-28},
  abstract = {Long-term decision-making and transfer between tasks 1 February 2023 Presenter: Aidan Scannell},
  howpublished = {https://docs.google.com/presentation/d/1rAVTavGanmXZ8Z2QTUhaO6iNeo6A8aerbhHZY6iBqsI/edit?ouid=100959914931771490491\&usp=slides\_home\&ths=true\&usp=embed\_facebook},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3X78LYBM/edit.html}
}

@inproceedings{feinbergModelBasedValueEstimation2018,
  title = {Model-{{Based Value Estimation}} for {{Efficient Model-Free Reinforcement Learning}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, Sergey},
  year = {2018},
  eprint = {1803.00101},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1803.00101},
  urldate = {2023-11-05},
  abstract = {Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Feinberg et al-2018 Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning/Feinberg et al_2018_Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/73QWFCPP/1803.html}
}

@book{ferberGames1958,
  title = {Games and {{Decisions}}: {{Introduction}} and {{Critical Survey}}},
  shorttitle = {Games and {{Decisions}}},
  author = {Ferber, R. and Luce, R. and Raiffa, H.},
  year = {1958},
  publisher = {{Wiley New York}},
  abstract = {Semantic Scholar extracted view of "Games and Decisions: Introduction and Critical Survey" by R. Ferber et al.}
}

@inproceedings{finnDeepSpatialAutoencoders2016,
  title = {Deep Spatial Autoencoders for Visuomotor Learning},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Finn, Chelsea and Tan, Xin Yu and Duan, Yan and Darrell, Trevor and Levine, Sergey and Abbeel, Pieter},
  year = {2016},
  month = may,
  pages = {512--519},
  doi = {10.1109/ICRA.2016.7487173},
  urldate = {2023-11-02},
  abstract = {Reinforcement learning provides a powerful and flexible framework for automated acquisition of robotic motion skills. However, applying reinforcement learning requires a sufficiently detailed representation of the state, including the configuration of task-relevant objects. We present an approach that automates state-space construction by learning a state representation directly from camera images. Our method uses a deep spatial autoencoder to acquire a set of feature points that describe the environment for the current task, such as the positions of objects, and then learns a motion skill with these feature points using an efficient reinforcement learning method based on local linear models. The resulting controller reacts continuously to the learned feature points, allowing the robot to dynamically manipulate objects in the world with closed-loop control. We demonstrate our method with a PR2 robot on tasks that include pushing a free-standing toy block, picking up a bag of rice using a spatula, and hanging a loop of rope on a hook at various positions. In each task, our method automatically learns to track task-relevant objects and manipulate their configuration with the robot's arm.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Finn et al-2016 Deep spatial autoencoders for visuomotor learning/Finn et al_2016_Deep spatial autoencoders for visuomotor learning.pdf;/Users/scannea1/Zotero/storage/CK45KSHG/7487173.html}
}

@misc{finnGeneralizingSkillsSemiSupervised2017,
  title = {Generalizing {{Skills}} with {{Semi-Supervised Reinforcement Learning}}},
  author = {Finn, Chelsea and Yu, Tianhe and Fu, Justin and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = mar,
  number = {arXiv:1612.00429},
  eprint = {1612.00429},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {Deep reinforcement learning (RL) can acquire complex behaviors from low-level inputs, such as images. However, real-world applications of such methods require generalizing to the vast variability of the real world. Deep networks are known to achieve remarkable generalization when provided with massive amounts of labeled data, but can we provide this breadth of experience to an RL agent, such as a robot? The robot might continuously learn as it explores the world around it, even while deployed. However, this learning requires access to a reward function, which is often hard to measure in real-world domains, where the reward could depend on, for example, unknown positions of objects or the emotional state of the user. Conversely, it is often quite practical to provide the agent with reward functions in a limited set of situations, such as when a human supervisor is present or in a controlled setting. Can we make use of this limited supervision, and still benefit from the breadth of experience an agent might collect on its own? In this paper, we formalize this problem as semisupervised reinforcement learning, where the reward function can only be evaluated in a set of "labeled" MDPs, and the agent must generalize its behavior to the wide range of states it might encounter in a set of "unlabeled" MDPs, by using experience from both settings. Our proposed method infers the task objective in the unlabeled MDPs through an algorithm that resembles inverse RL, using the agent's own prior experience in the labeled MDPs as a kind of demonstration of optimal behavior. We evaluate our method on challenging tasks that require control directly from images, and show that our approach can improve the generalization of a learned deep neural network policy by using experience for which no reward function is available. We also show that our method outperforms direct supervised learning of the reward.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: ICLR 2017},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Finn et al-2017 Generalizing Skills with Semi-Supervised Reinforcement Learning/Finn et al_2017_Generalizing Skills with Semi-Supervised Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/3WAITKY8/1612.html}
}

@article{fortuinPriorsBayesianDeep2022,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}}},
  shorttitle = {Priors in {{Bayesian Deep Learning}}},
  author = {Fortuin, Vincent},
  year = {2022},
  journal = {International Statistical Review},
  volume = {90},
  number = {3},
  pages = {563--591},
  issn = {1751-5823},
  doi = {10.1111/insr.12502},
  urldate = {2023-09-30},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  copyright = {{\textcopyright} 2022 The Authors. International Statistical Review published by John Wiley \& Sons Ltd on behalf of International Statistical Institute.},
  langid = {english},
  keywords = {Bayesian deep learning,Bayesian learning,deep learning,priors},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Fortuin-2022 Priors in Bayesian Deep Learning/Fortuin_2022_Priors in Bayesian Deep Learning.pdf}
}

@misc{frankVisionandLanguageVisionforLanguageCrossModal2021,
  title = {Vision-and-{{Language}} or {{Vision-for-Language}}? {{On Cross-Modal Influence}} in {{Multimodal Transformers}}},
  shorttitle = {Vision-and-{{Language}} or {{Vision-for-Language}}?},
  author = {Frank, Stella and Bugliarello, Emanuele and Elliott, Desmond},
  year = {2021},
  month = sep,
  number = {arXiv:2109.04448},
  eprint = {2109.04448},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.04448},
  urldate = {2023-10-07},
  abstract = {Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: EMNLP 2021},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Frank et al-2021 Vision-and-Language or Vision-for-Language/Frank et al_2021_Vision-and-Language or Vision-for-Language.pdf;/Users/scannea1/Zotero/storage/YVHZ8PGB/2109.html}
}

@book{freemanRobust1996,
  title = {Robust {{Nonlinear Control Design}}: {{State-Space}} and {{Lyapunov Techniques}}},
  shorttitle = {Robust {{Nonlinear Control Design}}},
  author = {Freeman, Randy and Kokotovic, Petar V.},
  year = {1996},
  series = {Modern {{Birkh{\"a}user Classics}}},
  publisher = {{Birkh{\"a}user Basel}},
  doi = {10.1007/978-0-8176-4759-9},
  urldate = {2021-03-02},
  abstract = {This book presents advances in the theory and design of robust nonlinear control systems. In the first part of the book, the authors provide a unified framework for state-space and Lyapunov techniques by combining concepts from set-valued analysis, Lyapunov stability theory, and game theory. Within this unified framework, the authors then develop a variety of control design methods suitable for systems described by low-order nonlinear ordinary differential equations. Emphasis is placed on global controller designs, that is, designs for the entire region of model validity. Because linear theory deals well with local system behavior (except for critical cases in which Jacobian linearization fails), the authors focus on achieving robustness and performance for large deviations from a given operation condition. The purpose of the book is to summarize Lyapunov design techniques for nonlinear systems and to raise important issues concerning large-signal robustness and performance. The authors have been the first to address some of these issues, and they report their findings in this text. For example, they identify two potential sources of excessive control effort in Lyapunov design techniques and show how such effort can be greatly reduced. The researcher who wishes to enter the field of robust nonlinear control could use this book as a source of new research topics. For those already active in the field, the book may serve as a reference to a recent body of significant work. Finally, the design engineer faced with a nonlinear control problem will benefit from the techniques presented here. "The text is practically self-contained. The authors offer all necessary definitions and give a comprehensive introduction. Only the most basic knowledge of nonlinear analysis and design tools is required, including Lyapunov stability theory and optimal control. The authors also provide a review of set-valued maps for those readers who are not familiar with set-valued analysis. The book is intended for graduate students and researchers in control theory, serving as both a summary of recent results and a source of new research problems. In the opinion of this reviewer the authors do succeed in attaining these objectives." {\textemdash} Mathematical Reviews},
  isbn = {978-0-8176-4758-2},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GXRPXABP/9780817647582.html}
}

@book{freemanRobust2009,
  title = {Robust {{Nonlinear Control Design}}: {{State-Space}} and {{Lyapunov Techniques}}},
  shorttitle = {Robust {{Nonlinear Control Design}}},
  author = {Freeman, Randy A. and Kokotovic, Petar V.},
  year = {2009},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book presents advances in the theory and design of robust nonlinear control systems. In the first part of the book, the authors provide a unified framework for state-space and Lyapunov techniques by combining concepts from set-valued analysis, Lyapunov stability theory, and game theory. Within this unified framework, the authors then develop a variety of control design methods suitable for systems described by low-order nonlinear ordinary differential equations. Emphasis is placed on global controller designs, that is, designs for the entire region of model validity. Because linear theory deals well with local system behavior (except for critical cases in which Jacobian linearization fails), the authors focus on achieving robustness and performance for large deviations from a given operation condition. The purpose of the book is to summarize Lyapunov design techniques for nonlinear systems and to raise important issues concerning large-signal robustness and performance. The authors have been the first to address some of these issues, and they report their findings in this text. For example, they identify two potential sources of excessive control effort in Lyapunov design techniques and show how such effort can be greatly reduced. The researcher who wishes to enter the field of robust nonlinear control could use this book as a source of new research topics. For those already active in the field, the book may serve as a reference to a recent body of significant work. Finally, the design engineer faced with a nonlinear control problem will benefit from the techniques presented here. "The text is practically self-contained. The authors offer all necessary definitions and give a comprehensive introduction. Only the most basic knowledge of nonlinear analysis and design tools is required, including Lyapunov stability theory and optimal control. The authors also provide a review of set-valued maps for those readers who are not familiar with set-valued analysis. The book is intended for graduate students and researchers in control theory, serving as both a summary of recent results and a source of new research problems. In the opinion of this reviewer the authors do succeed in attaining these objectives." {\textemdash} Mathematical Reviews},
  googlebooks = {vb\_cBwAAQBAJ},
  isbn = {978-0-8176-4759-9},
  langid = {english},
  keywords = {Language Arts \& Disciplines / Library \& Information Science / General,Mathematics / Differential Equations / General,Mathematics / General,Mathematics / Linear \& Nonlinear Programming,Mathematics / Mathematical Analysis,Science / System Theory}
}

@article{frohlichCautious2020,
  title = {Cautious {{Bayesian Optimization}} for {{Efficient}} and {{Scalable Policy Search}}},
  author = {Fr{\"o}hlich, Lukas P. and Zeilinger, M. and Klenske, Edgar D.},
  year = {2020},
  journal = {undefined},
  urldate = {2021-06-01},
  abstract = {Sample efficiency is one of the key factors when applying policy search to real-world problems. In recent years, Bayesian Optimization (BO) has become prominent in the field of robotics due to its sample efficiency and little prior knowledge needed. However, one drawback of BO is its poor performance on high-dimensional search spaces as it focuses on global search. In the policy search setting, local optimization is typically sufficient as initial policies are often available, e.g., via meta-learning, kinesthetic demonstrations or sim-to-real approaches. In this paper, we propose to constrain the policy search space to a sublevel-set of the Bayesian surrogate model\&\#39;s predictive uncertainty. This simple yet effective way of constraining the policy update enables BO to scale to high-dimensional spaces (\&gt;100) as well as reduces the risk of damaging the system. We demonstrate the effectiveness of our approach on a wide range of problems, including a motor skills task, adapting deep RL agents to new reward signals and a sim-to-real task for an inverted pendulum system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/A3PDNMG4/Frhlich et al. - 2020 - Cautious Bayesian Optimization for Efficient and S.pdf;/Users/scannea1/Zotero/storage/XRS7FCMY/a6b6de5bcb1f609aed6d78d735622ab196455700.html}
}

@inproceedings{fujimotoAddressingFunctionApproximation2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor-Critic Methods}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Fujimoto, Scott and Hoof, Herke and Meger, David},
  year = {2018},
  month = jul,
  pages = {1587--1596},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-07},
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Fujimoto et al-2018 Addressing Function Approximation Error in Actor-Critic Methods/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf;/Users/scannea1/Zotero/storage/2FISEVSI/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf}
}

@inproceedings{fuLearningTaskInformed2021,
  title = {Learning {{Task Informed Abstractions}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Fu, Xiang and Yang, Ge and Agrawal, Pulkit and Jaakkola, Tommi},
  year = {2021},
  month = jul,
  pages = {3480--3491},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {Current model-based reinforcement learning methods struggle when operating from complex visual scenes due to their inability to prioritize task-relevant features. To mitigate this problem, we propose learning Task Informed Abstractions (TIA) that explicitly separates reward-correlated visual features from distractors. For learning TIA, we introduce the formalism of Task Informed MDP (TiMDP) that is realized by training two models that learn visual features via cooperative reconstruction, but one model is adversarially dissociated from the reward signal. Empirical evaluation shows that TIA leads to significant performance gains over state-of-the-art methods on many visual control tasks where natural and unconstrained visual distractions pose a formidable challenge. Project page: https://xiangfu.co/tia},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Fu et al-2021 Learning Task Informed Abstractions/Fu et al_2021_Learning Task Informed Abstractions.pdf;/Users/scannea1/Zotero/storage/CEX38UDB/Fu et al. - 2021 - Learning Task Informed Abstractions.pdf}
}

@inproceedings{gaddEnriched2020,
  title = {Enriched Mixtures of Generalised {{Gaussian}} Process Experts},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gadd, Charles and Wade, Sara and Boukouvalas, Alexis},
  year = {2020},
  month = jun,
  pages = {3144--3154},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-26},
  abstract = {Mixtures of experts probabilistically divide the input space into regions, where the assumptions of each expert, or conditional model, need only hold locally. Combined with Gaussian process (GP) experts, this results in a powerful and highly flexible model. We focus on alternative mixtures of GP experts, which  model the joint distribution of the inputs and targets explicitly. We highlight issues of this approach in multi-dimensional input spaces, namely,  poor scalability and the need for an unnecessarily large number of experts, degrading the predictive performance and increasing uncertainty. We construct a novel model to address these issues through a nested partitioning scheme that automatically infers the number of components at both levels. Multiple response types are accommodated through a generalised GP framework, while multiple input types are included through a factorised exponential family structure. We show the effectiveness of our approach in estimating a parsimonious probabilistic description of both  synthetic data of increasing dimension and an Alzheimer's challenge dataset.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/77MNIVHX/Gadd et al. - 2020 - Enriched mixtures of generalised Gaussian process .pdf;/Users/scannea1/Zotero/storage/GXCW2FXW/Gadd et al. - 2020 - Enriched mixtures of generalised Gaussian process .pdf}
}

@inproceedings{galDeepBayesianActive2017,
  title = {Deep {{Bayesian Active Learning}} with {{Image Data}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin},
  year = {2017},
  month = jul,
  pages = {1183--1192},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Gal et al-2017 Deep Bayesian Active Learning with Image Data/Gal et al_2017_Deep Bayesian Active Learning with Image Data.pdf}
}

@inproceedings{galImproving2016,
  title = {Improving {{PILCO}} with {{Bayesian Neural Network Dynamics Models}}},
  booktitle = {{{ICML Workshop}} on {{Data-Efficient Machine Learning}}},
  author = {Gal, Yarin and McAllister, Rowan and Rasmussen, Carl},
  year = {2016},
  urldate = {2022-03-09},
  abstract = {PILCO's framework is extended to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality, and it is shown that moment matching is a crucial simplifying assumption made by the model. Model-based reinforcement learning (RL) allows an agent to discover good policies with a small number of trials by generalising observed transitions. Data efficiency can be further improved with a probabilistic model of the agent's ignorance about the world, allowing it to choose actions under uncertainty. Bayesian modelling offers tools for this task, with PILCO [1] being a prominent example, achieving state-of-theart data efficiency on low dimensional RL benchmarks. But PILCO relies on Gaussian processes (GPs), which prohibits its applicability to problems that require a larger number of trials to be solved. Further, PILCO does not consider temporal correlation in model uncertainty between successive state transitions, which results in PILCO underestimating state uncertainty at future time steps [2]. In this paper we extend PILCO's framework to use Bayesian deep dynamics models with approximate variational inference, allowing PILCO to scale linearly with number of trials and observation space dimensionality. Using particle methods we sample dynamics function realisations, and obtain lower cumulative cost than PILCO. We give insights into the modelling assumptions made in PILCO, and show that moment matching is a crucial simplifying assumption made by the model. Our implementation can leverage GPU architectures, offering faster running time than PILCO, and will allow structured observation spaces to be modelled (images or higher dimensional inputs) in the future.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/7CC35VP4/127d856c8b74d3e54a2f7da7b11b784014832ed9.html}
}

@article{gargUnified2010,
  title = {A Unified Framework for the Numerical Solution of Optimal Control Problems Using Pseudospectral Methods},
  author = {Garg, Divya and Patterson, Michael and Hager, William W. and Rao, Anil V. and Benson, David A. and Huntington, Geoffrey T.},
  year = {2010},
  month = nov,
  journal = {Automatica},
  volume = {46},
  number = {11},
  pages = {1843--1851},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2010.06.048},
  urldate = {2021-03-02},
  abstract = {A unified framework is presented for the numerical solution of optimal control problems using collocation at Legendre{\textendash}Gauss (LG), Legendre{\textendash}Gauss{\textendash}Radau (LGR), and Legendre{\textendash}Gauss{\textendash}Lobatto (LGL) points. It is shown that the LG and LGR differentiation matrices are rectangular and full rank whereas the LGL differentiation matrix is square and singular. Consequently, the LG and LGR schemes can be expressed equivalently in either differential or integral form, while the LGL differential and integral forms are not equivalent. Transformations are developed that relate the Lagrange multipliers of the discrete nonlinear programming problem to the costates of the continuous optimal control problem. The LG and LGR discrete costate systems are full rank while the LGL discrete costate system is rank-deficient. The LGL costate approximation is found to have an error that oscillates about the true solution and this error is shown by example to be due to the null space in the LGL discrete costate system. An example is considered to assess the accuracy and features of each collocation scheme.},
  langid = {english},
  keywords = {Nonlinear programming,Optimal control,Pseudospectral methods},
  file = {/Users/scannea1/Zotero/storage/T9YSQEZM/Garg et al. - 2010 - A unified framework for the numerical solution of .pdf;/Users/scannea1/Zotero/storage/NSLYL44L/S0005109810002980.html}
}

@inproceedings{geladaDeepMDPLearningContinuous2019,
  title = {{{DeepMDP}}: {{Learning Continuous Latent Space Models}} for {{Representation Learning}}},
  shorttitle = {{{DeepMDP}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Gelada, Carles and Kumar, Saurabh and Buckman, Jacob and Nachum, Ofir and Bellemare, Marc G.},
  year = {2019},
  month = may,
  pages = {2170--2179},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {Many reinforcement learning (RL) tasks provide the agent with high-dimensional observations that can be simplified into low-dimensional continuous states. To formalize this process, we introduce the concept of a {\textbackslash}texit\{DeepMDP\}, a parameterized latent space model that is trained via the minimization of two tractable latent space losses: prediction of rewards and prediction of the distribution over next latent states. We show that the optimization of these objectives guarantees (1) the quality of the embedding function as a representation of the state space and (2) the quality of the DeepMDP as a model of the environment. Our theoretical findings are substantiated by the experimental result that a trained DeepMDP recovers the latent structure underlying high-dimensional observations on a synthetic environment. Finally, we show that learning a DeepMDP as an auxiliary task in the Atari 2600 domain leads to large performance improvements over model-free RL.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Gelada et al-2019 DeepMDP/Gelada et al_2019_DeepMDP.pdf}
}

@inproceedings{gelbartBayesian2014,
  title = {{Bayesian optimization with unknown constraints}},
  booktitle = {{Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014}},
  author = {Gelbart, Michael A. and Snoek, Jasper and Adams, Ryan P.},
  year = {2014},
  pages = {250--259},
  publisher = {{AUAI Press}},
  urldate = {2022-05-02},
  langid = {English (US)},
  file = {/Users/scannea1/Zotero/storage/VG2LGR3L/bayesian-optimization-with-unknown-constraints.html}
}

@inproceedings{ghahramaniLearning1999,
  title = {Learning {{Nonlinear Dynamical Systems}} Using an {{EM Algorithm}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 11},
  author = {Ghahramani, Zoubin and Roweis, Sam T.},
  year = {1999},
  pages = {599--605},
  publisher = {{MIT Press}},
  abstract = {The Expectation Maximization (EM) algorithm is an iterative procedure for maximum likelihood parameter estimation from data sets with missing or hidden variables[2]. It has been applied to system identification in linear stochastic state-space models, where the state variables are hidden from the observer and both the state and the parameters of the model have to be estimated simultaneously [9]. We present a generalization of the EM algorithm for parameter estimation in nonlinear dynamical systems. The "expectation" step makes use of Extended Kalman Smoothing to estimate the state, while the "maximization" step re-estimates the parameters using these uncertain state estimates. In general, the nonlinear maximization step is difficult because it requires integrating out the uncertainty in the states. However, if Gaussian radial basis function (RBF) approximators are used to model the nonlinearities, the integrals become tractable and the maximization step can be solved via systems of linear equations.},
  file = {/Users/scannea1/Zotero/storage/72TJ6IPJ/Ghahramani and Roweis - 1999 - Learning Nonlinear Dynamical Systems using an EM A.pdf;/Users/scannea1/Zotero/storage/MX5NPPSJ/summary.html}
}

@inproceedings{ghahramaniSwitching1996,
  title = {Switching {{State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1996},
  doi = {10.1007/978-0-387-35768-3_13},
  abstract = {We introduce a statistical model for times series data with nonlinear dynamics which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of those regimes. This model combines and generalizes two of the most widely used stochastic time series models{\textbar}the hidden Markov model and the linear dynamical system{\textbar}and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward\{backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems.\vphantom\}},
  file = {/Users/scannea1/Zotero/storage/QDGJI8XS/11-1999.pdf}
}

@techreport{ghahramaniSwitching1996a,
  title = {Switching {{State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1996},
  institution = {{King's College Road, Toronto M5S 3H5}},
  abstract = {We introduce a statistical model for non-linear time series which iteratively segments the data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time series models---the hidden Markov model and the linear dynamical system---and is related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) alogithm cannot be applied. However, we present a variational approximation which maximizes a lower bound on the log likelihood and makes use of both the forward--backward recursions for hidden Markov mo...},
  file = {/Users/scannea1/Zotero/storage/6FJPB3J2/Ghahramani and Hinton - 1996 - Switching State-Space Models.pdf;/Users/scannea1/Zotero/storage/8GP9X52Q/download.html}
}

@article{ghahramaniVariational1998,
  title = {Variational Learning for Switching State-Space Models},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {1998},
  journal = {Neural Computation},
  volume = {12},
  pages = {963--996},
  abstract = {We introduce a new statistical model for time series which iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time series models -- hidden Markov models and linear dynamical systems -- and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs et al., 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact Expectation Maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log likelihood and makes use of both the forward-backward recursions for hidden Markov models and the Kalman lter recursions for linear dynamical systems. We tested the algorithm both on artificial data sets and on a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
  file = {/Users/scannea1/Zotero/storage/D9J5HK5G/Ghahramani and Hinton - 1998 - Variational learning for switching state-space mod.pdf;/Users/scannea1/Zotero/storage/P9IELXW9/summary.html}
}

@article{ghahramaniVariational2000,
  title = {Variational {{Learning}} for {{Switching State-Space Models}}},
  author = {Ghahramani, Zoubin and Hinton, Geoffrey E.},
  year = {2000},
  month = apr,
  journal = {Neural Computation},
  volume = {12},
  number = {4},
  pages = {831--864},
  issn = {0899-7667},
  doi = {10.1162/089976600300015619},
  abstract = {We introduce a new statistical model for time series that iteratively segments data into regimes with approximately linear dynamics and learns the parameters of each of these linear regimes. This model combines and generalizes two of the most widely used stochastic time-series models{\textemdash}hidden Markov models and linear dynamical systems{\textemdash}and is closely related to models that are widely used in the control and econometrics literatures. It can also be derived by extending the mixture of experts neural network (Jacobs, Jordan, Nowlan, \& Hinton, 1991) to its fully dynamical version, in which both expert and gating networks are recurrent. Inferring the posterior probabilities of the hidden states of this model is computationally intractable, and therefore the exact expectation maximization (EM) algorithm cannot be applied. However, we present a variational approximation that maximizes a lower bound on the log-likelihood and makes use of both the forward and backward recursions for hidden Markov models and the Kalman filter recursions for linear dynamical systems. We tested the algorithm on artificial data sets and a natural data set of respiration force from a patient with sleep apnea. The results suggest that variational approximations are a viable method for inference and learning in switching state-space models.},
  file = {/Users/scannea1/Zotero/storage/RW4JB25F/6789465.html}
}

@inproceedings{ghasemipourWhyPessimisticEstimating2022,
  title = {Why {{So Pessimistic}}? {{Estimating Uncertainties}} for {{Offline RL}} through {{Ensembles}}, and {{Why Their Independence Matters}}},
  shorttitle = {Why {{So Pessimistic}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ghasemipour, Kamyar and Gu, Shixiang (Shane) and Nachum, Ofir},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {18267--18281},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ghasemipour et al-2022 Why So Pessimistic/Ghasemipour et al_2022_Why So Pessimistic.pdf}
}

@inproceedings{ghoshRepresentationsStableOffPolicy2020,
  title = {Representations for {{Stable Off-Policy Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Ghosh, Dibya and Bellemare, Marc G.},
  year = {2020},
  month = nov,
  pages = {3556--3565},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {Reinforcement learning with function approximation can be unstable and even divergent, especially when combined with off-policy learning and Bellman updates. In deep reinforcement learning, these issues have been dealt with empirically by adapting and regularizing the representation, in particular with auxiliary tasks. This suggests that representation learning may provide a means to guarantee stability. In this paper, we formally show that there are indeed nontrivial state representations under which the canonical SARSA algorithm is stable, even when learning off-policy. We analyze representation learning schemes that are based on the transition matrix of a policy, such as proto-value functions, along three axes: approximation error, stability, and ease of estimation. In the most general case of a defective transition matrix, we show that a Schur basis provides convergence guarantees, but is difficult to estimate from samples. For a fixed reward function, we find that an orthogonal basis of the corresponding Krylov subspace is an even better choice. We conclude by empirically demonstrating that these stable representations can be learned using stochastic gradient descent, opening the door to improved techniques for representation learning with deep networks.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ghosh_Bellemare-2020 Representations for Stable Off-Policy Reinforcement Learning/Ghosh_Bellemare_2020_Representations for Stable Off-Policy Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/QIJM6QR5/Ghosh and Bellemare - 2020 - Representations for Stable Off-Policy Reinforcemen.pdf}
}

@misc{ghugareSimplifyingModelbasedRL2022,
  title = {Simplifying {{Model-based RL}}: {{Learning Representations}}, {{Latent-space Models}}, and {{Policies}} with {{One Objective}}},
  shorttitle = {Simplifying {{Model-based RL}}},
  author = {Ghugare, Raj and Bharadhwaj, Homanga and Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  year = {2022},
  month = sep,
  number = {arXiv:2209.08466},
  eprint = {2209.08466},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.08466},
  urldate = {2022-11-18},
  abstract = {While reinforcement learning (RL) methods that learn an internal model of the environment have the potential to be more sample efficient than their model-free counterparts, learning to model raw observations from high dimensional sensors can be challenging. Prior work has addressed this challenge by learning low-dimensional representation of observations through auxiliary objectives, such as reconstruction or value prediction. However, the alignment between these auxiliary objectives and the RL objective is often unclear. In this work, we propose a single objective which jointly optimizes a latent-space model and policy to achieve high returns while remaining self-consistent. This objective is a lower bound on expected returns. Unlike prior bounds for model-based RL on policy exploration or model guarantees, our bound is directly on the overall RL objective. We demonstrate that the resulting algorithm matches or improves the sample-efficiency of the best prior model-based and model-free RL methods. While such sample efficient methods typically are computationally demanding, our method attains the performance of SAC in about 50{\textbackslash}\% less wall-clock time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 9 pages (without references and appendix), 17 figures, 25 Pages (total), Project website with code: {\textbackslash}url\{https://alignedlatentmodels.github.io/\}},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ghugare et al-2022 Simplifying Model-based RL/Ghugare et al_2022_Simplifying Model-based RL.pdf;/Users/scannea1/Zotero/storage/VITWFA8M/2209.html}
}

@phdthesis{girardApproximate2004,
  title = {Approximate {{Methods}} for {{Propagation}} of {{Uncertainty}} with {{Gaussian Process Models}}},
  author = {Girard, Agathe},
  year = {2004},
  abstract = {This thesis presents extensions of the Gaussian Process (GP) model, based on approximate methods allowing the model to deal with input uncertainty. Zero-mean GPs with Gaussian covariance function are of particular interest, as they allow to carry out many derivations exactly, as well as having been shown to have modelling abilities and predictive performance comparable to that of neural networks (Rasmussen, 1996a). With this model, given observed data and a new input, making a prediction corresponds to computing the (Gaussian) predictive distribution of the associated output, whose mean can be used as an estimate. This way, the predictive variance provides error-bars or confidence intervals on this estimate: It quantifies the model's degree of belief in its `best guess'. Using the knowledge of the predictive variance in an informative manner is at the centre of this thesis, as the problems of how to propagate it in the model, how to account for it when derivative observations are available, and how to derive a control law with a cautious behaviour are addressed.},
  langid = {english},
  school = {University of Glasgow},
  file = {/Users/scannea1/Zotero/storage/HQFVDXPM/Girard - Approximate Methods for Propagation of Uncertainty.pdf}
}

@incollection{girardGaussian2003,
  title = {Gaussian {{Process}} Priors with Uncertain Inputs? {{Application}} to Multiple-Step Ahead Time Series Forecasting},
  booktitle = {Becker, {{S}}},
  author = {Girard, Agathe and Rasmussen, Carl Edward and {Quinonero-Candela}, Joaquin and {Murray-Smith}, Roderick},
  year = {2003},
  publisher = {{MIT Press}},
  address = {{Vancouver, Canada}},
  urldate = {2021-09-24},
  abstract = {We consider the problem of multi-step ahead prediction in time series analysis using the non-parametric Gaussian process model. k-step ahead forecasting of a discrete-time non-linear dynamic system can be performed by doing repeated one-step ahead predictions. For a state-space model of the form y t = f(Yt-1 ,..., Yt-L ), the prediction of y at time t + k is based on the point estimates of the previous outputs. In this paper, we show how, using an analytical Gaussian approximation, we can formally incorporate the uncertainty about intermediate regressor values, thus updating the uncertainty on the current prediction.},
  isbn = {978-0-262-02550-8},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/2RIMIIAU/Girard et al. - 2003 - Gaussian Process priors with uncertain inputs App.pdf;/Users/scannea1/Zotero/storage/36RXX8DX/3117.html}
}

@inproceedings{gonzalezGLASSES2015,
  title = {{{GLASSES}}: {{Relieving The Myopia Of Bayesian Optimisation}}},
  shorttitle = {{{GLASSES}}},
  booktitle = {Proceedings of the {{Nineteenth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Gonz{\'a}lez, Javier and Osborne, Michael and Lawrence, Neil D.},
  year = {2015},
  month = oct,
  eprint = {1510.06299},
  urldate = {2021-01-21},
  abstract = {We present GLASSES: Global optimisation with Look-Ahead through Stochastic Simulation and Expected-loss Search. The majority of global optimisation approaches in use are myopic, in only considering the impact of the next function value; the non-myopic approaches that do exist are able to consider only a handful of future evaluations. Our novel algorithm, GLASSES, permits the consideration of dozens of evaluations into the future. This is done by approximating the ideal look-ahead loss function, which is expensive to evaluate, by a cheaper alternative in which the future steps of the algorithm are simulated beforehand. An Expectation Propagation algorithm is used to compute the expected value of the loss.We show that the far-horizon planning thus enabled leads to substantive performance gains in empirical tests.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Machine Learning},
  note = {Comment: 12 pages, 9 figures},
  file = {/Users/scannea1/Zotero/storage/QFYN2BAH/Gonzlez et al. - 2015 - GLASSES Relieving The Myopia Of Bayesian Optimisa.pdf;/Users/scannea1/Zotero/storage/LVQ7D5AY/1510.html}
}

@article{GPflow2017,
  title = {{{GPflow}}: {{A Gaussian}} Process Library Using {{TensorFlow}}},
  author = {Matthews, Alexander G. de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, {\relax Keisuke}. and Boukouvalas, Alexis and {Le{\'o}n-Villagr{\'a}}, Pablo and Ghahramani, Zoubin and Hensman, James},
  year = {2017},
  month = apr,
  journal = {Journal of Machine Learning Research},
  volume = {18},
  number = {40},
  pages = {1--6}
}

@inproceedings{gregorTemporalDifferenceVariational2018,
  title = {Temporal {{Difference Variational Auto-Encoder}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  year = {2018},
  month = sep,
  urldate = {2023-11-02},
  abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Gregor et al-2018 Temporal Difference Variational Auto-Encoder/Gregor et al_2018_Temporal Difference Variational Auto-Encoder.pdf}
}

@article{ha2018recurrent,
	title={Recurrent world models facilitate policy evolution},
	author={Ha, David and Schmidhuber, J{\"u}rgen},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}

@inproceedings{haarnojaSoft2018,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jul,
  pages = {1861--1870},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-08},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major cha...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8HLAHQGR/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf;/Users/scannea1/Zotero/storage/SVVM4X8W/haarnoja18b.html}
}

@inproceedings{haDreamGeneralizeZeroShot2023,
  title = {Dream to {{Generalize}}: {{Zero-Shot Model-Based Reinforcement Learning}} for {{Unseen Visual Distractions}}},
  shorttitle = {Dream to {{Generalize}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Ha, Jeongsoo and Kim, Kyungsoo and Kim, Yusung},
  year = {2023},
  month = jun,
  volume = {37},
  pages = {7802--7810},
  doi = {10.1609/aaai.v37i6.25945},
  urldate = {2023-09-30},
  abstract = {Model-based reinforcement learning (MBRL) has been used to efficiently solve vision-based control tasks in high-dimensional image observations. Although recent MBRL algorithms perform well in trained observations, they fail when faced with visual distractions in observations. These task-irrelevant distractions (e.g., clouds, shadows, and light) may be constantly present in real-world scenarios. In this study, we propose a novel self-supervised method, Dream to Generalize (Dr. G), for zero-shot MBRL. Dr. G trains its encoder and world model with dual contrastive learning which efficiently captures task-relevant features among multi-view data augmentations. We also introduce a recurrent state inverse dynamics model that helps the world model to better understand the temporal structure. The proposed methods can enhance the robustness of the world model against visual distractions. To evaluate the generalization performance, we first train Dr. G on simple backgrounds and then test it on complex natural video backgrounds in the DeepMind Control suite, and the randomizing environments in Robosuite. Dr. G yields a performance improvement of 117\% and 14\% over prior works, respectively. Our code is open-sourced and available at https://github.com/JeongsooHa/DrG.git},
  copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {ROB: Learning \& Optimization for ROB},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ha et al-2023 Dream to Generalize/Ha et al_2023_Dream to Generalize.pdf}
}

@inproceedings{haesaerts.Datadriven2015,
  title = {Data-Driven and Model-Based Verification: A {{Bayesian}} Identification Approach},
  shorttitle = {Data-Driven and Model-Based Verification},
  booktitle = {Proceedings of the {{Conference}} on {{Decision}} and {{Control}}, 15-18 {{December}} 2015, {{Osaka}}, {{Japan}}},
  author = {{Haesaert, S.} and {van den Hof, P.M.J.} and {Abate, A.} and {Control Systems} and {Dynamic Networks: Data-Driven Modeling and Control} and {Formal methods for control of cyber-physical systems}},
  year = {2015},
  month = sep,
  pages = {6830--6835},
  publisher = {{Institute of Electrical and Electronics Engineers}},
  doi = {10.1109/cdc.2015.7403295},
  urldate = {2020-12-13},
  abstract = {This work develops a measurement-driven and model-based formal verification approach, applicable to systems with partly unknown dynamics. We provide a principled method, grounded on reachability analysis and on Bayesian inference, to compute the confidence that a physical system driven by external inputs and accessed under noisy measurements, verifies a temporal logic property. A case study is discussed, where we investigate the bounded- and unbounded-time safety of a partly unknown linear time invariant system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/DML32MZN/Haesaert, S. et al. - 2015 - Data-driven and model-based verification a Bayesi.pdf}
}

@misc{hafnerDeepHierarchicalPlanning2022,
  title = {Deep {{Hierarchical Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lee, Kuang-Huei and Fischer, Ian and Abbeel, Pieter},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04114},
  eprint = {2206.04114},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {Intelligent agents need to select long sequences of actions to solve complex tasks. While humans easily break down tasks into subgoals and reach them through millions of muscle commands, current artificial intelligence is limited to tasks with horizons of a few hundred decisions, despite large compute budgets. Research on hierarchical reinforcement learning aims to overcome this limitation but has proven to be challenging, current methods rely on manually specified goal spaces or subtasks, and no general solution exists. We introduce Director, a practical method for learning hierarchical behaviors directly from pixels by planning inside the latent space of a learned world model. The high-level policy maximizes task and exploration rewards by selecting latent goals and the low-level policy learns to achieve the goals. Despite operating in latent space, the decisions are interpretable because the world model can decode goals into images for visualization. Director outperforms exploration methods on tasks with sparse rewards, including 3D maze traversal with a quadruped robot from an egocentric camera and proprioception, without access to the global position or top-down view that was used by prior work. Director also learns successful behaviors across a wide range of environments, including visual control, Atari games, and DMLab levels.},
  archiveprefix = {arxiv},
  keywords = {\_tablet,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  note = {Comment: Website: https://danijar.com/director},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hafner et al-2022 Deep Hierarchical Planning from Pixels/Hafner et al_2022_Deep Hierarchical Planning from Pixels.pdf;/Users/scannea1/Zotero/storage/YHQKECUK/2206.html}
}

@inproceedings{hafnerLearning2019,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2019},
  month = may,
  pages = {2555--2565},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-02-02},
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the w...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/36E8ER7E/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf;/Users/scannea1/Zotero/storage/HB9RTJLG/hafner19a.html}
}

@inproceedings{hafnerMasteringAtariDiscrete2022,
  title = {Mastering {{Atari}} with {{Discrete World Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hafner, Danijar and Lillicrap, Timothy P. and Norouzi, Mohammad and Ba, Jimmy},
  year = {2022},
  month = feb,
  urldate = {2022-10-26},
  abstract = {Intelligent agents need to generalize from past experience to achieve goals in complex environments. World models facilitate such generalization and allow learning behaviors from imagined outcomes to increase sample-efficiency. While learning world models from image inputs has recently become feasible for some tasks, modeling Atari games accurately enough to derive successful behaviors has remained an open challenge for many years. We introduce DreamerV2, a reinforcement learning agent that learns behaviors purely from predictions in the compact latent space of a powerful world model. The world model uses discrete representations and is trained separately from the policy. DreamerV2 constitutes the first agent that achieves human-level performance on the Atari benchmark of 55 tasks by learning behaviors inside a separately trained world model. With the same computational budget and wall-clock time, Dreamer V2 reaches 200M frames and surpasses the final performance of the top single-GPU agents IQN and Rainbow. DreamerV2 is also applicable to tasks with continuous actions, where it learns an accurate world model of a complex humanoid robot and solves stand-up and walking from only pixel inputs.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hafner et al-2022 Mastering Atari with Discrete World Models/Hafner et al_2022_Mastering Atari with Discrete World Models.pdf;/Users/scannea1/Zotero/storage/S86WW9BM/forum.html}
}

@article{hafner2023mastering,
	title={Mastering diverse domains through world models},
	author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
	journal={arXiv preprint arXiv:2301.04104},
	year={2023}
}

@misc{hamrickRolePlanningModelbased2021a,
  title = {On the Role of Planning in Model-Based Deep Reinforcement Learning},
  author = {Hamrick, Jessica B. and Friesen, Abram L. and Behbahani, Feryal and Guez, Arthur and Viola, Fabio and Witherspoon, Sims and Anthony, Thomas and Buesing, Lars and Veli{\v c}kovi{\'c}, Petar and Weber, Th{\'e}ophane},
  year = {2021},
  month = mar,
  number = {arXiv:2011.04021},
  eprint = {2011.04021},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.04021},
  urldate = {2023-09-29},
  abstract = {Model-based planning is often thought to be necessary for deep, careful reasoning and generalization in artificial agents. While recent successes of model-based reinforcement learning (MBRL) with deep function approximation have strengthened this hypothesis, the resulting diversity of model-based methods has also made it difficult to track which components drive success and why. In this paper, we seek to disentangle the contributions of recent methods by focusing on three questions: (1) How does planning benefit MBRL agents? (2) Within planning, what choices drive performance? (3) To what extent does planning improve generalization? To answer these questions, we study the performance of MuZero (Schrittwieser et al., 2019), a state-of-the-art MBRL algorithm with strong connections and overlapping components with many other MBRL algorithms. We perform a number of interventions and ablations of MuZero across a wide range of environments, including control tasks, Atari, and 9x9 Go. Our results suggest the following: (1) Planning is most useful in the learning process, both for policy updates and for providing a more useful data distribution. (2) Using shallow trees with simple Monte-Carlo rollouts is as performant as more complex methods, except in the most difficult reasoning tasks. (3) Planning alone is insufficient to drive strong generalization. These results indicate where and how to utilize planning in reinforcement learning settings, and highlight a number of open questions for future MBRL research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Published at ICLR 2021},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hamrick et al-2021 On the role of planning in model-based deep reinforcement learning/false;/Users/scannea1/Zotero/storage/KFTCDZW2/2011.html}
}

@inproceedings{hansenTemporalDifferenceLearning2022,
  title = {Temporal {{Difference Learning}} for {{Model Predictive Control}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Hansen, Nicklas A. and Su, Hao and Wang, Xiaolong},
  year = {2022},
  month = jun,
  pages = {8387--8406},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {Data-driven model predictive control has two key advantages over model-free methods: a potential for improved sample efficiency through model learning, and better performance as computational budget for planning increases. However, it is both costly to plan over long horizons and challenging to obtain an accurate model of the environment. In this work, we combine the strengths of model-free and model-based methods. We use a learned task-oriented latent dynamics model for local trajectory optimization over a short horizon, and use a learned terminal value function to estimate long-term return, both of which are learned jointly by temporal difference learning. Our method, TD-MPC, achieves superior sample efficiency and asymptotic performance over prior work on both state and image-based continuous control tasks from DMControl and Meta-World. Code and videos are available at https://nicklashansen.github.io/td-mpc.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hansen et al-2022 Temporal Difference Learning for Model Predictive Control/Hansen et al_2022_Temporal Difference Learning for Model Predictive Control.pdf}
}

@article{hansen2023td,
	title={Td-mpc2: Scalable, robust world models for continuous control},
	author={Hansen, Nicklas and Su, Hao and Wang, Xiaolong},
	journal={arXiv preprint arXiv:2310.16828},
	year={2023}
}

@misc{haoRegret2022,
  title = {Regret {{Bounds}} for {{Information-Directed Reinforcement Learning}}},
  author = {Hao, Botao and Lattimore, Tor},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04640},
  eprint = {2206.04640},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2022-06-25},
  abstract = {Information-directed sampling (IDS) has revealed its potential as a data-efficient algorithm for reinforcement learning (RL). However, theoretical understanding of IDS for Markov Decision Processes (MDPs) is still limited. We develop novel information-theoretic tools to bound the information ratio and cumulative information gain about the learning target. Our theoretical results shed light on the importance of choosing the learning target such that the practitioners can balance the computation and regret bounds. As a consequence, we derive prior-free Bayesian regret bounds for vanilla-IDS which learns the whole environment under tabular finite-horizon MDPs. In addition, we propose a computationally-efficient regularized-IDS that maximizes an additive form rather than the ratio form and show that it enjoys the same regret bound as vanilla-IDS. With the aid of rate-distortion theory, we improve the regret bound by learning a surrogate, less informative environment. Furthermore, we extend our analysis to linear MDPs and prove similar regret bounds for Thompson sampling as a by-product.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/scannea1/Zotero/storage/4CL6EINE/Hao and Lattimore - 2022 - Regret Bounds for Information-Directed Reinforceme.pdf;/Users/scannea1/Zotero/storage/8435UCVT/2206.html}
}

@inproceedings{haRecurrentWorldModels2018,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-10-25},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ha_Schmidhuber-2018 Recurrent World Models Facilitate Policy Evolution/Ha_Schmidhuber_2018_Recurrent World Models Facilitate Policy Evolution.pdf}
}

@inproceedings{haRecurrentWorldModels2018a,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-02},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ha_Schmidhuber-2018 Recurrent World Models Facilitate Policy Evolution/Ha_Schmidhuber_2018_Recurrent World Models Facilitate Policy Evolution2.pdf}
}

@inproceedings{haubergGeometric2012,
  title = {A {{Geometric}} Take on {{Metric Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hauberg, S{\o}ren and Freifeld, Oren and Black, Michael J},
  year = {2012},
  pages = {9},
  abstract = {Multi-metric learning techniques learn local metric tensors in different parts of a feature space. With such an approach, even simple classifiers can be competitive with the state-of-the-art because the distance measure locally adapts to the structure of the data. The learned distance measure is, however, non-metric, which has prevented multi-metric learning from generalizing to tasks such as dimensionality reduction and regression in a principled way. We prove that, with appropriate changes, multi-metric learning corresponds to learning the structure of a Riemannian manifold. We then show that this structure gives us a principled way to perform dimensionality reduction and regression according to the learned metrics. Algorithmically, we provide the first practical algorithm for computing geodesics according to the learned metrics, as well as algorithms for computing exponential and logarithmic maps on the Riemannian manifold. Together, these tools let many Euclidean algorithms take advantage of multi-metric learning. We illustrate the approach on regression and dimensionality reduction tasks that involve predicting measurements of the human body from shape data.},
  langid = {english},
  keywords = {geodesics,geometric-learning,riemannian},
  file = {/Users/scannea1/Zotero/storage/JSWHM5S9/Hauberg et al. - A Geometric take on Metric Learning.pdf}
}

@inproceedings{heDeep2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jun,
  pages = {770--778},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8{\texttimes} deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  keywords = {Complexity theory,Degradation,Image recognition,Image segmentation,Neural networks,Training,Visualization},
  file = {/Users/scannea1/Zotero/storage/6HAE7E65/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/Users/scannea1/Zotero/storage/YV6ZAGER/citations.html}
}

@misc{heDiffusionModelEffective2023,
  title = {Diffusion {{Model}} Is an {{Effective Planner}} and {{Data Synthesizer}} for {{Multi-Task Reinforcement Learning}}},
  author = {He, Haoran and Bai, Chenjia and Xu, Kang and Yang, Zhuoran and Zhang, Weinan and Wang, Dong and Zhao, Bin and Li, Xuelong},
  year = {2023},
  month = oct,
  number = {arXiv:2305.18459},
  eprint = {2305.18459},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18459},
  urldate = {2023-11-21},
  abstract = {Diffusion models have demonstrated highly-expressive generative capabilities in vision and NLP. Recent studies in reinforcement learning (RL) have shown that diffusion models are also powerful in modeling complex policies or trajectories in offline datasets. However, these works have been limited to single-task settings where a generalist agent capable of addressing multi-task predicaments is absent. In this paper, we aim to investigate the effectiveness of a single diffusion model in modeling large-scale multi-task offline data, which can be challenging due to diverse and multimodal data distribution. Specifically, we propose Multi-Task Diffusion Model ({\textbackslash}textsc\{MTDiff\}), a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings. {\textbackslash}textsc\{MTDiff\} leverages vast amounts of knowledge available in multi-task data and performs implicit knowledge sharing among tasks. For generative planning, we find {\textbackslash}textsc\{MTDiff\} outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D. For data synthesis, {\textbackslash}textsc\{MTDiff\} generates high-quality data for testing tasks given a single demonstration as a prompt, which enhances the low-quality datasets for even unseen tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Accepted by NeurIPS 2023. 22 pages},
  file = {/Users/scannea1/Zotero/storage/4PVWA5V3/He et al. - 2023 - Diffusion Model is an Effective Planner and Data S.pdf;/Users/scannea1/Zotero/storage/LIEVSPB3/2305.html}
}

@inproceedings{heessLearningContinuousControl2015,
  title = {Learning {{Continuous Control Policies}} by {{Stochastic Value Gradients}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-09-22},
  abstract = {We present a unified framework for learning continuous control policies usingbackpropagation. It supports stochastic control by treating stochasticity in theBellman equation as a deterministic function of exogenous noise. The productis a spectrum of general policy gradient algorithms that range from model-freemethods with value functions to model-based methods without value functions.We use learned models but only require observations from the environment insteadof observations from model-predicted trajectories, minimizing the impactof compounded model errors. We apply these algorithms first to a toy stochasticcontrol problem and then to several physics-based control problems in simulation.One of these variants, SVG(1), shows the effectiveness of learning models, valuefunctions, and policies simultaneously in continuous domains.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Heess et al-2015 Learning Continuous Control Policies by Stochastic Value Gradients/Heess et al_2015_Learning Continuous Control Policies by Stochastic Value Gradients.pdf}
}

@misc{hegdeGeneratingBehaviorallyDiverse2023,
  title = {Generating {{Behaviorally Diverse Policies}} with {{Latent Diffusion Models}}},
  author = {Hegde, Shashank and Batra, Sumeet and Zentner, K. R. and Sukhatme, Gaurav S.},
  year = {2023},
  month = jun,
  number = {arXiv:2305.18738},
  eprint = {2305.18738},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18738},
  urldate = {2023-11-21},
  abstract = {Recent progress in Quality Diversity Reinforcement Learning (QD-RL) has enabled learning a collection of behaviorally diverse, high performing policies. However, these methods typically involve storing thousands of policies, which results in high space-complexity and poor scaling to additional behaviors. Condensing the archive into a single model while retaining the performance and coverage of the original collection of policies has proved challenging. In this work, we propose using diffusion models to distill the archive into a single generative model over policy parameters. We show that our method achieves a compression ratio of 13x while recovering 98\% of the original rewards and 89\% of the original coverage. Further, the conditioning mechanism of diffusion models allows for flexibly selecting and sequencing behaviors, including using language. Project website: https://sites.google.com/view/policydiffusion/home},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/MICAUQUU/Hegde et al. - 2023 - Generating Behaviorally Diverse Policies with Late.pdf;/Users/scannea1/Zotero/storage/J62HFKP4/2305.html}
}

@article{hehnRealTime2015,
  title = {Real-{{Time Trajectory Generation}} for {{Quadrocopters}}},
  author = {Hehn, Markus and D'Andrea, Raffaello},
  year = {2015},
  month = aug,
  journal = {IEEE Transactions on Robotics},
  volume = {31},
  number = {4},
  pages = {877--892},
  issn = {1941-0468},
  doi = {10.1109/TRO.2015.2432611},
  abstract = {This paper presents a trajectory generation algorithm that efficiently computes high-performance flight trajectories that are capable of moving a quadrocopter from a large class of initial states to a given target point that will be reached at rest. The approach consists of planning separate trajectories in each of the three translational degrees of freedom, and ensuring feasibility by deriving decoupled constraints for each degree of freedom through approximations that preserve feasibility. The presented algorithm can compute a feasible trajectory within tens of microseconds on a laptop computer; remaining computation time can be used to iteratively improve the trajectory. By replanning the trajectory at a high rate, the trajectory generator can be used as an implicit feedback law similar to model predictive control. The solutions generated by the algorithm are analyzed by comparing them with time-optimal motions, and experimental results validate the approach.},
  keywords = {Acceleration,Heuristic algorithms,Motion control,optimal control,quadrocopter,Real-time systems,Robots,Trajectory,trajectory generation,unmanned aerial vehicles,Vehicle dynamics,Vehicles},
  file = {/Users/scannea1/Zotero/storage/XM4Y3DUK/Hehn and DAndrea - 2015 - Real-Time Trajectory Generation for Quadrocopters.pdf;/Users/scannea1/Zotero/storage/LUYRASXX/7128399.html}
}

@inproceedings{heMaskedAutoencodersAre2022,
  title = {Masked {{Autoencoders Are Scalable Vision Learners}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  year = {2022},
  pages = {16000--16009},
  urldate = {2023-10-09},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/He et al-2022 Masked Autoencoders Are Scalable Vision Learners/He et al_2022_Masked Autoencoders Are Scalable Vision Learners.pdf}
}

@article{hennigEntropy2012,
  title = {Entropy {{Search}} for {{Information-Efficient Global Optimization}}},
  author = {Hennig, Philipp and Schuler, Christian J.},
  year = {2012},
  journal = {Journal of Machine Learning Research},
  volume = {13},
  pages = {1809--1837},
  urldate = {2021-05-12},
  abstract = {Contemporary global optimization algorithms are based on local measures of utility, rather than a probability measure over location and value of the optimum. They thus attempt to collect low function values, not to learn about the optimum. The reason for the absence of probabilistic global optimizers is that the corresponding inference problem is intractable in several ways. This paper develops desiderata for probabilistic optimization algorithms, then presents a concrete algorithm which addresses each of the computational intractabilities with a sequence of approximations and explicitly addresses the decision problem of maximizing information gain from each evaluation.},
  file = {/Users/scannea1/Zotero/storage/9HLX5WWP/Hennig and Schuler - 2012 - Entropy Search for Information-Efficient Global Op.pdf}
}

@article{hennigProbabilistic2015,
  title = {Probabilistic Numerics and Uncertainty in Computations},
  author = {Hennig, Philipp and Osborne, Michael A. and Girolami, Mark},
  year = {2015},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {471},
  number = {2179},
  pages = {20150142},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2015.0142},
  urldate = {2021-08-31},
  abstract = {We deliver a call to arms for probabilistic numerical methods: algorithms for numerical tasks, including linear algebra, integration, optimization and solving differential equations, that return uncertainties in their calculations. Such uncertainties, arising from the loss of precision induced by numerical calculation with limited time or hardware, are important for much contemporary science and industry. Within applications such as climate science and astrophysics, the need to make decisions on the basis of computations with large and complex data have led to a renewed focus on the management of numerical uncertainty. We describe how several seminal classic numerical methods can be interpreted naturally as probabilistic inference. We then show that the probabilistic view suggests new algorithms that can flexibly be adapted to suit application specifics, while delivering improved empirical performance. We provide concrete illustrations of the benefits of probabilistic numeric algorithms on real scientific problems from astrometry and astronomical imaging, while highlighting open problems with these new algorithms. Finally, we describe how probabilistic numerical methods provide a coherent framework for identifying the uncertainty in calculations performed with a combination of numerical algorithms (e.g. both numerical optimizers and differential equation solvers), potentially allowing the diagnosis (and control) of error sources in computations.},
  keywords = {inference,numerical methods,probability,statistics},
  file = {/Users/scannea1/Zotero/storage/3PDGNY4K/Hennig et al. - 2015 - Probabilistic numerics and uncertainty in computat.pdf}
}

@inproceedings{hensmanGaussian2013,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  booktitle = {Proceedings of the 29th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  year = {2013},
  volume = {29},
  pages = {282--290},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  langid = {english},
  keywords = {gaussian-processes,sparse-gaussian-processes,variational-inference},
  file = {/Users/scannea1/Zotero/storage/KLY9PMZH/Hensman et al. - Gaussian Processes for Big Data.pdf}
}

@inproceedings{hensmanScalable2015,
  title = {Scalable {{Variational Gaussian Process Classification}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Hensman, James and Matthews, Alexander and Ghahramani, Zoubin},
  year = {2015},
  month = feb,
  pages = {351--360},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-02-02},
  abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/9HDJVCTJ/Hensman et al. - 2015 - Scalable Variational Gaussian Process Classificati.pdf;/Users/scannea1/Zotero/storage/FD6TZPEI/hensman15.html}
}

@article{herzallahPMAC2020,
  title = {{{PMAC}}: Probabilistic Multimodality Adaptive Control},
  shorttitle = {{{PMAC}}},
  author = {Herzallah, Randa and Lowe, David},
  year = {2020},
  month = jul,
  journal = {International Journal of Control},
  volume = {93},
  number = {7},
  pages = {1637--1650},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179.2018.1523567},
  urldate = {2021-03-02},
  abstract = {This paper develops a probabilistic multimodal adaptive control approach for systems that are characterised by temporal multimodality where the system dynamics are subject to abrupt mode switching at arbitrary times. In this framework, the control objective is redefined such that it utilises the complete probability distribution of the system dynamics. The derived probabilistic control law is thus of a dual type that incorporates the functional uncertainty of the controlled system. A multi-modal density model with prediction error-dependent mixing coefficients is introduced to effect the mode switching. This approach can deal with arbitrary noise distributions, nonlinear plant dynamics and arbitrary mode switching. For the affine systems focussed upon for illustration in this paper the approach has global stability. The theoretical architecture constructs are verified by validation on a simulation example.},
  keywords = {Multi-modal density model,multiobjective probabilistic control,operator Riccati equation,probabilistic multimodal adaptive control,switching control,temporal multimodality},
  file = {/Users/scannea1/Zotero/storage/SKYIVLD6/Herzallah and Lowe - 2020 - PMAC probabilistic multimodality adaptive control.pdf;/Users/scannea1/Zotero/storage/4Z7VFDS9/00207179.2018.html}
}

@article{hewingCautious2020,
  title = {Cautious {{Model Predictive Control Using Gaussian Process Regression}}},
  author = {Hewing, Lukas and Kabzan, Juraj and Zeilinger, Melanie N.},
  year = {2020},
  month = nov,
  journal = {IEEE Transactions on Control Systems Technology},
  volume = {28},
  number = {6},
  pages = {2736--2743},
  issn = {1558-0865},
  doi = {10.1109/TCST.2019.2949757},
  abstract = {Gaussian process (GP) regression has been widely used in supervised machine learning due to its flexibility and inherent ability to describe uncertainty in function estimation. In the context of control, it is seeing increasing use for modeling of nonlinear dynamical systems from data, as it allows the direct assessment of residual model uncertainty. We present a model predictive control (MPC) approach that integrates a nominal system with an additive nonlinear part of the dynamics modeled as a GP. We describe a principled way of formulating the chance-constrained MPC problem, which takes into account residual uncertainties provided by the GP model to enable cautious control. Using additional approximations for efficient computation, we finally demonstrate the approach in a simulation example, as well as in a hardware implementation for autonomous racing of remote-controlled race cars with fast sampling times of 20 ms, highlighting improvements with regard to both performance and safety over a nominal controller.},
  keywords = {Autonomous racing,Computational modeling,Data models,Gaussian processes,Gaussian processes (GPs),Kernel,learning-based control,model learning,model predictive control (MPC),Predictive control,Predictive models,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/5NP9WH9Y/Hewing et al. - 2020 - Cautious Model Predictive Control Using Gaussian P.pdf;/Users/scannea1/Zotero/storage/5LZ4583Z/8909368.html}
}

@inproceedings{hewingCorrespondence2018,
  title = {On a {{Correspondence}} between {{Probabilistic}} and {{Robust Invariant Sets}} for {{Linear Systems}}},
  booktitle = {2018 {{European Control Conference}} ({{ECC}})},
  author = {Hewing, Lukas and Carron, Andrea and Wabersich, Kim P. and Zeilinger, Melanie N.},
  year = {2018},
  month = jun,
  pages = {1642--1647},
  doi = {10.23919/ECC.2018.8550160},
  abstract = {Dynamical systems with stochastic uncertainties are ubiquitous in the field of control, with linear systems under additive Gaussian disturbances a most prominent example. The concept of probabilistic invariance was introduced to extend the widely applied concept of invariance to this class of problems. Computational methods for their synthesis, however, are limited. In this paper we present a relationship between probabilistic and robust invariant sets for linear systems, which enables the use of well-studied robust design methods. Conditions are shown, under which a robust invariant set, designed with a confidence region of the disturbance, results in a probabilistic invariant set. We furthermore show that this condition holds for common box and ellipsoidal confidence regions, generalizing and improving existing results for probabilistic invariant set computation. We finally exemplify the synthesis for an ellipsoidal probabilistic invariant set. Two numerical examples demonstrate the approach and the advantages to be gained from exploiting robust computations for probabilistic invariant sets.},
  keywords = {Additives,Control systems,Gaussian distribution,Linear systems,Probabilistic logic,Random variables,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/A7RQGY5L/Hewing et al. - 2018 - On a Correspondence between Probabilistic and Robu.pdf;/Users/scannea1/Zotero/storage/L4TEPLYJ/8550160.html}
}

@article{hewingLearningBased2020,
  ids = {hewingLearningBased2020a},
  title = {Learning-{{Based Model Predictive Control}}: {{Toward Safe Learning}} in {{Control}}},
  shorttitle = {Learning-{{Based Model Predictive Control}}},
  author = {Hewing, Lukas and Wabersich, Kim P. and Menner, Marcel and Zeilinger, Melanie N.},
  year = {2020},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {3},
  number = {1},
  pages = {269--296},
  doi = {10.1146/annurev-control-090419-075625},
  urldate = {2021-06-24},
  abstract = {Recent successes in the field of machine learning, as well as the availability of increased sensing and computational capabilities in modern control systems, have led to a growing interest in learning and data-driven control techniques. Model predictive control (MPC), as the prime methodology for constrained control, offers a significant opportunity to exploit the abundance of data in a reliable manner, particularly while taking safety constraints into account. This review aims at summarizing and categorizing previous research on learning-based MPC, i.e., the integration or combination of MPC with learning methods, for which we consider three main categories. Most of the research addresses learning for automatic improvement of the prediction model from recorded data. There is, however, also an increasing interest in techniques to infer the parameterization of the MPC controller, i.e., the cost and constraints, that lead to the best closed-loop performance. Finally, we discuss concepts that leverage MPC to augment learning-based controllers with constraint satisfaction properties.},
  file = {/Users/scannea1/Zotero/storage/5M3WTY63/Hewing et al. - 2020 - Learning-Based Model Predictive Control Toward Sa.pdf}
}

@inproceedings{hewingSimulation2020,
  title = {On {{Simulation}} and {{Trajectory Prediction}} with {{Gaussian Process Dynamics}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Hewing, Lukas and Arcari, Elena and Fr{\"o}hlich, Lukas P. and Zeilinger, Melanie N.},
  year = {2020},
  month = jul,
  pages = {424--434},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-24},
  abstract = {Established techniques for simulation and prediction with Gaussian process (GP) dynamics implicitly make use of an independence assumption on successive function evaluations of the dynamics model. ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/SLW3YVJJ/Hewing et al. - 2020 - On Simulation and Trajectory Prediction with Gauss.pdf;/Users/scannea1/Zotero/storage/PM5RAFJM/hewing20a.html}
}

@inproceedings{hewingStochastic2018,
  title = {Stochastic {{Model Predictive Control}} for {{Linear Systems Using Probabilistic Reachable Sets}}},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Hewing, Lukas and Zeilinger, Melanie N.},
  year = {2018},
  month = dec,
  pages = {5182--5188},
  issn = {2576-2370},
  doi = {10.1109/CDC.2018.8619554},
  abstract = {In this paper, we propose a stochastic model predictive control (MPC) algorithm for linear discrete-time systems affected by possibly unbounded additive disturbances and subject to probabilistic constraints. Constraints are treated in analogy to robust MPC using a constraint tightening based on the concept of probabilistic reachable sets, which is shown to provide closed-loop fulfillment of chance constraints under a unimodality assumption on the disturbance distribution. A control scheme reverting to a backup solution from a previous time step in case of infeasibility is proposed, for which an asymptotic average performance bound is derived. Two examples illustrate the approach, highlighting closed-loop chance constraint satisfaction and the benefits of the proposed controller in the presence of unmodeled disturbances.},
  keywords = {Additives,Chebyshev approximation,Linear systems,Predictive control,Probabilistic logic,Random variables,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/D74JVUVJ/Hewing and Zeilinger - 2018 - Stochastic Model Predictive Control for Linear Sys.pdf;/Users/scannea1/Zotero/storage/KCQRLML6/8619554.html}
}

@inproceedings{higginsDARLAImprovingZeroShot2017,
  title = {{{DARLA}}: {{Improving Zero-Shot Transfer}} in {{Reinforcement Learning}}},
  shorttitle = {{{DARLA}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Higgins, Irina and Pal, Arka and Rusu, Andrei and Matthey, Loic and Burgess, Christopher and Pritzel, Alexander and Botvinick, Matthew and Blundell, Charles and Lerchner, Alexander},
  year = {2017},
  month = jul,
  pages = {1480--1490},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {Domain adaptation is an important open problem in deep reinforcement learning (RL). In many scenarios of interest data is hard to obtain, so agents may learn a source policy in a setting where data is readily available, with the hope that it generalises well to the target domain. We propose a new multi-stage RL agent, DARLA (DisentAngled Representation Learning Agent), which learns to see before learning to act. DARLA's vision is based on learning a disentangled representation of the observed environment. Once DARLA can see, it is able to acquire source policies that are robust to many domain shifts {\textendash} even with no access to the target domain. DARLA significantly outperforms conventional baselines in zero-shot domain adaptation scenarios, an effect that holds across a variety of RL environments (Jaco arm, DeepMind Lab) and base RL algorithms (DQN, A3C and EC).},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Higgins et al-2017 DARLA/Higgins et al_2017_DARLA.pdf;/Users/scannea1/Zotero/storage/HF6UAVEZ/Higgins et al. - 2017 - DARLA Improving Zero-Shot Transfer in Reinforceme.pdf}
}

@misc{higginsDefinitionDisentangledRepresentations2018,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  number = {arXiv:1812.02230},
  eprint = {1812.02230},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1812.02230},
  urldate = {2023-11-02},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Higgins et al-2018 Towards a Definition of Disentangled Representations/Higgins et al_2018_Towards a Definition of Disentangled Representations.pdf;/Users/scannea1/Zotero/storage/QKW9EV4N/1812.html}
}

@inproceedings{hiraokaDropoutQFunctionsDoubly2021,
  title = {Dropout {{Q-Functions}} for {{Doubly Efficient Reinforcement Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hiraoka, Takuya and Imagawa, Takahisa and Hashimoto, Taisei and Onishi, Takashi and Tsuruoka, Yoshimasa},
  year = {2021},
  month = oct,
  urldate = {2024-01-08},
  abstract = {Randomized ensembled double Q-learning (REDQ) (Chen et al., 2021b) has recently achieved state-of-the-art sample efficiency on continuous-action reinforcement learning benchmarks. This superior sample efficiency is made possible by using a large Q-function ensemble. However, REDQ is much less computationally efficient than non-ensemble counterparts such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018a). To make REDQ more computationally efficient, we propose a method of improving computational efficiency called DroQ, which is a variant of REDQ that uses a small ensemble of dropout Q-functions. Our dropout Q-functions are simple Q-functions equipped with dropout connection and layer normalization. Despite its simplicity of implementation, our experimental results indicate that DroQ is doubly (sample and computationally) efficient. It achieved comparable sample efficiency with REDQ, much better computational efficiency than REDQ, and comparable computational efficiency with that of SAC.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hiraoka et al-2021 Dropout Q-Functions for Doubly Efficient Reinforcement Learning/Hiraoka et al_2021_Dropout Q-Functions for Doubly Efficient Reinforcement Learning.pdf}
}

@article{hoBayesian1964,
  title = {A {{Bayesian}} Approach to Problems in Stochastic Estimation and Control},
  author = {Ho, Y. and Lee, R.},
  year = {1964},
  journal = {IEEE Transactions on Automatic Control},
  volume = {9},
  pages = {382--387},
  doi = {10.1109/JACC.1964.4168717},
  abstract = {In this paper, a general class of stochastic estimation and control problems is formulated from the Bayesian Decision-Theoretic viewpoint. A discussion as to how these problems can be solved step by step in principle and practice from this approach is presented. As a specific example, the closed form Wiener-Kalman solution for linear estimation in Gaussian noise is derived. The purpose of the paper is to show that the Bayesian approach provides; 1) a general unifying framework within which to pursue further researches in stochastic estimation and control problems, and 2) the necessary computations and difficulties that must be overcome for these problems. An example of a nonlinear, non-Gaussian estimation problem is also solved.}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-19},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/Users/scannea1/Zotero/storage/RZF3BF4U/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hoffmanStochastic2013,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {4},
  pages = {1303--1347},
  urldate = {2021-03-02},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {/Users/scannea1/Zotero/storage/9MQ4G5JD/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf}
}

@article{houlsbyBayesian2011,
  title = {Bayesian {{Active Learning}} for {{Classification}} and {{Preference Learning}}},
  author = {Houlsby, Neil and Husz{\'a}r, Ferenc and Ghahramani, Zoubin and Lengyel, M{\'a}t{\'e}},
  year = {2011},
  month = dec,
  journal = {arXiv:1112.5745 [cs, stat]},
  eprint = {1112.5745},
  primaryclass = {cs, stat},
  urldate = {2021-08-06},
  abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/NWQLADHZ/Houlsby et al. - 2011 - Bayesian Active Learning for Classification and Pr.pdf;/Users/scannea1/Zotero/storage/JGWGTBSD/1112.html}
}

@article{houthooftVIME2017,
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  year = {2017},
  month = jan,
  journal = {arXiv:1605.09674 [cs, stat]},
  eprint = {1605.09674},
  primaryclass = {cs, stat},
  urldate = {2021-02-02},
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  note = {Comment: Published in Advances in Neural Information Processing Systems 29 (NIPS), pages 1109-1117},
  file = {/Users/scannea1/Zotero/storage/JLYSSI3G/Houthooft et al. - 2017 - VIME Variational Information Maximizing Explorati.pdf;/Users/scannea1/Zotero/storage/PTVKGQUZ/1605.html}
}

@inproceedings{huActiveUncertaintyReduction2023,
  title = {Active {{Uncertainty Reduction}} for {{Human-Robot Interaction}}: {{An Implicit Dual Control Approach}}},
  shorttitle = {Active {{Uncertainty Reduction}} for {{Human-Robot Interaction}}},
  booktitle = {Algorithmic {{Foundations}} of {{Robotics XV}}},
  author = {Hu, Haimin and Fisac, Jaime F.},
  editor = {LaValle, Steven M. and O'Kane, Jason M. and Otte, Michael and Sadigh, Dorsa and Tokekar, Pratap},
  year = {2023},
  series = {Springer {{Proceedings}} in {{Advanced Robotics}}},
  pages = {385--401},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-21090-7_23},
  abstract = {The ability to accurately predict human behavior is central to the safety and efficiency of robot autonomy in interactive settings. Unfortunately, robots often lack access to key information on which these predictions may hinge, such as people's goals, attention, and willingness to cooperate. Dual control theory addresses this challenge by treating unknown parameters of a predictive model as stochastic hidden states and inferring their values at runtime using information gathered during system operation. While able to optimally and automatically trade off exploration and exploitation, dual control is computationally intractable for general interactive motion planning, mainly due to the fundamental coupling between robot trajectory optimization and human intent inference. In this paper, we present a novel algorithmic approach to enable active uncertainty reduction for interactive motion planning based on the implicit dual control paradigm. Our approach relies on sampling-based approximation of stochastic dynamic programming, leading to a model predictive control problem that can be readily solved by real-time gradient-based optimization methods. The resulting policy is shown to preserve the dual control effect for a broad class of predictive human models with both continuous and categorical uncertainty. The efficacy of our approach is demonstrated with simulated driving examples.},
  isbn = {978-3-031-21090-7},
  langid = {english},
  keywords = {Dual control theory,Human-robot interaction,Stochastic MPC},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Hu_Fisac-2023 Active Uncertainty Reduction for Human-Robot Interaction/Hu_Fisac_2023_Active Uncertainty Reduction for Human-Robot Interaction.pdf}
}

@inproceedings{immerImprovingPredictionsBayesian2021,
  title = {Improving Predictions of {{Bayesian}} Neural Nets via Local Linearization},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Immer, Alexander and Korzepa, Maciej and Bauer, Matthias},
  year = {2021},
  month = mar,
  pages = {703--711},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-03-10},
  abstract = {The generalized Gauss-Newton (GGN) approximation is often used to make practical Bayesian deep learning approaches scalable by replacing a second order derivative with a product of first order derivatives. In this paper we argue that the GGN approximation should be understood as a local linearization of the underlying Bayesian neural network (BNN), which turns the BNN into a generalized linear model (GLM). Because we use this linearized model for posterior inference, we should also predict using this modified model instead of the original one. We refer to this modified predictive as "GLM predictive" and show that it effectively resolves common underfitting problems of the Laplace approximation. It extends previous results in this vein to general likelihoods and has an equivalent Gaussian process formulation, which enables alternative inference schemes for BNNs in function space. We demonstrate the effectiveness of our approach on several standard classification datasets as well as on out-of-distribution detection. We provide an implementation at https://github.com/AlexImmer/BNN-predictions.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Immer et al-2021 Improving predictions of Bayesian neural nets via local linearization/Immer et al_2021_Improving predictions of Bayesian neural nets via local linearization.pdf;/Users/scannea1/Zotero/storage/2KZPLPWC/Immer et al. - 2021 - Improving predictions of Bayesian neural nets via .pdf}
}

@inproceedings{immerScalable2021,
  title = {Scalable {{Marginal Likelihood Estimation}} for {{Model Selection}} in {{Deep Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Immer, Alexander and Bauer, Matthias and Fortuin, Vincent and R{\"a}tsch, Gunnar and Emtiyaz, Khan Mohammad},
  year = {2021},
  month = jul,
  pages = {4563--4573},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-19},
  abstract = {Marginal-likelihood based model-selection, even though promising, is rarely used in deep learning due to estimation difficulties. Instead, most approaches rely on validation data, which may not be readily available. In this work, we present a scalable marginal-likelihood estimation method to select both hyperparameters and network architectures, based on the training data alone. Some hyperparameters can be estimated online during training, simplifying the procedure. Our marginal-likelihood estimate is based on Laplace's method and Gauss-Newton approximations to the Hessian, and it outperforms cross-validation and manual tuning on standard regression and image classification datasets, especially in terms of calibration and out-of-distribution detection. Our work shows that marginal likelihoods can improve generalization and be useful when validation data is unavailable (e.g., in nonstationary settings).},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/FGT3XJXL/Immer et al. - 2021 - Scalable Marginal Likelihood Estimation for Model .pdf;/Users/scannea1/Zotero/storage/ZQZQAGS7/Immer et al. - 2021 - Scalable Marginal Likelihood Estimation for Model .pdf}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jun,
  pages = {448--456},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-10-09},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ioffe_Szegedy-2015 Batch Normalization/Ioffe_Szegedy_2015_Batch Normalization.pdf}
}

@inproceedings{izmailovWhat2021,
  title = {What {{Are Bayesian Neural Network Posteriors Really Like}}?},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Izmailov, Pavel and Vikram, Sharad and Hoffman, Matthew D. and Wilson, Andrew Gordon Gordon},
  year = {2021},
  month = jul,
  pages = {4629--4640},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles; (2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains; (3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a ``cold posterior'' effect, which we show is largely an artifact of data augmentation; (4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors; (5) Bayesian neural networks show surprisingly poor generalization under domain shift; (6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4BHLCR25/Izmailov et al. - 2021 - What Are Bayesian Neural Network Posteriors Really.pdf;/Users/scannea1/Zotero/storage/E45S87FZ/Izmailov et al. - 2021 - What Are Bayesian Neural Network Posteriors Really.pdf}
}

@article{jacobsAdaptive1991,
  title = {Adaptive {{Mixtures}} of {{Local Experts}}},
  author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  year = {1991},
  month = mar,
  journal = {Neural Computation},
  volume = {3},
  number = {1},
  pages = {79--87},
  issn = {0899-7667},
  doi = {10.1162/neco.1991.3.1.79},
  urldate = {2021-04-27},
  abstract = {We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.},
  file = {/Users/scannea1/Zotero/storage/85CFX29F/Jacobs et al. - 1991 - Adaptive Mixtures of Local Experts.pdf;/Users/scannea1/Zotero/storage/EYULD8LB/Adaptive-Mixtures-of-Local-Experts.html}
}

@article{jacobsonDifferential1970,
  title = {Differential Dynamic Programming},
  author = {Jacobson, David H. and Mayne, David Q.},
  year = {1970},
  publisher = {{North-Holland}}
}

@inproceedings{jagtapControl2020,
  title = {Control {{Barrier Functions}} for {{Unknown Nonlinear Systems}} Using {{Gaussian Processes}}*},
  booktitle = {{{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Jagtap, Pushpak and Pappas, George J. and Zamani, M.},
  year = {2020},
  volume = {59},
  publisher = {{IEEE}},
  doi = {10.1109/CDC42340.2020.9303847},
  abstract = {This paper focuses on the controller synthesis for unknown, nonlinear systems while ensuring safety constraints. Our approach consists of two steps, a learning step that uses Gaussian processes and a controller synthesis step that is based on control barrier functions. In the learning step, we use a data-driven approach utilizing Gaussian processes to learn the unknown control affine nonlinear dynamics together with a statistical bound on the accuracy of the learned model. In the second controller synthesis steps, we develop a systematic approach to compute control barrier functions that explicitly take into consideration the uncertainty of the learned model. The control barrier function not only results in a safe controller by construction but also provides a rigorous lower bound on the probability of satisfaction of the safety specification. Finally, we illustrate the effectiveness of the proposed results by synthesizing a safety controller for a jet engine example.},
  file = {/Users/scannea1/Zotero/storage/URCCH3KQ/Jagtap et al. - 2020 - Control Barrier Functions for Unknown Nonlinear Sy.pdf}
}

@article{jakschNearoptimal2010,
  title = {Near-Optimal {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {51},
  pages = {1563--1600},
  issn = {1533-7928},
  urldate = {2022-04-06},
  abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret {\~O}(DSAT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of {$\Omega$}(DSAT) on the total regret of any learning algorithm is given as well. These results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T. Finally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of {\~O}(l1/3T2/3DSA).},
  file = {/Users/scannea1/Zotero/storage/4Z9LMIWU/Jaksch et al. - 2010 - Near-optimal Regret Bounds for Reinforcement Learn.pdf}
}

@inproceedings{jannerPlanning2022,
  title = {Planning with {{Diffusion}} for {{Flexible Behavior Synthesis}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Janner, Michael and Du, Yilun and Tenenbaum, Joshua and Levine, Sergey},
  year = {2022},
  month = jun,
  pages = {9902--9915},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {Model-based reinforcement learning methods often use learning only for the purpose of recovering an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control settings that emphasize long-horizon decision-making and test-time flexibility.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/EMR4UP3J/Janner et al. - 2022 - Planning with Diffusion for Flexible Behavior Synt.pdf}
}

@inproceedings{jannerWhen2019,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  year = {2019},
  volume = {32},
  urldate = {2021-06-08},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/CIG5RUDI/Janner et al. - 2019 - When to Trust Your Model Model-Based Policy Optim.pdf;/Users/scannea1/Zotero/storage/Q5UZUMN8/mbpo_2019_supp.pdf;/Users/scannea1/Zotero/storage/RUBIH4IV/5faf461eff3099671ad63c6f3f094f7f-Abstract.html}
}

@misc{jax2018github,
  title = {{{JAX}}: Composable Transformations of {{Python}}+{{NumPy}} Programs},
  author = {Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and {Wanderman-Milne}, Skye and Zhang, Qiao},
  year = {2018}
}

@misc{jiangVIMAGeneralRobot2023,
  title = {{{VIMA}}: {{General Robot Manipulation}} with {{Multimodal Prompts}}},
  shorttitle = {{{VIMA}}},
  author = {Jiang, Yunfan and Gupta, Agrim and Zhang, Zichen and Wang, Guanzhi and Dou, Yongqiang and Chen, Yanjun and {Fei-Fei}, Li and Anandkumar, Anima and Zhu, Yuke and Fan, Linxi},
  year = {2023},
  month = may,
  number = {arXiv:2210.03094},
  eprint = {2210.03094},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-30},
  abstract = {Prompt-based learning has emerged as a successful paradigm in natural language processing, where a single general-purpose language model can be instructed to perform any task specified by input prompts. Yet task specification in robotics comes in various forms, such as imitating one-shot demonstrations, following language instructions, and reaching visual goals. They are often considered different tasks and tackled by specialized models. We show that a wide spectrum of robot manipulation tasks can be expressed with multimodal prompts, interleaving textual and visual tokens. Accordingly, we develop a new simulation benchmark that consists of thousands of procedurally-generated tabletop tasks with multimodal prompts, 600K+ expert trajectories for imitation learning, and a four-level evaluation protocol for systematic generalization. We design a transformer-based robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively. VIMA features a recipe that achieves strong model scalability and data efficiency. It outperforms alternative designs in the hardest zero-shot generalization setting by up to \$2.9{\textbackslash}times\$ task success rate given the same training data. With \$10{\textbackslash}times\$ less training data, VIMA still performs \$2.7{\textbackslash}times\$ better than the best competing variant. Code and video demos are available at https://vimalabs.github.io/},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: ICML 2023 Camera-ready version. Project website: https://vimalabs.github.io/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Jiang et al-2023 VIMA/Jiang et al_2023_VIMA.pdf;/Users/scannea1/Zotero/storage/3H3TXZMP/2210.html}
}

@inproceedings{jingUnderstandingDimensionalCollapse2021,
  title = {Understanding {{Dimensional Collapse}} in {{Contrastive Self-supervised Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
  year = {2021},
  month = oct,
  urldate = {2024-02-01},
  abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Jing et al-2021 Understanding Dimensional Collapse in Contrastive Self-supervised Learning/Jing et al_2021_Understanding Dimensional Collapse in Contrastive Self-supervised Learning.pdf}
}

@article{julierUnscented2004,
  title = {Unscented Filtering and Nonlinear Estimation},
  author = {Julier, S.J. and Uhlmann, J.K.},
  year = {2004},
  month = mar,
  journal = {Proceedings of the IEEE},
  volume = {92},
  number = {3},
  pages = {401--422},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2003.823141},
  abstract = {The extended Kalman filter (EKF) is probably the most widely used estimation algorithm for nonlinear systems. However, more than 35 years of experience in the estimation community has shown that is difficult to implement, difficult to tune, and only reliable for systems that are almost linear on the time scale of the updates. Many of these difficulties arise from its use of linearization. To overcome this limitation, the unscented transformation (UT) was developed as a method to propagate mean and covariance information through nonlinear transformations. It is more accurate, easier to implement, and uses the same order of calculations as linearization. This paper reviews the motivation, development, use, and implications of the UT.},
  keywords = {Chemical processes,Control systems,Filtering,Kalman filters,Navigation,Nonlinear control systems,Nonlinear systems,Particle tracking,Target tracking,Vehicles},
  file = {/Users/scannea1/Zotero/storage/6TYL885C/Julier and Uhlmann - 2004 - Unscented filtering and nonlinear estimation.pdf;/Users/scannea1/Zotero/storage/IGQM2C8D/1271397.html}
}

@inproceedings{kaiserBayesian2018,
  title = {Bayesian {{Alignments}} of {{Warped Multi-Output Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  year = {2018},
  volume = {31},
  pages = {6995--7004},
  urldate = {2020-12-01},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JMCYFREV/Kaiser et al. - 2018 - Bayesian Alignments of Warped Multi-Output Gaussia.pdf;/Users/scannea1/Zotero/storage/RM276IC3/2974788b53f73e7950e8aa49f3a306db-Abstract.html}
}

@article{kaiserBayesian2020,
  title = {Bayesian Decomposition of Multi-Modal Dynamical Systems for Reinforcement Learning},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas A. and Ek, Carl Henrik},
  year = {2020},
  month = nov,
  journal = {Neurocomputing},
  volume = {416},
  pages = {352--359},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2019.12.132},
  urldate = {2020-12-01},
  abstract = {In this paper, we present a model-based reinforcement learning system where the transition model is treated in a Bayesian manner. The approach naturally lends itself to exploit expert knowledge by introducing priors to impose structure on the underlying learning task. The additional information introduced to the system means that we can learn from small amounts of data, recover an interpretable model and, importantly, provide predictions with an associated uncertainty. To show the benefits of the approach, we use a challenging data set where the dynamics of the underlying system exhibit both operational phase shifts and heteroscedastic noise. Comparing our model to NFQ and BNN+LV, we show how our approach yields human-interpretable insight about the underlying dynamics while also increasing data-efficiency.},
  langid = {english},
  keywords = {Bayesian machine learning,Data-efficiency,Gaussian processes,Hierarchical gaussian processes,Model-based reinforcement learning,Reinforcement learning,Stochastic policy search},
  file = {/Users/scannea1/Zotero/storage/HR34GBG6/Kaiser et al. - 2020 - Bayesian decomposition of multi-modal dynamical sy.pdf;/Users/scannea1/Zotero/storage/EJE7Q7YF/S0925231220305026.html}
}

@inproceedings{kaiserData2019,
  title = {Data {{Association}} with {{Gaussian Processes}}},
  booktitle = {Joint {{European Conference}} on {{Machine Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kaiser, Markus and Otte, Clemens and Runkler, Thomas and Ek, Carl Henrik},
  year = {2019},
  eprint = {1810.07158},
  urldate = {2020-12-01},
  abstract = {The data association problem is concerned with separating data coming from different generating processes, for example when data come from different data sources, contain significant noise, or exhibit multimodality. We present a fully Bayesian approach to this problem. Our model is capable of simultaneously solving the data association problem and the induced supervised learning problems. Underpinning our approach is the use of Gaussian process priors to encode the structure of both the data and the data associations. We present an efficient learning scheme based on doubly stochastic variational inference and discuss how it can be applied to deep Gaussian process priors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/2QN8CB7D/Kaiser et al. - 2019 - Data Association with Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/PPAGW99R/1810.html}
}

@inproceedings{kaiserModelBasedReinforcement2020,
  title = {Model {{Based Reinforcement Learning}} for {{Atari}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Kaiser, {\L}ukasz and Babaeizadeh, Mohammad and Mi{\l}os, Piotr and Osi{\'n}ski, B{\l}a{\.z}ej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2020},
  month = mar,
  urldate = {2022-11-12},
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of 100k interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kaiser et al-2020 Model Based Reinforcement Learning for Atari/Kaiser et al_2020_Model Based Reinforcement Learning for Atari.pdf;/Users/scannea1/Zotero/storage/YIHPAPDY/forum.html}
}

@article{kalmanNew1960,
  title = {A {{New Approach}} to {{Linear Filtering}} and {{Prediction Problems}}},
  author = {Kalman, R. E.},
  year = {1960},
  month = mar,
  journal = {Journal of Basic Engineering},
  volume = {82},
  number = {1},
  pages = {35--45},
  issn = {0021-9223},
  doi = {10.1115/1.3662552},
  urldate = {2021-08-31},
  abstract = {The classical filtering and prediction problem is re-examined using the Bode-Shannon representation of random processes and the ``state-transition'' method of analysis of dynamic systems. New results are: (1) The formulation and methods of solution of the problem apply without modification to stationary and nonstationary statistics and to growing-memory and infinite-memory filters. (2) A nonlinear difference (or differential) equation is derived for the covariance matrix of the optimal estimation error. From the solution of this equation the co-efficients of the difference (or differential) equation of the optimal linear filter are obtained without further calculations. (3) The filtering problem is shown to be the dual of the noise-free regulator problem. The new method developed here is applied to two well-known problems, confirming and extending earlier results. The discussion is largely self-contained and proceeds from first principles; basic concepts of the theory of random processes are reviewed in the Appendix.},
  file = {/Users/scannea1/Zotero/storage/S93L7H82/A-New-Approach-to-Linear-Filtering-and-Prediction.html}
}

@inproceedings{kamtheDataEfficient2018,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Probabilistic Model Predictive Control}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kamthe, Sanket and Deisenroth, Marc},
  year = {2018},
  month = mar,
  pages = {1701--1710},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2020-11-26},
  abstract = {Trial-and-error based reinforcement learning (RL) has seen rapid advancements in recent times, especially with the advent of deep neural networks. However, the majority of autonomous RL algorithms...},
  langid = {english},
  keywords = {model-based-rl,mpc},
  file = {/Users/scannea1/Zotero/storage/E96H6LF8/Kamthe and Deisenroth - 2018 - Data-Efficient Reinforcement Learning with Probabi.pdf;/Users/scannea1/Zotero/storage/UGM5DYC9/kamthe18a.html}
}

@inproceedings{kappenIntroduction2007,
  title = {An Introduction to Stochastic Control Theory, Path Integrals and Reinforcement Learning},
  booktitle = {{{AIP Conference Proceedings}}},
  author = {Kappen, Hilbert J},
  year = {2007},
  pages = {34},
  abstract = {Control theory is a mathematical description of how to act optimally to gain future rewards. In this paper I give an introduction to deterministic and stochastic control theory and I give an overview of the possible application of control theory to the modeling of animal behavior and learning. I discuss a class of non-linear stochastic control problems that can be efficiently solved using a path integral or by MC sampling. In this control formalism the central concept of cost-to-go becomes a free energy and methods and concepts from statistical physics can be readily applied.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/T5W7HD97/Kappen - An introduction to stochastic control theory, path.pdf}
}

@inproceedings{kappenOptimal2013,
  ids = {kappenOptimal2013a},
  title = {Optimal Control as a Graphical Model Inference Problem},
  booktitle = {Proceedings of the {{Twenty-Third International Conference}} on {{International Conference}} on {{Automated Planning}} and {{Scheduling}}},
  author = {Kappen, Hilbert J. and G{\'o}mez, Vicen{\c c} and Opper, Manfred},
  year = {2013},
  month = jun,
  series = {{{ICAPS}}'13},
  pages = {472--473},
  publisher = {{AAAI Press}},
  address = {{Rome, Italy}},
  urldate = {2021-08-31},
  abstract = {In this paper we show the identification between stochastic optimal control computation and probabilistic inference on a graphical model for certain class of control problems. We refer to these problems as Kullback-Leibler (KL) control problems. We illustrate how KL control can be used to model a multi-agent cooperative game for which optimal control can be approximated using belief propagation when exact inference is unfeasible.},
  file = {/Users/scannea1/Zotero/storage/C4NPGFTC/Kappen et al. - 2012 - Optimal control as a graphical model inference pro.pdf}
}

@misc{kattBayesian2018,
  title = {Bayesian {{Reinforcement Learning}} in {{Factored POMDPs}}},
  author = {Katt, Sammie and Oliehoek, Frans and Amato, Christopher},
  year = {2018},
  month = nov,
  number = {arXiv:1811.05612},
  eprint = {1811.05612},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-06-25},
  abstract = {Bayesian approaches provide a principled solution to the exploration-exploitation trade-off in Reinforcement Learning. Typical approaches, however, either assume a fully observable environment or scale poorly. This work introduces the Factored Bayes-Adaptive POMDP model, a framework that is able to exploit the underlying structure while learning the dynamics in partially observable systems. We also present a belief tracking method to approximate the joint posterior over state and model variables, and an adaptation of the Monte-Carlo Tree Search solution method, which together are capable of solving the underlying problem near-optimally. Our method is able to learn efficiently given a known factorization or also learn the factorization and the model parameters at the same time. We demonstrate that this approach is able to outperform current methods and tackle problems that were previously infeasible.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/scannea1/Zotero/storage/9X454S6J/Katt et al. - 2018 - Bayesian Reinforcement Learning in Factored POMDPs.pdf;/Users/scannea1/Zotero/storage/D6R7SLPX/1811.html}
}

@article{kellyIntroduction2017,
  title = {An {{Introduction}} to {{Trajectory Optimization}}: {{How}} to {{Do Your Own Direct Collocation}}},
  shorttitle = {An {{Introduction}} to {{Trajectory Optimization}}},
  author = {Kelly, Matthew},
  year = {2017},
  month = jan,
  journal = {SIAM Review},
  volume = {59},
  number = {4},
  pages = {849--904},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/16M1062569},
  urldate = {2021-02-19},
  abstract = {This paper is an introductory tutorial for numerical trajectory optimization with a focus on direct collocation methods. These methods are relatively simple to understand and effectively solve a wide variety of trajectory optimization problems. Throughout the paper we illustrate each new set of concepts by working through a sequence of four example problems. We start by using trapezoidal collocation to solve a simple one-dimensional toy problem and work up to using Hermite{\textendash}Simpson collocation to compute the optimal gait for a bipedal walking robot. Along the way, we cover basic debugging strategies and guidelines for posing well-behaved optimization problems. The paper concludes with a short overview of other methods for trajectory optimization. We also provide an electronic supplement that contains well-documented MATLAB code for all examples and methods presented. Our primary goal is to provide the reader with the resources necessary to understand and successfully implement their own direct collocation methods.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/ERJYVAY8/Kelly - 2017 - An Introduction to Trajectory Optimization How to.pdf}
}

@inproceedings{khanApproximate2019,
  title = {Approximate {{Inference Turns Deep Networks}} into {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Khan, Mohammad Emtiyaz E and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-14},
  abstract = {Deep neural networks (DNN) and Gaussian processes (GP) are two powerful models with several theoretical connections relating them, but the relationship between their training methods is not well understood. In this paper, we show that certain Gaussian posterior approximations for Bayesian DNNs are equivalent to GP posteriors. This enables us to relate solutions and iterations of a deep-learning algorithm to GP inference. As a result, we can obtain a GP kernel and a nonlinear feature map while training a DNN. Surprisingly, the resulting kernel is the neural tangent kernel. We show kernels obtained on real datasets and demonstrate the use of the GP marginal likelihood to tune hyperparameters of DNNs. Our work aims to facilitate further research on combining DNNs and GPs in practical settings.},
  file = {/Users/scannea1/Zotero/storage/ESU7S5H7/Khan et al. - 2019 - Approximate Inference Turns Deep Networks into Gau.pdf}
}

@misc{khanBayesian2022,
  title = {The {{Bayesian Learning Rule}}},
  author = {Khan, Mohammad Emtiyaz and Rue, H{\aa}vard},
  year = {2022},
  month = mar,
  number = {arXiv:2107.04562},
  eprint = {2107.04562},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.04562},
  urldate = {2022-07-14},
  abstract = {We show that many machine-learning algorithms are specific instances of a single algorithm called the Bayesian learning rule. The rule, derived from Bayesian principles, yields a wide-range of algorithms from fields such as optimization, deep learning, and graphical models. This includes classical algorithms such as ridge regression, Newton's method, and Kalman filter, as well as modern deep-learning algorithms such as stochastic-gradient descent, RMSprop, and Dropout. The key idea in deriving such algorithms is to approximate the posterior using candidate distributions estimated by using natural gradients. Different candidate distributions result in different algorithms and further approximations to natural gradients give rise to variants of those algorithms. Our work not only unifies, generalizes, and improves existing algorithms, but also helps us design new ones.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/4CNHZC3R/Khan and Rue - 2022 - The Bayesian Learning Rule.pdf;/Users/scannea1/Zotero/storage/A9MGRBDG/2107.html}
}

@inproceedings{khanFastScalableBayesian2018,
  title = {Fast and {{Scalable Bayesian Deep Learning}} by {{Weight-Perturbation}} in {{Adam}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Khan, Mohammad and Nielsen, Didrik and Tangkaratt, Voot and Lin, Wu and Gal, Yarin and Srivastava, Akash},
  year = {2018},
  month = jul,
  pages = {2611--2620},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-14},
  abstract = {Uncertainty computation in deep learning is essential to design robust and reliable systems. Variational inference (VI) is a promising approach for such computation, but requires more effort to implement and execute compared to maximum-likelihood methods. In this paper, we propose new natural-gradient algorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms can be implemented within the Adam optimizer by perturbing the network weights during gradient evaluations, and uncertainty estimates can be cheaply obtained by using the vector that adapts the learning rate. This requires lower memory, computation, and implementation effort than existing VI methods, while obtaining uncertainty estimates of comparable quality. Our empirical results confirm this and further suggest that the weight-perturbation in our algorithm could be useful for exploration in reinforcement learning and stochastic optimization.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Khan et al-2018 Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam/Khan et al_2018_Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam.pdf}
}

@inproceedings{khanKnowledgeAdaptation2021,
  title = {Knowledge-{{Adaptation Priors}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Khan, Mohammad Emtiyaz E and Swaroop, Siddharth},
  year = {2021},
  volume = {34},
  pages = {19757--19770},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-14},
  abstract = {Humans and animals have a natural ability to quickly adapt to their surroundings, but machine-learning models, when subjected to changes, often require a complete retraining from scratch. We present Knowledge-adaptation priors (K-priors) to reduce the cost of retraining by enabling quick and accurate adaptation for a wide-variety of tasks and models. This is made possible by a combination of weight and function-space priors to reconstruct the gradients of the past, which recovers and generalizes many existing, but seemingly-unrelated, adaptation strategies. Training with simple first-order gradient methods can often recover the exact retrained model to an arbitrary accuracy by choosing a sufficiently large memory of the past data. Empirical results show that adaptation with K-priors achieves performance similar to full retraining, but only requires training on a handful of past examples.},
  file = {/Users/scannea1/Zotero/storage/KSM9YXVA/Khan and Swaroop - 2021 - Knowledge-Adaptation Priors.pdf}
}

@article{khetarpalContinualReinforcementLearning2022,
  title = {Towards {{Continual Reinforcement Learning}}: {{A Review}} and {{Perspectives}}},
  shorttitle = {Towards {{Continual Reinforcement Learning}}},
  author = {Khetarpal, Khimya and Riemer, Matthew and Rish, Irina and Precup, Doina},
  year = {2022},
  month = dec,
  journal = {Journal of Artificial Intelligence Research},
  volume = {75},
  pages = {1401--1476},
  issn = {1076-9757},
  doi = {10.1613/jair.1.13673},
  urldate = {2023-09-30},
  abstract = {In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.},
  copyright = {Copyright (c) 2022 Journal of Artificial Intelligence Research},
  langid = {english},
  keywords = {markov decision processes,reinforcement learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Khetarpal et al-2022 Towards Continual Reinforcement Learning/Khetarpal et al_2022_Towards Continual Reinforcement Learning.pdf}
}

@inproceedings{khojastehProbabilistic2020,
  title = {Probabilistic {{Safety Constraints}} for {{Learned High Relative Degree System Dynamics}}},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {Khojasteh, Mohammad Javad and Dhiman, Vikas and Franceschetti, Massimo and Atanasov, Nikolay},
  year = {2020},
  month = jul,
  pages = {781--792},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-30},
  abstract = {This paper focuses on learning a model of system dynamics online while satisfying safety constraints. Our motivation is to avoid offline system identification or hand-specified dynamics models and ...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/8RWK9LT6/Khojasteh et al. - 2020 - Probabilistic Safety Constraints for Learned High .pdf;/Users/scannea1/Zotero/storage/YA9Z3G8C/khojasteh20a.html}
}

@inproceedings{kidambiMOReLModelBasedOffline2020,
  title = {{{MOReL}}: {{Model-Based Offline Reinforcement Learning}}},
  shorttitle = {{{MOReL}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  year = {2020},
  volume = {33},
  pages = {21810--21823},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-12},
  abstract = {In offline reinforcement learning (RL), the goal is to learn a highly rewarding policy based solely on a dataset of historical interactions with the environment. This serves as an extreme test for an agent's ability to effectively use historical data which is known to be critical for efficient RL. Prior work in offline RL has been confined almost exclusively to model-free RL approaches. In this work, we present MOReL, an algorithmic framework for model-based offline RL. This framework consists of two steps: (a) learning a pessimistic MDP using the offline dataset; (b) learning a near-optimal policy in this pessimistic MDP. The design of the pessimistic MDP is such that for any policy, the performance in the real environment is approximately lower-bounded by the performance in the pessimistic MDP. This enables the pessimistic MDP to serve as a good surrogate for purposes of policy evaluation and learning. Theoretically, we show that MOReL is minimax optimal (up to log factors) for offline RL. Empirically, MOReL matches or exceeds state-of-the-art results on widely used offline RL benchmarks. Overall, the modular design of MOReL enables translating advances in its components (for e.g., in model learning, planning etc.) to improvements in offline RL.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kidambi et al-2020 MOReL/Kidambi et al_2020_MOReL.pdf}
}

@misc{kidgerNeural2022,
  title = {On {{Neural Differential Equations}}},
  author = {Kidger, Patrick},
  year = {2022},
  month = feb,
  number = {arXiv:2202.02435},
  eprint = {2202.02435},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.02435},
  urldate = {2022-08-01},
  abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Classical Analysis and ODEs,Mathematics - Dynamical Systems,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  note = {Comment: Doctoral thesis, Mathematical Institute, University of Oxford. 231 pages},
  file = {/Users/scannea1/Zotero/storage/W446TT3L/Kidger - 2022 - On Neural Differential Equations.pdf;/Users/scannea1/Zotero/storage/PMTBJNK3/2202.html}
}

@article{kimAnalyzing2005,
  title = {Analyzing {{Nonstationary Spatial Data Using Piecewise Gaussian Processes}}},
  author = {Kim, Hyoung-Moon and Mallick, Bani K and Holmes, C. C},
  year = {2005},
  month = jun,
  journal = {Journal of the American Statistical Association},
  volume = {100},
  number = {470},
  pages = {653--668},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1198/016214504000002014},
  urldate = {2021-10-26},
  abstract = {In many problems in geostatistics the response variable of interest is strongly related to the underlying geology of the spatial location. In these situations there is often little correlation in the responses found in different rock strata, so the underlying covariance structure shows sharp changes at the boundaries of the rock types. Conventional stationary and nonstationary spatial methods are inappropriate, because they typically assume that the covariance between points is a smooth function of distance. In this article we propose a generic method for the analysis of spatial data with sharp changes in the underlying covariance structure. Our method works by automatically decomposing the spatial domain into disjoint regions within which the process is assumed to be stationary, but the data are assumed independent across regions. Uncertainty in the number of disjoint regions, their shapes, and the model within regions is dealt with in a fully Bayesian fashion. We illustrate our approach on a previously unpublished dataset relating to soil permeability of the Schneider Buda oil field in Wood County, Texas.},
  keywords = {Bayes factor,Kriging,Model averaging,Reversible-jump Markov chain Monte Carlo,Voronoi tessellation},
  file = {/Users/scannea1/Zotero/storage/YPI64BPC/016214504000002014.html}
}

@article{kingmaAdam2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  journal = {arXiv:1412.6980 [cs]},
  eprint = {1412.6980},
  primaryclass = {cs},
  urldate = {2021-09-28},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  file = {/Users/scannea1/Zotero/storage/BA9JP6IL/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/scannea1/Zotero/storage/R2QSRLK6/1412.html}
}

@article{kingmaAutoEncoding2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, M.},
  year = {2014},
  journal = {ICLR},
  abstract = {A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced. Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  file = {/Users/scannea1/Zotero/storage/STWNYJNX/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf}
}

@misc{kipfContrastiveLearningStructured2020,
  title = {Contrastive {{Learning}} of {{Structured World Models}}},
  author = {Kipf, Thomas and {van der Pol}, Elise and Welling, Max},
  year = {2020},
  month = jan,
  number = {arXiv:1911.12247},
  eprint = {1911.12247},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {A structured understanding of our world in terms of objects, relations, and hierarchies is an important component of human cognition. Learning such a structured world model from raw sensory data remains a challenge. As a step towards this goal, we introduce Contrastively-trained Structured World Models (C-SWMs). C-SWMs utilize a contrastive approach for representation learning in environments with compositional structure. We structure each state embedding as a set of object representations and their relations, modeled by a graph neural network. This allows objects to be discovered from raw pixel observations without direct supervision as part of the learning process. We evaluate C-SWMs on compositional environments involving multiple interacting objects that can be manipulated independently by an agent, simple Atari games, and a multi-object physics simulation. Our experiments demonstrate that C-SWMs can overcome limitations of models based on pixel reconstruction and outperform typical representatives of this model class in highly structured environments, while learning interpretable object-based representations.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICLR 2020},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kipf et al-2020 Contrastive Learning of Structured World Models/Kipf et al_2020_Contrastive Learning of Structured World Models.pdf;/Users/scannea1/Zotero/storage/U6RLTHBK/1911.html}
}

@inproceedings{kirillovSegmentAnything2023,
  title = {Segment {{Anything}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Dollar, Piotr and Girshick, Ross},
  year = {2023},
  pages = {4015--4026},
  urldate = {2023-11-23},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/K9Y9F6B2/Kirillov et al. - 2023 - Segment Anything.pdf}
}

@book{kirkOptimal2004,
  title = {Optimal Control Theory: An Introduction},
  author = {Kirk, Donald},
  year = {2004},
  publisher = {{Courier Corporation}}
}

@inproceedings{kirschBatchBALDEfficientDiverse2019,
  title = {{{BatchBALD}}: {{Efficient}} and {{Diverse Batch Acquisition}} for {{Deep Bayesian Active Learning}}},
  shorttitle = {{{BatchBALD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kirsch, Andreas and {van Amersfoort}, Joost and Gal, Yarin},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-30},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kirsch et al-2019 BatchBALD/Kirsch et al_2019_BatchBALD.pdf}
}

@inproceedings{kirschnerAdaptiveSafeBayesian2019,
  title = {Adaptive and {{Safe Bayesian Optimization}} in {{High Dimensions}} via {{One-Dimensional Subspaces}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kirschner, Johannes and Mutny, Mojmir and Hiller, Nicole and Ischebeck, Rasmus and Krause, Andreas},
  year = {2019},
  month = may,
  pages = {3429--3438},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-02-22},
  abstract = {Bayesian optimization is known to be difficult to scale to high dimensions, because the acquisition step requires solving a non-convex optimization problem in the same search space. In order to scale the method and keep its benefits, we propose an algorithm (LineBO) that restricts the problem to a sequence of iteratively chosen one-dimensional sub-problems that can be solved efficiently. We show that our algorithm converges globally and obtains a fast local rate when the function is strongly convex. Further, if the objective has an invariant subspace, our method automatically adapts to the effective dimension without changing the algorithm. When combined with the SafeOpt algorithm to solve the sub-problems, we obtain the first safe Bayesian optimization algorithm with theoretical guarantees applicable in high-dimensional settings. We evaluate our method on multiple synthetic benchmarks, where we obtain competitive performance. Further, we deploy our algorithm to optimize the beam intensity of the Swiss Free Electron Laser with up to 40 parameters while satisfying safe operation constraints.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kirschner et al-2019 Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional/Kirschner et al_2019_Adaptive and Safe Bayesian Optimization in High Dimensions via One-Dimensional.pdf;/Users/scannea1/Zotero/storage/EICNB4Y8/Kirschner et al. - 2019 - Adaptive and Safe Bayesian Optimization in High Di.pdf}
}

@article{klenskeDual2016,
  title = {Dual {{Control}} for {{Approximate Bayesian Reinforcement Learning}}},
  author = {Klenske, Edgar D. and Hennig, Philipp},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {127},
  pages = {1--30},
  issn = {1533-7928},
  urldate = {2021-08-04},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/CG3X7PCY/Klenske and Hennig - 2016 - Dual Control for Approximate Bayesian Reinforcemen.pdf;/Users/scannea1/Zotero/storage/6CPR4MF6/15-162.html}
}

@article{koExact1995,
  title = {An {{Exact Algorithm}} for {{Maximum Entropy Sampling}}},
  author = {Ko, Chun-Wa and Lee, Jon and Queyranne, Maurice},
  year = {1995},
  month = aug,
  journal = {Operations Research},
  volume = {43},
  number = {4},
  pages = {684--691},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.43.4.684},
  urldate = {2021-08-06},
  abstract = {We study the experimental design problem of selecting a most informative subset, having prespecified size, from a set of correlated random variables. The problem arises in many applied domains, such as meteorology, environmental statistics, and statistical geology. In these applications, observations can be collected at different locations, and possibly, at different times. Information is measured by ``entropy.'' In the Gaussian case, the problem is recast as that of maximizing the determinant of the covariance matrix of the chosen subset. We demonstrate that this problem is NP-hard. We establish an upper bound for the entropy, based on the eigenvalue interlacing property, and we incorporate this bound in a branch-and-bound algorithm for the exact solution of the problem. We present computational results for estimated covariance matrices that correspond to sets of environmental monitoring stations in the United States.},
  keywords = {facilities/equipment planning,location of environmental monitoring stations,maximum entropy sampling,nonlinear combinatorial optimization,programming,statistics}
}

@article{kofmanProbabilistic2012,
  title = {Probabilistic Set Invariance and Ultimate Boundedness},
  author = {Kofman, Ernesto and De Don{\'a}, Jos{\'e} A. and Seron, Maria M.},
  year = {2012},
  month = oct,
  journal = {Automatica},
  volume = {48},
  number = {10},
  pages = {2670--2676},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2012.06.074},
  urldate = {2021-06-28},
  abstract = {The notions of invariant sets and ultimate bounds are important concepts in the analysis of dynamical systems and very useful tools for the design of control systems. Several approaches have been reported for the characterisation of these sets, including constructive methods for their computation and procedures to obtain different approximations. However, there are shortcomings in those concepts, in the sense that no general probability distributions can be considered for the disturbances affecting the system (which, for example, precludes the assumption of Gaussian distributions insofar as they are not bounded). Motivated by those shortcomings, we propose in this paper the novel concepts of probabilistic ultimate bounds and probabilistic invariant sets, which extend the notions of invariant sets and ultimate bounds to consider `containment in probability', and have the important feature of allowing stochastic noises with more general distributions, including the ubiquitous Gaussian distribution, to be considered. We introduce some key definitions for these sets, establish their main properties and develop methods for their computation. A numerical example illustrates the main ideas.},
  langid = {english},
  keywords = {Invariant sets,Linear systems,Probabilistic methods,Ultimate bounds},
  file = {/Users/scannea1/Zotero/storage/6UFJUVQ6/Kofman et al. - 2012 - Probabilistic set invariance and ultimate boundedn.pdf;/Users/scannea1/Zotero/storage/BYUCB2YS/S0005109812003408.html}
}

@inproceedings{kollerLearningBased2018,
  title = {Learning-{{Based Model Predictive Control}} for {{Safe Exploration}}},
  booktitle = {2018 {{IEEE Conference}} on {{Decision}} and {{Control}} ({{CDC}})},
  author = {Koller, T. and Berkenkamp, F. and Turchetta, M. and Krause, A.},
  year = {2018},
  month = dec,
  pages = {6059--6066},
  issn = {2576-2370},
  doi = {10.1109/CDC.2018.8619572},
  abstract = {Learning-based methods have been successful in solving complex control tasks without significant prior knowledge about the system. However, these methods typically do not provide any safety guarantees, which prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that can provide provable high-probability safety guarantees. To this end, we exploit regularity assumptions on the dynamics in terms of a Gaussian process prior to construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we do not assume that model uncertainties are independent. Based on these predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. In our experiments, we show that the resulting algorithm can be used to safely and efficiently explore and learn about dynamic systems.},
  keywords = {complex control tasks,Data models,dynamic systems,Ellipsoids,Gaussian process,Gaussian processes,high-probability safety guarantees,Kernel,learning (artificial intelligence),learning-based model predictive control scheme,model uncertainties,mpc,predicted trajectories,predictive control,Predictive models,probability,provably accurate confidence intervals,safe-exploration,Safety,safety constraints,safety-critical,Trajectory,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/XQVMNHZC/Koller et al. - 2018 - Learning-Based Model Predictive Control for Safe E.pdf;/Users/scannea1/Zotero/storage/9DEXJKB7/8619572.html}
}

@article{krauseNearOptimal2008,
  title = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}: {{Theory}}, {{Efficient Algorithms}} and {{Empirical Studies}}},
  shorttitle = {Near-{{Optimal Sensor Placements}} in {{Gaussian Processes}}},
  author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  number = {8},
  pages = {235--284},
  urldate = {2021-08-04},
  abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
  file = {/Users/scannea1/Zotero/storage/LRZX27GX/Krause et al. - 2008 - Near-Optimal Sensor Placements in Gaussian Process.pdf}
}

@inproceedings{krauseNonmyopic2007,
  title = {Nonmyopic Active Learning of {{Gaussian}} Processes: An Exploration-Exploitation Approach},
  shorttitle = {Nonmyopic Active Learning of {{Gaussian}} Processes},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Krause, Andreas and Guestrin, Carlos},
  year = {2007},
  month = jun,
  series = {{{ICML}} '07},
  pages = {449--456},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1273496.1273553},
  urldate = {2021-08-04},
  abstract = {When monitoring spatial phenomena, such as the ecological condition of a river, deciding where to make observations is a challenging task. In these settings, a fundamental question is when an active learning, or sequential design, strategy, where locations are selected based on previous measurements, will perform significantly better than sensing at an a priori specified set of locations. For Gaussian Processes (GPs), which often accurately model spatial phenomena, we present an analysis and efficient algorithms that address this question. Central to our analysis is a theoretical bound which quantifies the performance difference between active and a priori design strategies. We consider GPs with unknown kernel parameters and present a nonmyopic approach for trading off exploration, i.e., decreasing uncertainty about the model parameters, and exploitation, i.e., near-optimally selecting observations when the parameters are (approximately) known. We discuss several exploration strategies, and present logarithmic sample complexity bounds for the exploration phase. We then extend our algorithm to handle nonstationary GPs exploiting local structure in the model. We also present extensive empirical evaluation on several real-world problems.},
  isbn = {978-1-59593-793-3},
  file = {/Users/scannea1/Zotero/storage/74BTCB6D/Krause and Guestrin - 2007 - Nonmyopic active learning of Gaussian processes a.pdf}
}

@misc{kristiadiPromisesPitfallsLinearized2023,
  title = {Promises and {{Pitfalls}} of the {{Linearized Laplace}} in {{Bayesian Optimization}}},
  author = {Kristiadi, Agustinus and Immer, Alexander and Eschenhagen, Runa and Fortuin, Vincent},
  year = {2023},
  month = jul,
  number = {arXiv:2304.08309},
  eprint = {2304.08309},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {The linearized-Laplace approximation (LLA) has been shown to be effective and efficient in constructing Bayesian neural networks. It is theoretically compelling since it can be seen as a Gaussian process posterior with the mean function given by the neural network's maximum-a-posteriori predictive function and the covariance function induced by the empirical neural tangent kernel. However, while its efficacy has been studied in large-scale tasks like image classification, it has not been studied in sequential decision-making problems like Bayesian optimization where Gaussian processes -- with simple mean functions and kernels such as the radial basis function -- are the de-facto surrogate models. In this work, we study the usefulness of the LLA in Bayesian optimization and highlight its strong performance and flexibility. However, we also present some pitfalls that might arise and a potential problem with the LLA when the search space is unbounded.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: AABI 2023},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kristiadi et al-2023 Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization/Kristiadi et al_2023_Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization.pdf;/Users/scannea1/Zotero/storage/Q2WRDGAD/2304.html}
}

@inproceedings{krizhevskyImageNet2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  volume = {25},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-15},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file = {/Users/scannea1/Zotero/storage/RHQ56BCC/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf}
}

@inproceedings{kumarConservativeQLearningOffline2020,
  title = {Conservative {{Q-Learning}} for {{Offline Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  year = {2020},
  volume = {33},
  pages = {1179--1191},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-30},
  abstract = {Effectively leveraging large, previously collected datasets in reinforcement learn- ing (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a policy learning procedure with theoretical improvement guarantees. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.},
  keywords = {\_tablet\_modified},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Kumar et al-2020 Conservative Q-Learning for Offline Reinforcement Learning/Kumar et al_2020_Conservative Q-Learning for Offline Reinforcement Learning.pdf}
}

@article{kurutachModelEnsemble2018,
  title = {Model-{{Ensemble Trust-Region Policy Optimization}}},
  author = {Kurutach, Thanard and Clavera, I. and Duan, Yan and Tamar, Aviv and Abbeel, P.},
  year = {2018},
  journal = {ICLR},
  abstract = {This paper analyzes the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and shows that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning. However, they tend to suffer from high sample complexity, which hinders their use in real-world domains. Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.},
  file = {/Users/scannea1/Zotero/storage/BGUTUYR2/Kurutach et al. - 2018 - Model-Ensemble Trust-Region Policy Optimization.pdf}
}

@phdthesis{kussGaussian2006,
  title = {Gaussian {{Process Models}} for {{Robust Regression}}, {{Classification}}, and {{Reinforcement Learning}}},
  author = {Kuss, Malte},
  year = {2006},
  urldate = {2021-09-24},
  abstract = {Our goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems.},
  langid = {english},
  school = {Technische Universit{\"a}t Darmstadt, Darmstadt, Germany},
  file = {/Users/scannea1/Zotero/storage/PA5ZCPBF/Kuss - 2006 - Gaussian Process Models for Robust Regression, Cla.pdf;/Users/scannea1/Zotero/storage/CE36ZCDW/4050.html}
}

@article{lambertLowLevel2019,
  title = {Low-{{Level Control}} of a {{Quadrotor With Deep Model-Based Reinforcement Learning}}},
  author = {Lambert, Nathan O. and Drew, Daniel S. and Yaconelli, Joseph and Levine, Sergey and Calandra, Roberto and Pister, Kristofer S. J.},
  year = {2019},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {4},
  number = {4},
  pages = {4224--4230},
  issn = {2377-3766},
  doi = {10.1109/LRA.2019.2930489},
  abstract = {Designing effective low-level robot controllers often entail platform-specific implementations that require manual heuristic parameter tuning, significant system knowledge, or long design times. With the rising number of robotic and mechatronic systems deployed across areas ranging from industrial automation to intelligent toys, the need for a general approach to generating low-level controllers is increasing. To address the challenge of rapidly generating low-level controllers, we argue for using model-based reinforcement learning (MBRL) trained on relatively small amounts of automatically generated (i.e., without system simulation) data. In this letter, we explore the capabilities of MBRL on a Crazyflie centimeter-scale quadrotor with rapid dynamics to predict and control at {$\leq$}50 Hz. To our knowledge, this is the first use of MBRL for controlled hover of a quadrotor using only on-board sensors, direct motor input signals, and no initial dynamics knowledge. Our controller leverages rapid simulation of a neural network forward dynamics model on a graphic processing unit enabled base station, which then transmits the best current action to the quadrotor firmware via radio. In our experiments, the quadrotor achieved hovering capability of up to 6 s with 3 min of experimental training data.},
  keywords = {aerial systems: mechanics and control,Attitude control,Data models,Deep learning in robotics and automation,Predictive models,Pulse width modulation,Robots,Trajectory,Vehicle dynamics},
  file = {/Users/scannea1/Zotero/storage/NU9R9BNE/Lambert et al. - 2019 - Low-Level Control of a Quadrotor With Deep Model-B.pdf;/Users/scannea1/Zotero/storage/DMLF4SDJ/8769882.html}
}

@misc{lambertObjectiveMismatchModelbased2021,
  title = {Objective {{Mismatch}} in {{Model-based Reinforcement Learning}}},
  author = {Lambert, Nathan and Amos, Brandon and Yadan, Omry and Calandra, Roberto},
  year = {2021},
  month = apr,
  number = {arXiv:2002.04523},
  eprint = {2002.04523},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.04523},
  urldate = {2022-11-07},
  abstract = {Model-based reinforcement learning (MBRL) has been shown to be a powerful framework for data-efficiently learning control of continuous tasks. Recent work in MBRL has mostly focused on using more advanced function approximators and planning schemes, with little development of the general framework. In this paper, we identify a fundamental issue of the standard MBRL framework -- what we call the objective mismatch issue. Objective mismatch arises when one objective is optimized in the hope that a second, often uncorrelated, metric will also be optimized. In the context of MBRL, we characterize the objective mismatch between training the forward dynamics model w.r.t.{\textasciitilde}the likelihood of the one-step ahead prediction, and the overall goal of improving performance on a downstream control task. For example, this issue can emerge with the realization that dynamics models effective for a specific task do not necessarily need to be globally accurate, and vice versa globally accurate models might not be sufficiently accurate locally to obtain good control performance on a specific task. In our experiments, we study this objective mismatch issue and demonstrate that the likelihood of one-step ahead predictions is not always correlated with control performance. This observation highlights a critical limitation in the MBRL framework which will require further research to be fully understood and addressed. We propose an initial method to mitigate the mismatch issue by re-weighting dynamics model training. Building on it, we conclude with a discussion about other potential directions of research for addressing this issue.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  note = {Comment: 9 pages, 2 pages references, 5 pages appendices},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lambert et al-2021 Objective Mismatch in Model-based Reinforcement Learning/Lambert et al_2021_Objective Mismatch in Model-based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/TTWIQRAE/2002.html}
}

@phdthesis{lambertSynergy2022,
  title = {Synergy of {{Prediction}} and {{Control}} in {{Model-based Reinforcement Learning}}},
  author = {Lambert, Nathan},
  year = {2022},
  month = may,
  urldate = {2022-05-12},
  file = {/Users/scannea1/Zotero/storage/ZE9E4ZKP/EECS-2022-65.pdf;/Users/scannea1/Zotero/storage/UWRG3272/EECS-2022-65.html}
}

@inproceedings{langeAutonomousReinforcementLearning2012,
  title = {Autonomous Reinforcement Learning on Raw Visual Input Data in a Real World Application},
  booktitle = {The 2012 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Lange, Sascha and Riedmiller, Martin and Voigtl{\"a}nder, Arne},
  year = {2012},
  month = jun,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2012.6252823},
  urldate = {2023-11-02},
  abstract = {We propose a learning architecture, that is able to do reinforcement learning based on raw visual input data. In contrast to previous approaches, not only the control policy is learned. In order to be successful, the system must also autonomously learn, how to extract relevant information out of a high-dimensional stream of input information, for which the semantics are not provided to the learning system. We give a first proof-of-concept of this novel learning architecture on a challenging benchmark, namely visual control of a racing slot car. The resulting policy, learned only by success or failure, is hardly beaten by an experienced human player.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lange et al-2012 Autonomous reinforcement learning on raw visual input data in a real world/Lange et al_2012_Autonomous reinforcement learning on raw visual input data in a real world.pdf;/Users/scannea1/Zotero/storage/J73IW626/6252823.html}
}

@inproceedings{larsenBisimulationProbabilisticTesting1989,
  title = {Bisimulation through Probabilistic Testing (Preliminary Report)},
  booktitle = {Proceedings of the 16th {{ACM SIGPLAN-SIGACT}} Symposium on {{Principles}} of Programming Languages},
  author = {Larsen, K. G. and Skou, A.},
  year = {1989},
  month = jan,
  series = {{{POPL}} '89},
  pages = {344--352},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/75277.75307},
  urldate = {2023-11-02},
  abstract = {We propose a language for testing concurrent processes and examine its strength in terms of the processes that are distinguished by a test. By using probabilistic transition systems as the underlying semantic model, we show how a testing algorithm with a probability arbitrary close to 1 can distinguish processes that are not bisimulation equivalent. We also show a similar result (in a slightly stronger form) for a new process relation called 2/3-bisimulation {\textemdash} lying strictly between that of simulation and bisimulation. Finally, the ultimately strength of the testing language is shown to identify an even stronger process relation, called probabilistic bisimulation.},
  isbn = {978-0-89791-294-5},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Larsen_Skou-1989 Bisimulation through probabilistic testing (preliminary report)/Larsen_Skou_1989_Bisimulation through probabilistic testing (preliminary report).pdf}
}

@inproceedings{laskinCURLContrastiveUnsupervised2020,
  title = {{{CURL}}: {{Contrastive Unsupervised Representations}} for {{Reinforcement Learning}}},
  shorttitle = {{{CURL}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  year = {2020},
  month = nov,
  pages = {5639--5650},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://www.github.com/MishaLaskin/curl.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Laskin et al-2020 CURL/Laskin et al_2020_CURL.pdf;/Users/scannea1/Zotero/storage/EJIYMHQC/Laskin et al. - 2020 - CURL Contrastive Unsupervised Representations for.pdf}
}

@inproceedings{lavoieSimplicialEmbeddingsSelfSupervised2022,
  title = {Simplicial {{Embeddings}} in {{Self-Supervised Learning}} and {{Downstream Classification}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Lavoie, Samuel and Tsirigotis, Christos and Schwarzer, Max and Vani, Ankit and Noukhovitch, Michael and Kawaguchi, Kenji and Courville, Aaron},
  year = {2022},
  month = sep,
  urldate = {2023-11-02},
  abstract = {Simplicial Embeddings (SEM) are representations learned through self-supervised learning (SSL), wherein a representation is projected into \$L\$ simplices of \$V\$ dimensions each using a {\textbackslash}texttt\{softmax\} operation. This procedure conditions the representation onto a constrained space during pretraining and imparts an inductive bias for group sparsity. For downstream classification, we formally prove that the SEM representation leads to better generalization than an unnormalized representation. Furthermore, we empirically demonstrate that SSL methods trained with SEMs have improved generalization on natural image datasets such as CIFAR-100 and ImageNet. Finally, when used in a downstream classification task, we show that SEM features exhibit emergent semantic coherence where small groups of learned features are distinctly predictive of semantically-relevant classes.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lavoie et al-2022 Simplicial Embeddings in Self-Supervised Learning and Downstream Classification/Lavoie et al_2022_Simplicial Embeddings in Self-Supervised Learning and Downstream Classification.pdf}
}

@article{lecunGradientbased1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/scannea1/Zotero/storage/6GIQW4J5/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/scannea1/Zotero/storage/LMPT8P2U/726791.html}
}

@article{leeGPILQG2017,
  title = {{{GP-ILQG}}: {{Data-driven Robust Optimal Control}} for {{Uncertain Nonlinear Dynamical Systems}}},
  shorttitle = {{{GP-ILQG}}},
  author = {Lee, Gilwoo and Srinivasa, Siddhartha S. and Mason, Matthew T.},
  year = {2017},
  month = may,
  journal = {arXiv:1705.05344 [cs]},
  eprint = {1705.05344},
  primaryclass = {cs},
  urldate = {2021-06-30},
  abstract = {As we aim to control complex systems, use of a simulator in model-based reinforcement learning is becoming more common. However, it has been challenging to overcome the Reality Gap, which comes from nonlinear model bias and susceptibility to disturbance. To address these problems, we propose a novel algorithm that combines data-driven system identification approach (Gaussian Process) with a Differential-Dynamic-Programming-based robust optimal control method (Iterative Linear Quadratic Control). Our algorithm uses the simulator's model as the mean function for a Gaussian Process and learns only the difference between the simulator's prediction and actual observations, making it a natural hybrid of simulation and real-world observation. We show that our approach quickly corrects incorrect models, comes up with robust optimal controllers, and transfers its acquired model knowledge to new tasks efficiently.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/scannea1/Zotero/storage/KEQGYL7T/Lee et al. - 2017 - GP-ILQG Data-driven Robust Optimal Control for Un.pdf;/Users/scannea1/Zotero/storage/2NUXIVH4/1705.html}
}

@inproceedings{leeOfflinetoOnlineReinforcementLearning2022,
  title = {Offline-to-{{Online Reinforcement Learning}} via {{Balanced Replay}} and {{Pessimistic Q-Ensemble}}},
  booktitle = {Proceedings of the 5th {{Conference}} on {{Robot Learning}}},
  author = {Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo},
  year = {2022},
  month = jan,
  pages = {1702--1712},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-13},
  abstract = {Recent advance in deep offline reinforcement learning (RL) has made it possible to train strong robotic agents from offline datasets. However, depending on the quality of the trained agents and the application being considered, it is often desirable to fine-tune such agents via further online interactions. In this paper, we observe that state-action distribution shift may lead to severe bootstrap error during fine-tuning, which destroys the good initial policy obtained via offline RL. To address this issue, we first propose a balanced replay scheme that prioritizes samples encountered online while also encouraging the use of near-on-policy samples from the offline dataset. Furthermore, we leverage multiple Q-functions trained pessimistically offline, thereby preventing overoptimism concerning unfamiliar actions at novel states during the initial training phase. We show that the proposed method improves sample-efficiency and final performance of the fine-tuned robotic agents on various locomotion and manipulation tasks. Our code is available at: https://github.com/shlee94/Off2OnRL.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lee et al-2022 Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic/Lee et al_2022_Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic.pdf}
}

@inproceedings{leeStochasticLatentActorCritic2020,
  title = {Stochastic {{Latent Actor-Critic}}: {{Deep Reinforcement Learning}} with a {{Latent Variable Model}}},
  shorttitle = {Stochastic {{Latent Actor-Critic}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lee, Alex X. and Nagabandi, Anusha and Abbeel, Pieter and Levine, Sergey},
  year = {2020},
  volume = {33},
  pages = {741--752},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-01-27},
  abstract = {Deep reinforcement learning (RL) algorithms can use high-capacity deep networks to learn directly from image observations. However, these high-dimensional observation spaces present a number of  challenges in practice, since the policy must now solve two problems: representation learning and task learning. In this work, we tackle these two problems separately, by explicitly learning latent representations that can accelerate reinforcement learning from images. We propose the stochastic latent actor-critic (SLAC) algorithm: a sample-efficient and high-performing RL algorithm for learning policies for complex continuous control tasks directly from high-dimensional image inputs. SLAC provides a novel and principled approach for unifying stochastic sequential models and RL into a single method, by learning a compact latent representation and then performing RL in the model's learned latent space. Our experimental evaluation demonstrates that our method outperforms both model-free and model-based alternatives in terms of final performance and sample efficiency, on a range of difficult image-based control tasks. Our code and videos of our results are available at our website.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lee et al-2020 Stochastic Latent Actor-Critic/Lee et al_2020_Stochastic Latent Actor-Critic.pdf}
}

@article{leibfriedTutorial2021,
  title = {A {{Tutorial}} on {{Sparse Gaussian Processes}} and {{Variational Inference}}},
  author = {Leibfried, Felix and Dutordoir, Vincent and John, S. T. and Durrande, Nicolas},
  year = {2021},
  month = feb,
  journal = {arXiv:2012.13962 [cs, stat]},
  eprint = {2012.13962},
  primaryclass = {cs, stat},
  abstract = {Gaussian processes (GPs) provide a framework for Bayesian inference that can offer principled uncertainty estimates for a large range of problems. For example, if we consider regression problems with Gaussian likelihoods, a GP model enjoys a posterior in closed form. However, identifying the posterior GP scales cubically with the number of training examples and requires to store all examples in memory. In order to overcome these obstacles, sparse GPs have been proposed that approximate the true posterior GP with pseudo-training examples. Importantly, the number of pseudo-training examples is user-defined and enables control over computational and memory complexity. In the general case, sparse GPs do not enjoy closed-form solutions and one has to resort to approximate inference. In this context, a convenient choice for approximate inference is variational inference (VI), where the problem of Bayesian inference is cast as an optimization problem -- namely, to maximize a lower bound of the log marginal likelihood. This paves the way for a powerful and versatile framework, where pseudo-training examples are treated as optimization arguments of the approximate posterior that are jointly identified together with hyperparameters of the generative model (i.e. prior and likelihood). The framework can naturally handle a wide scope of supervised learning problems, ranging from regression with heteroscedastic and non-Gaussian likelihoods to classification problems with discrete labels, but also multilabel problems. The purpose of this tutorial is to provide access to the basic matter for readers without prior knowledge in both GPs and VI. A proper exposition to the subject enables also access to more recent advances (like importance-weighted VI as well as inderdomain, multioutput and deep GPs) that can serve as an inspiration for new research ideas.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/P96DJSJL/Leibfried et al. - 2021 - A Tutorial on Sparse Gaussian Processes and Variat.pdf;/Users/scannea1/Zotero/storage/Z55ZREHB/2012.html}
}

@misc{leibfriedVariationalInferenceModelFree2022,
  title = {Variational {{Inference}} for {{Model-Free}} and {{Model-Based Reinforcement Learning}}},
  author = {Leibfried, Felix},
  year = {2022},
  month = sep,
  number = {arXiv:2209.01693},
  eprint = {2209.01693},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.01693},
  urldate = {2022-09-14},
  abstract = {Variational inference (VI) is a specific type of approximate Bayesian inference that approximates an intractable posterior distribution with a tractable one. VI casts the inference problem as an optimization problem, more specifically, the goal is to maximize a lower bound of the logarithm of the marginal likelihood with respect to the parameters of the approximate posterior. Reinforcement learning (RL) on the other hand deals with autonomous agents and how to make them act optimally such as to maximize some notion of expected future cumulative reward. In the non-sequential setting where agents' actions do not have an impact on future states of the environment, RL is covered by contextual bandits and Bayesian optimization. In a proper sequential scenario, however, where agents' actions affect future states, instantaneous rewards need to be carefully traded off against potential long-term rewards. This manuscript shows how the apparently different subjects of VI and RL are linked in two fundamental ways. First, the optimization objective of RL to maximize future cumulative rewards can be recovered via a VI objective under a soft policy constraint in both the non-sequential and the sequential setting. This policy constraint is not just merely artificial but has proven as a useful regularizer in many RL tasks yielding significant improvements in agent performance. And second, in model-based RL where agents aim to learn about the environment they are operating in, the model-learning part can be naturally phrased as an inference problem over the process that governs environment dynamics. We are going to distinguish between two scenarios for the latter: VI when environment states are fully observable by the agent and VI when they are only partially observable through an observation distribution.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: arXiv admin note: substantial text overlap with arXiv:2012.13962},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Leibfried-2022 Variational Inference for Model-Free and Model-Based Reinforcement Learning/Leibfried_2022_Variational Inference for Model-Free and Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/DDVDJY48/2209.html}
}

@inproceedings{levineGuided2013,
  title = {Guided {{Policy Search}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  year = {2013},
  month = may,
  pages = {1--9},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-06-17},
  abstract = {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and o...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3256U5QT/Levine and Koltun - 2013 - Guided Policy Search.pdf;/Users/scannea1/Zotero/storage/VKD6XSNY/levine13.html}
}

@inproceedings{levineLearning2014,
  title = {Learning {{Neural Network Policies}} with {{Guided Policy Search}} under {{Unknown Dynamics}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levine, Sergey and Abbeel, Pieter},
  year = {2014},
  volume = {27},
  urldate = {2021-06-17},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/M7QQLZRR/Levine and Abbeel - 2014 - Learning Neural Network Policies with Guided Polic.pdf;/Users/scannea1/Zotero/storage/RUZWAZFU/6766aa2750c19aad2fa1b32f36ed4aee-Abstract.html}
}

@article{levineReinforcement2018,
  title = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}: {{Tutorial}} and {{Review}}},
  shorttitle = {Reinforcement {{Learning}} and {{Control}} as {{Probabilistic Inference}}},
  author = {Levine, Sergey},
  year = {2018},
  journal = {ArXiv},
  abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  file = {/Users/scannea1/Zotero/storage/74K38XL7/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf}
}

@inproceedings{levineVariational2013,
  title = {Variational {{Policy Search}} via {{Trajectory Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Levine, Sergey and Koltun, Vladlen},
  year = {2013},
  volume = {26},
  urldate = {2021-08-04},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/LHGXIJIL/Levine and Koltun - 2013 - Variational Policy Search via Trajectory Optimizat.pdf;/Users/scannea1/Zotero/storage/RSGHGXI9/38af86134b65d0f10fe33d30dd76442e-Abstract.html}
}

@techreport{liaoAdvantages1992,
  title = {Advantages of {{Differential Dynamic Programming Over Newton}}''s {{Method}} for {{Discrete-time Optimal Control Problems}}},
  author = {Liao, L. and Shoemaker, C.},
  year = {1992},
  institution = {{Cornell University, Ithaca, NY}},
  urldate = {2021-06-21},
  abstract = {Differential Dynamic Programming (DDP) and stagewise Newton\&\#39;\&\#39;s method are both quadratically convergent algorithms for solving discrete time optimal control problems. Although these two algorithms share many theoretical similarities, they demonstrate significantly different numerical performance. In this paper, we will compare and analyze these two algorithms in detail and derive another quadratically convergent algorithm which is a combination of the DDP algorithm and Newton\&\#39;\&\#39;s method. This new second-order algorithm plays a key role in the explanation of the numerical differences between the DDP algorithm and Newton\&\#39;\&\#39;s method. The detailed algorithmic and structural differences for these three algorithms and their impact on numerical performance will be discussed and explored. Two test problems with various dimensions solved by these three algorithms will be presented. One nonlinear test problem demonstrates that the DDP algorithm can be as much as 28 times faster than the stagewise Newton\&\#39;\&\#39;s method. The numerical comparsion indicates that the DDP algorithm is numerically superior to the stagewise Newton\&\#39;\&\#39;s method.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/9GC5A94M/787b49f4ad64f175d6afa47fd16068d2281be2c5.html}
}

@inproceedings{liDoesSelfsupervisedLearning2022,
  title = {Does {{Self-supervised Learning Really Improve Reinforcement Learning}} from {{Pixels}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Xiang and Shang, Jinghuan and Das, Srijan and Ryoo, Michael},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {30865--30881},
  urldate = {2023-11-23},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/HMF2B7ZQ/Li et al. - 2022 - Does Self-supervised Learning Really Improve Reinf.pdf}
}

@inproceedings{lillicrapContinuousControlDeep2016,
  title = {Continuous Control with Deep Reinforcement Learning},
  booktitle = {4th International Conference on Learning Representations, {{ICLR}} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/LillicrapHPHETS15.bib},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 10 pages + supplementary},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lillicrap et al-2019 Continuous control with deep reinforcement learning/Lillicrap et al_2019_Continuous control with deep reinforcement learning2.pdf;/Users/scannea1/Zotero/storage/HWS4YW6U/1509.html}
}

@inproceedings{lindingerMeanField2020,
  title = {Beyond the {{Mean-Field}}: {{Structured Deep Gaussian Processes Improve}} the {{Predictive Uncertainties}}},
  shorttitle = {Beyond the {{Mean-Field}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Lindinger, J. and Reeb, D. and Lippert, C. and Rakitsch, Barbara},
  year = {2020},
  volume = {34},
  abstract = {Deep Gaussian Processes learn probabilistic data representations for supervised learning by cascading multiple Gaussian Processes. While this model family promises flexible predictive distributions, exact inference is not tractable. Approximate inference techniques trade off the ability to closely resemble the posterior distribution against speed of convergence and computational efficiency. We propose a novel Gaussian variational family that allows for retaining covariances between latent processes while achieving fast convergence by marginalising out all global latent variables. After providing a proof of how this marginalisation can be done for general covariances, we restrict them to the ones we empirically found to be most important in order to also achieve computational efficiency. We provide an efficient implementation of our new approach and apply it to several regression benchmark datasets. We find that it yields more accurate predictive distributions, in particular for test data points that are distant from the training set.},
  file = {/Users/scannea1/Zotero/storage/5FZWEGC3/Lindinger et al. - 2020 - Beyond the Mean-Field Structured Deep Gaussian Pr.pdf}
}

@misc{linLearningModelWorld2023,
  title = {Learning to {{Model}} the {{World}} with {{Language}}},
  author = {Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner, Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca},
  year = {2023},
  month = jul,
  number = {arXiv:2308.01399},
  eprint = {2308.01399},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Website: https://dynalang.github.io/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lin et al-2023 Learning to Model the World with Language/Lin et al_2023_Learning to Model the World with Language.pdf;/Users/scannea1/Zotero/storage/MJ2XLJJA/2308.html}
}

@misc{linLearningModelWorld2023a,
  title = {Learning to {{Model}} the {{World}} with {{Language}}},
  author = {Lin, Jessy and Du, Yuqing and Watkins, Olivia and Hafner, Danijar and Abbeel, Pieter and Klein, Dan and Dragan, Anca},
  year = {2023},
  month = jul,
  number = {arXiv:2308.01399},
  eprint = {2308.01399},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.01399},
  urldate = {2023-10-25},
  abstract = {To interact with humans in the world, agents need to understand the diverse types of language that people use, relate them to the visual world, and act based on them. While current agents learn to execute simple language instructions from task rewards, we aim to build agents that leverage diverse language that conveys general knowledge, describes the state of the world, provides interactive feedback, and more. Our key idea is that language helps agents predict the future: what will be observed, how the world will behave, and which situations will be rewarded. This perspective unifies language understanding with future prediction as a powerful self-supervised learning objective. We present Dynalang, an agent that learns a multimodal world model that predicts future text and image representations and learns to act from imagined model rollouts. Unlike traditional agents that use language only to predict actions, Dynalang acquires rich language understanding by using past language also to predict future language, video, and rewards. In addition to learning from online interaction in an environment, Dynalang can be pretrained on datasets of text, video, or both without actions or rewards. From using language hints in grid worlds to navigating photorealistic scans of homes, Dynalang utilizes diverse types of language to improve task performance, including environment descriptions, game rules, and instructions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: Website: https://dynalang.github.io/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lin et al-2023 Learning to Model the World with Language/Lin et al_2023_Learning to Model the World with Language2.pdf;/Users/scannea1/Zotero/storage/BTP3NHF6/2308.html}
}

@misc{liStudyBayesianNeural2023,
  title = {A {{Study}} of {{Bayesian Neural Network Surrogates}} for {{Bayesian Optimization}}},
  author = {Li, Yucen Lily and Rudner, Tim G. J. and Wilson, Andrew Gordon},
  year = {2023},
  month = may,
  number = {arXiv:2305.20028},
  eprint = {2305.20028},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.20028},
  urldate = {2023-09-30},
  abstract = {Bayesian optimization is a highly efficient approach to optimizing objective functions which are expensive to query. These objectives are typically represented by Gaussian process (GP) surrogate models which are easy to optimize and support exact inference. While standard GP surrogates have been well-established in Bayesian optimization, Bayesian neural networks (BNNs) have recently become practical function approximators, with many benefits over standard GPs such as the ability to naturally handle non-stationarity and learn representations for high-dimensional data. In this paper, we study BNNs as alternatives to standard GP surrogates for optimization. We consider a variety of approximate inference procedures for finite-width BNNs, including high-quality Hamiltonian Monte Carlo, low-cost stochastic MCMC, and heuristics such as deep ensembles. We also consider infinite-width BNNs and partially stochastic models such as deep kernel learning. We evaluate this collection of surrogate models on diverse problems with varying dimensionality, number of objectives, non-stationarity, and discrete and continuous inputs. We find: (i) the ranking of methods is highly problem dependent, suggesting the need for tailored inductive biases; (ii) HMC is the most successful approximate inference procedure for fully stochastic BNNs; (iii) full stochasticity may be unnecessary as deep kernel learning is relatively competitive; (iv) infinite-width BNNs are particularly promising, especially in high dimensions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Li et al-2023 A Study of Bayesian Neural Network Surrogates for Bayesian Optimization/Li et al_2023_A Study of Bayesian Neural Network Surrogates for Bayesian Optimization.pdf;/Users/scannea1/Zotero/storage/542LNRKQ/2305.html}
}

@article{liuConstrained2021,
  title = {Constrained {{Model-based Reinforcement Learning}} with {{Robust Cross-Entropy Method}}},
  author = {Liu, Zuxin and Zhou, Hongyi and Chen, Baiming and Zhong, Sicheng and Hebert, Martial and Zhao, Ding},
  year = {2021},
  month = mar,
  journal = {arXiv:2010.07968 [cs]},
  eprint = {2010.07968},
  primaryclass = {cs},
  urldate = {2022-01-11},
  abstract = {This paper studies the constrained/safe reinforcement learning (RL) problem with sparse indicator signals for constraint violations. We propose a model-based approach to enable RL agents to effectively explore the environment with unknown system dynamics and environment constraints given a significantly small number of violation budgets. We employ the neural network ensemble model to estimate the prediction uncertainty and use model predictive control as the basic control framework. We propose the robust cross-entropy method to optimize the control sequence considering the model uncertainty and constraints. We evaluate our methods in the Safety Gym environment. The results show that our approach learns to complete the tasks with a much smaller number of constraint violations than state-of-the-art baselines. Additionally, we are able to achieve several orders of magnitude better sample efficiency when compared with constrained model-free RL approaches. The code is available at {\textbackslash}url\{https://github.com/liuzuxin/safe-mbrl\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: 8 pages, 5 figures},
  file = {/Users/scannea1/Zotero/storage/DSGIGWHF/Liu et al. - 2021 - Constrained Model-based Reinforcement Learning wit.pdf;/Users/scannea1/Zotero/storage/MKP3BWF3/2010.html}
}

@inproceedings{liuMaskedAutoencodingScalable2022,
  title = {Masked {{Autoencoding}} for {{Scalable}} and {{Generalizable Decision Making}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Fangchen and Liu, Hao and Grover, Aditya and Abbeel, Pieter},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {12608--12618},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/XMY7QSGM/Liu et al. - 2022 - Masked Autoencoding for Scalable and Generalizable.pdf}
}

@inproceedings{liUnifiedTheoryState2006,
  title = {Towards a {{Unified Theory}} of {{State Abstraction}} for {{MDPs}}.},
  booktitle = {Proceedings of the {{Ninth International Symposium}} on {{Artificial Intelligence}} and {{Mathematics}}},
  author = {Li, Lihong and Walsh, Thomas and Littman, Michael},
  year = {2006},
  month = jan,
  abstract = {State abstraction (or state aggregation) has been extensively studied in the fields of artificial intel- ligence and operations research. Instead of work- ing in the ground state space, the decision maker usually finds solutions in the abstract state space much faster by treating groups of states as a unit by ignoring irrelevant state information. A num- ber of abstractions have been proposed and studied in the reinforcement-learning and planning litera- tures, and positive and negative results are known. We provide a unified treatment of state abstraction for Markov decision processes. We study five partic- ular abstraction schemes, some of which have been proposed in the past in dierent forms, and analyze their usability for planning and learning.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Li et al-2006 Towards a Unified Theory of State Abstraction for MDPs/Li et al_2006_Towards a Unified Theory of State Abstraction for MDPs.pdf}
}

@inproceedings{liuSimplePrincipledUncertainty2020,
  title = {Simple and {{Principled Uncertainty Estimation}} with {{Deterministic Deep Learning}} via {{Distance Awareness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Jeremiah and Lin, Zi and Padhy, Shreyas and Tran, Dustin and Bedrax Weiss, Tania and Lakshminarayanan, Balaji},
  year = {2020},
  volume = {33},
  pages = {7498--7512},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-01-25},
  abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model.  However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer.  On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Liu et al-2020 Simple and Principled Uncertainty Estimation with Deterministic Deep Learning/Liu et al_2020_Simple and Principled Uncertainty Estimation with Deterministic Deep Learning.pdf}
}

@book{ljungSystem1999,
  title = {System {{Identification}}: {{Theory}} for the {{User}}},
  author = {Ljung, Lennart},
  year = {1999},
  series = {Prentice {{Hall Information}} and {{System Sciences Series}}},
  edition = {2nd},
  publisher = {{Pearson}}
}

@article{loeligerFactor2007,
  title = {The {{Factor Graph Approach}} to {{Model-Based Signal Processing}}},
  author = {Loeliger, Hans-Andrea and Dauwels, Justin and Hu, Junli and Korl, Sascha and Ping, Li and Kschischang, Frank R.},
  year = {2007},
  month = jun,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {6},
  pages = {1295--1322},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2007.896497},
  abstract = {The message-passing approach to model-based signal processing is developed with a focus on Gaussian message passing in linear state-space models, which includes recursive least squares, linear minimum-mean-squared-error estimation, and Kalman filtering algorithms. Tabulated message computation rules for the building blocks of linear models allow us to compose a variety of such algorithms without additional derivations or computations. Beyond the Gaussian case, it is emphasized that the message-passing approach encourages us to mix and match different algorithmic techniques, which is exemplified by two different approaches - steepest descent and expectation maximization - to message passing through a multiplier node.},
  keywords = {Algorithm design and analysis,Estimation,factor graphs,graphical models,Graphical models,Information technology,Kalman filtering,Kalman filters,Least squares approximation,Machine learning algorithms,message passing,Message passing,Signal design,signal processing,Signal processing,Signal processing algorithms},
  file = {/Users/scannea1/Zotero/storage/JBWKUUSB/Loeliger et al. - 2007 - The Factor Graph Approach to Model-Based Signal Pr.pdf;/Users/scannea1/Zotero/storage/6ARVVJ8G/4282128.html}
}

@inproceedings{lotfiBayesian2022,
  title = {Bayesian {{Model Selection}}, the {{Marginal Likelihood}}, and {{Generalization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  year = {2022},
  month = jun,
  pages = {14223--14247},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We provide a partial remedy through a conditional marginal likelihood, which we show is more aligned with generalization, and practically valuable for large-scale hyperparameter learning, such as in deep kernel learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4ZKFR66N/Lotfi et al. - 2022 - Bayesian Model Selection, the Marginal Likelihood,.pdf}
}

@article{lowreyPlan2019,
  title = {Plan {{Online}}, {{Learn Offline}}: {{Efficient Learning}} and {{Exploration}} via {{Model-Based Control}}},
  shorttitle = {Plan {{Online}}, {{Learn Offline}}},
  author = {Lowrey, Kendall and Rajeswaran, A. and Kakade, S. and Todorov, E. and Mordatch, Igor},
  year = {2019},
  journal = {ICLR},
  abstract = {A plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world and how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. We propose a plan online and learn offline (POLO) framework for the setting where an agent, with an internal model, needs to continually act and learn in the world. Our work builds on the synergistic relationship between local model-based control, global value function learning, and exploration. We study how local trajectory optimization can cope with approximation errors in the value function, and can stabilize and accelerate value function learning. Conversely, we also study how approximate value functions can help reduce the planning horizon and allow for better policies beyond local solutions. Finally, we also demonstrate how trajectory optimization can be used to perform temporally coordinated exploration in conjunction with estimating uncertainty in value function approximation. This exploration is critical for fast and stable learning of the value function. Combining these components enable solutions to complex simulated control tasks, like humanoid locomotion and dexterous in-hand manipulation, in the equivalent of a few minutes of experience in the real world.},
  file = {/Users/scannea1/Zotero/storage/AW7BCGGU/Lowrey et al. - 2019 - Plan Online, Learn Offline Efficient Learning and.pdf}
}

@inproceedings{luisModelBasedUncertaintyValue2023,
  title = {Model-{{Based Uncertainty}} in {{Value Functions}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Luis, Carlos E. and Bottero, Alessandro G. and Vinogradska, Julia and Berkenkamp, Felix and Peters, Jan},
  year = {2023},
  month = apr,
  pages = {8029--8052},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {We consider the problem of quantifying uncertainty over expected cumulative rewards in model-based reinforcement learning. In particular, we focus on characterizing the variance over values induced by a distribution over MDPs. Previous work upper bounds the posterior variance over values by solving a so-called uncertainty Bellman equation, but the over-approximation may result in inefficient exploration. We propose a new uncertainty Bellman equation whose solution converges to the true posterior variance over values and explicitly characterizes the gap in previous work. Moreover, our uncertainty quantification technique is easily integrated into common exploration strategies and scales naturally beyond the tabular setting by using standard deep reinforcement learning architectures. Experiments in difficult exploration tasks, both in tabular and continuous control settings, show that our sharper uncertainty estimates improve sample-efficiency.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Luis et al-2023 Model-Based Uncertainty in Value Functions/Luis et al_2023_Model-Based Uncertainty in Value Functions.pdf}
}

@misc{luisValueDistributionalModelBasedReinforcement2023,
  title = {Value-{{Distributional Model-Based Reinforcement Learning}}},
  author = {Luis, Carlos E. and Bottero, Alessandro G. and Vinogradska, Julia and Berkenkamp, Felix and Peters, Jan},
  year = {2023},
  month = aug,
  number = {arXiv:2308.06590},
  eprint = {2308.06590},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {Quantifying uncertainty about a policy's long-term performance is important to solve sequential decision-making tasks. We study the problem from a model-based Bayesian reinforcement learning perspective, where the goal is to learn the posterior distribution over value functions induced by parameter (epistemic) uncertainty of the Markov decision process. Previous work restricts the analysis to a few moments of the distribution over values or imposes a particular distribution shape, e.g., Gaussians. Inspired by distributional reinforcement learning, we introduce a Bellman operator whose fixed-point is the value distribution function. Based on our theory, we propose Epistemic Quantile-Regression (EQR), a model-based algorithm that learns a value distribution function that can be used for policy optimization. Evaluation across several continuous-control tasks shows performance benefits with respect to established model-based and model-free algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Luis et al-2023 Value-Distributional Model-Based Reinforcement Learning/Luis et al_2023_Value-Distributional Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/XTH3GZB2/2308.html}
}

@misc{luSyntheticExperienceReplay2023,
  title = {Synthetic {{Experience Replay}}},
  author = {Lu, Cong and Ball, Philip J. and Teh, Yee Whye and {Parker-Holder}, Jack},
  year = {2023},
  month = oct,
  number = {arXiv:2303.06614},
  eprint = {2303.06614},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.06614},
  urldate = {2023-11-05},
  abstract = {A key theme in the past decade has been that when large neural networks and large datasets combine they can produce remarkable results. In deep reinforcement learning (RL), this paradigm is commonly made possible through experience replay, whereby a dataset of past experiences is used to train a policy or value function. However, unlike in supervised or self-supervised learning, an RL agent has to collect its own data, which is often limited. Thus, it is challenging to reap the benefits of deep learning, and even small neural networks can overfit at the start of training. In this work, we leverage the tremendous recent progress in generative modeling and propose Synthetic Experience Replay (SynthER), a diffusion-based approach to flexibly upsample an agent's collected experience. We show that SynthER is an effective method for training RL agents across offline and online settings, in both proprioceptive and pixel-based environments. In offline settings, we observe drastic improvements when upsampling small offline datasets and see that additional synthetic data also allows us to effectively train larger networks. Furthermore, SynthER enables online agents to train with a much higher update-to-data ratio than before, leading to a significant increase in sample efficiency, without any algorithmic changes. We believe that synthetic training data could open the door to realizing the full potential of deep learning for replay-based RL algorithms from limited data. Finally, we open-source our code at https://github.com/conglu1997/SynthER.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published at NeurIPS, 2023},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lu et al-2023 Synthetic Experience Replay/Lu et al_2023_Synthetic Experience Replay.pdf;/Users/scannea1/Zotero/storage/8UNYZZJ8/2303.html}
}

@article{lyapunovGeneral1992a,
  title = {The General Problem of the Stability of Motion},
  author = {Lyapunov, A. M.},
  year = {1992},
  month = mar,
  journal = {International Journal of Control},
  volume = {55},
  number = {3},
  pages = {531--534},
  publisher = {{Taylor \& Francis}},
  issn = {0020-7179},
  doi = {10.1080/00207179208934253},
  urldate = {2022-05-02},
  file = {/Users/scannea1/Zotero/storage/L73PVNMP/00207179208934253.html}
}

@inproceedings{lyleUnderstandingPlasticityNeural2023,
  title = {Understanding {{Plasticity}} in {{Neural Networks}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Lyle, Clare and Zheng, Zeyu and Nikishin, Evgenii and Pires, Bernardo Avila and Pascanu, Razvan and Dabney, Will},
  year = {2023},
  month = jul,
  pages = {23190--23211},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {Plasticity, the ability of a neural network to quickly change its predictions in response to new information, is essential for the adaptability and robustness of deep reinforcement learning systems. Deep neural networks are known to lose plasticity over the course of training even in relatively simple learning problems, but the mechanisms driving this phenomenon are still poorly understood. This paper conducts a systematic empirical analysis into plasticity loss, with the goal of understanding the phenomenon mechanistically in order to guide the future development of targeted solutions. We find that loss of plasticity is deeply connected to changes in the curvature of the loss landscape, but that it often occurs in the absence of saturated units. Based on this insight, we identify a number of parameterization and optimization design choices which enable networks to better preserve plasticity over the course of training. We validate the utility of these findings on larger-scale RL benchmarks in the Arcade Learning Environment.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lyle et al-2023 Understanding Plasticity in Neural Networks/Lyle et al_2023_Understanding Plasticity in Neural Networks2.pdf}
}

@misc{lyuOffPolicyRLAlgorithms2023,
  title = {Off-{{Policy RL Algorithms Can}} Be {{Sample-Efficient}} for {{Continuous Control}} via {{Sample Multiple Reuse}}},
  author = {Lyu, Jiafei and Wan, Le and Lu, Zongqing and Li, Xiu},
  year = {2023},
  month = may,
  number = {arXiv:2305.18443},
  eprint = {2305.18443},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18443},
  urldate = {2023-11-02},
  abstract = {Sample efficiency is one of the most critical issues for online reinforcement learning (RL). Existing methods achieve higher sample efficiency by adopting model-based methods, Q-ensemble, or better exploration mechanisms. We, instead, propose to train an off-policy RL agent via updating on a fixed sampled batch multiple times, thus reusing these samples and better exploiting them within a single optimization loop. We name our method sample multiple reuse (SMR). We theoretically show the properties of Q-learning with SMR, e.g., convergence. Furthermore, we incorporate SMR with off-the-shelf off-policy RL algorithms and conduct experiments on a variety of continuous control benchmarks. Empirical results show that SMR significantly boosts the sample efficiency of the base methods across most of the evaluated tasks without any hyperparameter tuning or additional tricks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: 37 pages},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Lyu et al-2023 Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via/Lyu et al_2023_Off-Policy RL Algorithms Can be Sample-Efficient for Continuous Control via.pdf;/Users/scannea1/Zotero/storage/5JDSBW45/2305.html}
}

@inproceedings{ma2021contrastive,
  title={Contrastive variational reinforcement learning for complex observations},
  author={Ma, Xiao and Chen, Siwei and Hsu, David and Lee, Wee Sun},
  booktitle={Conference on Robot Learning},
  pages={959--972},
  year={2021},
  organization={PMLR}
}

@article{mackayBayesian1992,
  title = {Bayesian {{Interpolation}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {415--447},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.415},
  urldate = {2021-10-13},
  abstract = {Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularization and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modeling problems. Regularizing constants are set by examining their posterior probability distribution. Alternative regularizers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. ``Occam's razor'' is automatically embodied by this process. The way in which Bayes infers the values of regularizing constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.},
  file = {/Users/scannea1/Zotero/storage/3GJG8PUA/MacKay - 1992 - Bayesian Interpolation.pdf;/Users/scannea1/Zotero/storage/H5INT59A/Bayesian-Interpolation.html}
}

@article{mackayInformationBased1992,
  title = {Information-{{Based Objective Functions}} for {{Active Data Selection}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {590--604},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.590},
  urldate = {2021-10-13},
  abstract = {Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.},
  file = {/Users/scannea1/Zotero/storage/NAF2ZXPE/MacKay - 1992 - Information-Based Objective Functions for Active D.pdf;/Users/scannea1/Zotero/storage/GL9H3DEV/Information-Based-Objective-Functions-for-Active.html}
}

@article{mackayPractical1992,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  journal = {Neural Computation},
  volume = {4},
  number = {3},
  pages = {448--472},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.3.448},
  urldate = {2021-10-13},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  file = {/Users/scannea1/Zotero/storage/FME5SXC4/MacKay - 1992 - A Practical Bayesian Framework for Backpropagation.pdf;/Users/scannea1/Zotero/storage/2BPFDD26/A-Practical-Bayesian-Framework-for-Backpropagation.html}
}

@article{mackayProbable1995,
  title = {Probable Networks and Plausible Predictions {\textemdash} a Review of Practical {{Bayesian}} Methods for Supervised Neural Networks},
  author = {Mackay, David J. C.},
  year = {1995},
  month = jan,
  journal = {Network: Computation in Neural Systems},
  volume = {6},
  number = {3},
  pages = {469--505},
  publisher = {{Informa UK Limited}},
  issn = {0954-898X},
  doi = {10.1088/0954-898X/6/3/011},
  urldate = {2021-10-13},
  abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. The article describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
  langid = {english}
}

@inproceedings{maConservativeOfflineDistributional2021,
  title = {Conservative {{Offline Distributional Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ma, Yecheng Jason and Jayaraman, Dinesh and Bastani, Osbert},
  year = {2021},
  month = nov,
  urldate = {2023-11-10},
  abstract = {Many reinforcement learning (RL) problems in practice are offline, learning purely from observational data. A key challenge is how to ensure the learned policy is safe, which requires quantifying the risk associated with different actions. In the online setting, distributional RL algorithms do so by learning the distribution over returns (i.e., cumulative rewards) instead of the expected return; beyond quantifying risk, they have also been shown to learn better representations for planning. We propose Conservative Offline Distributional Actor Critic (CODAC), an offline RL algorithm suitable for both risk-neutral and risk-averse domains. CODAC adapts distributional RL to the offline setting by penalizing the predicted quantiles of the return for out-of-distribution actions. We prove that CODAC learns a conservative return distribution---in particular, for finite MDPs, CODAC converges to an uniform lower bound on the quantiles of the return distribution; our proof relies on a novel analysis of the distributional Bellman operator. In our experiments, on two challenging robot navigation tasks, CODAC successfully learns risk-averse policies using offline data collected purely from risk-neutral agents. Furthermore, CODAC is state-of-the-art on the D4RL MuJoCo benchmark in terms of both expected and risk-sensitive performance.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ma et al-2021 Conservative Offline Distributional Reinforcement Learning/Ma et al_2021_Conservative Offline Distributional Reinforcement Learning.pdf}
}

@inproceedings{maddoxConditioningSparseVariational2021,
  title = {Conditioning {{Sparse Variational Gaussian Processes}} for {{Online Decision-making}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maddox, Wesley J and Stanton, Samuel and Wilson, Andrew G},
  year = {2021},
  volume = {34},
  pages = {6365--6379},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-01-24},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Maddox et al-2021 Conditioning Sparse Variational Gaussian Processes for Online Decision-making/Maddox et al_2021_Conditioning Sparse Variational Gaussian Processes for Online Decision-making.pdf}
}

@misc{maHarmonyWorldModels2023,
  title = {Harmony {{World Models}}: {{Boosting Sample Efficiency}} for {{Model-based Reinforcement Learning}}},
  shorttitle = {Harmony {{World Models}}},
  author = {Ma, Haoyu and Wu, Jialong and Feng, Ningya and Wang, Jianmin and Long, Mingsheng},
  year = {2023},
  month = sep,
  number = {arXiv:2310.00344},
  eprint = {2310.00344},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.00344},
  urldate = {2023-10-07},
  abstract = {Model-based reinforcement learning (MBRL) holds the promise of sample-efficient learning by utilizing a world model, which models how the environment works and typically encompasses components for two tasks: observation modeling and reward modeling. In this paper, through a dedicated empirical investigation, we gain a deeper understanding of the role each task plays in world models and uncover the overlooked potential of more efficient MBRL by harmonizing the interference between observation and reward modeling. Our key insight is that while prevalent approaches of explicit MBRL attempt to restore abundant details of the environment through observation models, it is difficult due to the environment's complexity and limited model capacity. On the other hand, reward models, while dominating in implicit MBRL and adept at learning task-centric dynamics, are inadequate for sample-efficient learning without richer learning signals. Capitalizing on these insights and discoveries, we propose a simple yet effective method, Harmony World Models (HarmonyWM), that introduces a lightweight harmonizer to maintain a dynamic equilibrium between the two tasks in world model learning. Our experiments on three visual control domains show that the base MBRL method equipped with HarmonyWM gains 10\%-55\% absolute performance boosts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ma et al-2023 Harmony World Models/Ma et al_2023_Harmony World Models.pdf;/Users/scannea1/Zotero/storage/NX9K75B6/2310.html}
}

@inproceedings{maLearningPolicyAwareModels2023,
  title = {Learning {{Policy-Aware Models}} for {{Model-Based Reinforcement Learning}} via {{Transition Occupancy Matching}}},
  booktitle = {Proceedings of {{The}} 5th {{Annual Learning}} for {{Dynamics}} and {{Control Conference}}},
  author = {Ma, Yecheng Jason and Sivakumar, Kausik and Yan, Jason and Bastani, Osbert and Jayaraman, Dinesh},
  year = {2023},
  month = jun,
  pages = {259--271},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {Standard model-based reinforcement learning (MBRL) approaches fit a transition model of the environment to all past experience, but this wastes model capacity on data that is irrelevant for policy improvement. We instead propose a new ``transition occupancy matching'' (TOM) objective for MBRL model learning: a model is good to the extent that the current policy experiences the same distribution of transitions inside the model as in the real environment. We derive TOM directly from a novel lower bound on the standard reinforcement learning objective. To optimize TOM, we show how to reduce it to a form of importance weighted maximum-likelihood estimation, where the automatically computed importance weights identify policy-relevant past experiences from a replay buffer, enabling stable optimization. TOM thus offers a plug-and-play model learning sub-routine that is compatible with any backbone MBRL algorithm. On various Mujoco continuous robotic control tasks, we show that TOM successfully focuses model learning on policy-relevant experience and drives policies faster to higher task rewards than alternative model learning approaches. The full paper and code can be found on our project website: https://penn-pal-lab.github.io/TOM/},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ma et al-2023 Learning Policy-Aware Models for Model-Based Reinforcement Learning via/Ma et al_2023_Learning Policy-Aware Models for Model-Based Reinforcement Learning via.pdf}
}

@inproceedings{mannorDynamicAbstractionReinforcement2004,
  title = {Dynamic Abstraction in Reinforcement Learning via Clustering},
  booktitle = {Proceedings of the Twenty-First International Conference on {{Machine}} Learning},
  author = {Mannor, Shie and Menache, Ishai and Hoze, Amit and Klein, Uri},
  year = {2004},
  month = jul,
  series = {{{ICML}} '04},
  pages = {71},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1015330.1015355},
  urldate = {2023-11-02},
  abstract = {We consider a graph theoretic approach for automatic construction of options in a dynamic environment. A map of the environment is generated on-line by the learning agent, representing the topological structure of the state transitions. A clustering algorithm is then used to partition the state space to different regions. Policies for reaching the different parts of the space are separately learned and added to the model in a form of options (macro-actions). The options are used for accelerating the Q-Learning algorithm. We extend the basic algorithm and consider building a map that includes preliminary indication of the location of "interesting" regions of the state space, where the value gradient is significant and additional exploration might be beneficial. Experiments indicate significant speedups, especially in the initial learning phase.},
  isbn = {978-1-58113-838-2},
  keywords = {Clustering,Hierarchical Reinforcement Learning,Options,Q-Learning,Reinforcement Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mannor et al-2004 Dynamic abstraction in reinforcement learning via clustering/Mannor et al_2004_Dynamic abstraction in reinforcement learning via clustering.pdf}
}

@inproceedings{marcoAutomatic2016,
  ids = {marcoAutomatic2016a},
  title = {Automatic {{LQR}} Tuning Based on {{Gaussian}} Process Global Optimization},
  booktitle = {2016 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Marco, Alonso and Hennig, Philipp and Bohg, Jeannette and Schaal, Stefan and Trimpe, Sebastian},
  year = {2016},
  month = may,
  pages = {270--277},
  doi = {10.1109/ICRA.2016.7487144},
  abstract = {This paper proposes an automatic controller tuning framework based on linear optimal control combined with Bayesian optimization. With this framework, an initial set of controller gains is automatically improved according to a pre-defined performance objective evaluated from experimental data. The underlying Bayesian optimization algorithm is Entropy Search, which represents the latent objective as a Gaussian process and constructs an explicit belief over the location of the objective minimum. This is used to maximize the information gain from each experimental evaluation. Thus, this framework shall yield improved controllers with fewer evaluations compared to alternative approaches. A seven-degree-of-freedom robot arm balancing an inverted pole is used as the experimental demonstrator. Results of two- and four-dimensional tuning problems highlight the method's potential for automatic controller tuning on robotic platforms.},
  keywords = {Bayes methods,Computational modeling,Cost function,Gaussian processes,Robots,Tuning},
  file = {/Users/scannea1/Zotero/storage/7H6NWTCX/Marco et al. - 2016 - Automatic LQR tuning based on Gaussian process glo.pdf;/Users/scannea1/Zotero/storage/IS625MU9/Marco et al. - 2016 - Automatic LQR tuning based on Gaussian process glo.pdf;/Users/scannea1/Zotero/storage/2MZEI9XT/7487144.html;/Users/scannea1/Zotero/storage/GLH4RV2S/7487144.html}
}

@inproceedings{martensOptimizing2015,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  year = {2015},
  month = jun,
  pages = {2408--2417},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2022-07-15},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JYW3T4NX/Martens and Grosse - 2015 - Optimizing Neural Networks with Kronecker-factored.pdf}
}

@inproceedings{martensOptimizingNeuralNetworks2015,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  year = {2015},
  month = jun,
  pages = {2408--2417},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-11-13},
  abstract = {We propose an efficient method for approximating natural gradient descent in neural networks which we call Kronecker-factored Approximate Curvature (K-FAC). K-FAC is based on an efficiently invertible approximation of a neural network's Fisher information matrix which is neither diagonal nor low-rank, and in some cases is completely non-sparse. It is derived by approximating various large blocks of the Fisher (corresponding to entire layers) as being the Kronecker product of two much smaller matrices. While only several times more expensive to compute than the plain stochastic gradient, the updates produced by K-FAC make much more progress optimizing the objective, which results in an algorithm that can be much faster than stochastic gradient descent with momentum in practice. And unlike some previously proposed approximate natural-gradient/Newton methods which use high-quality non-diagonal curvature matrices (such as Hessian-free optimization), K-FAC works very well in highly stochastic optimization regimes. This is because the cost of storing and inverting K-FAC's approximation to the curvature matrix does not depend on the amount of data used to estimate it, which is a feature typically associated only with diagonal or low-rank approximations to the curvature matrix.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Martens_Grosse-2015 Optimizing Neural Networks with Kronecker-factored Approximate Curvature/Martens_Grosse_2015_Optimizing Neural Networks with Kronecker-factored Approximate Curvature.pdf}
}

@misc{mathieuAlphaStarUnpluggedLargeScale2023,
  title = {{{AlphaStar Unplugged}}: {{Large-Scale Offline Reinforcement Learning}}},
  shorttitle = {{{AlphaStar Unplugged}}},
  author = {Mathieu, Micha{\"e}l and Ozair, Sherjil and Srinivasan, Srivatsan and Gulcehre, Caglar and Zhang, Shangtong and Jiang, Ray and Paine, Tom Le and Powell, Richard and {\.Z}o{\l}na, Konrad and Schrittwieser, Julian and Choi, David and Georgiev, Petko and Toyama, Daniel and Huang, Aja and Ring, Roman and Babuschkin, Igor and Ewalds, Timo and Bordbar, Mahyar and Henderson, Sarah and Colmenarejo, Sergio G{\'o}mez and van den Oord, A{\"a}ron and Czarnecki, Wojciech Marian and {de Freitas}, Nando and Vinyals, Oriol},
  year = {2023},
  month = aug,
  number = {arXiv:2308.03526},
  eprint = {2308.03526},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.03526},
  urldate = {2023-09-30},
  abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90\% win rate against previously published AlphaStar behavior cloning agent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: 32 pages, 13 figures, previous version published as a NeurIPS 2021 workshop: https://openreview.net/forum?id=Np8Pumfoty},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mathieu et al-2023 AlphaStar Unplugged/Mathieu et al_2023_AlphaStar Unplugged.pdf;/Users/scannea1/Zotero/storage/KBIINPI5/2308.html}
}

@article{matthewsGPflow2017,
  title = {\{\vphantom\}{{GP}}\vphantom\{\}flow: {{A}} \{\vphantom\}{{G}}\vphantom\{\}aussian Process Library Using \{\vphantom\}{{T}}\vphantom\{\}ensor\{\vphantom\}{{F}}\vphantom\{\}low\vphantom\{\}},
  author = {Matthews, Alexander G de G. and {van der Wilk}, Mark and Nickson, Tom and Fujii, Keisuke and Boukouvalas, Alexis and {\{Le\{{\textbackslash}'o\}n-Villagr\{{\textbackslash}'a\}\}}, Pablo and Ghahramani, Zoubin},
  year = {2017},
  month = apr,
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--6}
}

@phdthesis{mcallisterBayesian2017,
  type = {Thesis},
  title = {Bayesian {{Learning}} for {{Data-Efficient Control}}},
  author = {McAllister, Rowan},
  year = {2017},
  month = apr,
  doi = {10.17863/CAM.16688},
  urldate = {2021-09-23},
  abstract = {Applications to learn control of unfamiliar dynamical systems with increasing autonomy are ubiquitous. From robotics, to finance, to industrial processing, autonomous learning helps obviate a heavy reliance on experts for system identification and controller design. Often real world systems are nonlinear, stochastic, and expensive to operate (e.g. slow, energy intensive, prone to wear and tear). Ideally therefore, nonlinear systems can be identified with minimal system interaction. This thesis considers data efficient autonomous learning of control of nonlinear, stochastic systems. Data efficient learning critically requires probabilistic modelling of dynamics. Traditional control approaches use deterministic models, which easily overfit data, especially small datasets. We use probabilistic Bayesian modelling to learn systems from scratch, similar to the PILCO algorithm, which achieved unprecedented data efficiency in learning control of several benchmarks. We extend PILCO in three principle ways. First, we learn control under significant observation noise by simulating a filtered control process using a tractably analytic framework of Gaussian distributions. In addition, we develop the `latent variable belief Markov decision process' when filters must predict under real-time constraints. Second, we improve PILCO's data efficiency by directing exploration with predictive loss uncertainty and Bayesian optimisation, including a novel approximation to the Gittins index. Third, we take a step towards data efficient learning of high-dimensional control using Bayesian neural networks (BNN). Experimentally we show although filtering mitigates adverse effects of observation noise, much greater performance is achieved when optimising controllers with evaluations faithful to reality: by simulating closed-loop filtered control if executing closed-loop filtered control. Thus, controllers are optimised w.r.t. how they are used, outperforming filters applied to systems optimised by unfiltered simulations. We show directed exploration improves data efficiency. Lastly, we show BNN dynamics models are almost as data efficient as Gaussian process models. Results show data efficient learning of high-dimensional control is possible as BNNs scale to high-dimensional state inputs.},
  copyright = {All Rights Reserved},
  langid = {english},
  school = {Department of Engineering, University of Cambridge},
  annotation = {Accepted: 2017-11-28T16:41:59Z},
  file = {/Users/scannea1/Zotero/storage/JPLABBYW/McAllister - 2017 - Bayesian Learning for Data-Efficient Control.pdf;/Users/scannea1/Zotero/storage/DM29JAFZ/269779.html}
}

@inproceedings{mckinnonLearning2017,
  ids = {mckinnonLearning2017b},
  title = {Learning Multimodal Models for Robot Dynamics Online with a Mixture of {{Gaussian}} Process Experts},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {McKinnon, C. D. and Schoellig, A. P.},
  year = {2017},
  month = may,
  pages = {322--328},
  publisher = {{IEEE}},
  doi = {10.1109/ICRA.2017.7989041},
  abstract = {For decades, robots have been essential allies alongside humans in controlled industrial environments like heavy manufacturing facilities. However, without the guidance of a trusted human operator to shepherd a robot safely through a wide range of conditions, they have been barred from the complex, ever changing environments that we live in from day to day. Safe learning control has emerged as a promising way to start bridging algorithms based on first principles to complex real-world scenarios by using data to adapt, and improve performance over time. Safe learning methods rely on a good estimate of the robot dynamics and of the bounds on modelling error in order to be effective. Current methods focus on either a single adaptive model, or a fixed, known set of models for the robot dynamics. This limits them to static or slowly changing environments. This paper presents a method using Gaussian Processes in a Dirichlet Process mixture model to learn an increasing number of non-linear models for the robot dynamics. We show that this approach enables a robot to re-use past experience from an arbitrary number of previously visited operating conditions, and to automatically learn a new model when a new and distinct operating condition is encountered. This approach improves the robustness of existing Gaussian Process-based models to large changes in dynamics that do not have to be specified ahead of time.},
  keywords = {Aerodynamics,controlled industrial environment,Data models,Dirichlet process mixture model,Gaussian processes,Heuristic algorithms,human operator,learning (artificial intelligence),mixture models,mixture of Gaussian process experts,multimodal model learning,robot dynamics,Robots,Safety,System dynamics},
  file = {/Users/scannea1/Zotero/storage/6XLZFBSE/McKinnon and Schoellig - 2017 - Learning multimodal models for robot dynamics onli.pdf;/Users/scannea1/Zotero/storage/9JHYDX6T/7989041.html;/Users/scannea1/Zotero/storage/Y27Y938R/7989041.html}
}

@inproceedings{meloTransformersAreMetaReinforcement2022,
  title = {Transformers Are {{Meta-Reinforcement Learners}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Melo, Luckeciano C.},
  year = {2022},
  month = jun,
  pages = {15340--15359},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-30},
  abstract = {The transformer architecture and variants presented a remarkable success across many machine learning tasks in recent years. This success is intrinsically related to the capability of handling long sequences and the presence of context-dependent weights from the attention mechanism. We argue that these capabilities suit the central role of a Meta-Reinforcement Learning algorithm. Indeed, a meta-RL agent needs to infer the task from a sequence of trajectories. Furthermore, it requires a fast adaptation strategy to adapt its policy for a new task - which can be achieved using the self-attention mechanism. In this work, we present TrMRL (Transformers for Meta-Reinforcement Learning), a meta-RL agent that mimics the memory reinstatement mechanism using the transformer architecture. It associates the recent past of working memories to build an episodic memory recursively through the transformer layers. We show that the self-attention computes a consensus representation that minimizes the Bayes Risk at each layer and provides meaningful features to compute the best actions. We conducted experiments in high-dimensional continuous control environments for locomotion and dexterous manipulation. Results show that TrMRL presents comparable or superior asymptotic performance, sample efficiency, and out-of-distribution generalization compared to the baselines in these environments.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Melo-2022 Transformers are Meta-Reinforcement Learners/Melo_2022_Transformers are Meta-Reinforcement Learners.pdf}
}

@inproceedings{mendoncaDiscoveringAchievingGoals2021,
  title = {Discovering and {{Achieving Goals}} via {{World Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mendonca, Russell and Rybkin, Oleh and Daniilidis, Kostas and Hafner, Danijar and Pathak, Deepak},
  year = {2021},
  volume = {34},
  pages = {24379--24391},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-04},
  abstract = {How can artificial agents learn to solve many diverse tasks in complex visual environments without any supervision? We decompose this question into two challenges: discovering new goals and learning to reliably achieve them. Our proposed agent, Latent Explorer Achiever (LEXA), addresses both challenges by learning a world model from image inputs and using it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal reaching, both on prior benchmarks and on a new challenging benchmark with 40 test tasks spanning across four robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. Project page: https://orybkin.github.io/lexa/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mendonca et al-2021 Discovering and Achieving Goals via World Models/Mendonca et al_2021_Discovering and Achieving Goals via World Models.pdf}
}

@misc{mentzerFiniteScalarQuantization2023,
  title = {Finite {{Scalar Quantization}}: {{VQ-VAE Made Simple}}},
  shorttitle = {Finite {{Scalar Quantization}}},
  author = {Mentzer, Fabian and Minnen, David and Agustsson, Eirikur and Tschannen, Michael},
  year = {2023},
  month = sep,
  number = {arXiv:2309.15505},
  eprint = {2309.15505},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.15505},
  urldate = {2023-10-11},
  abstract = {We propose to replace vector quantization (VQ) in the latent representation of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where we project the VAE representation down to a few dimensions (typically less than 10). Each dimension is quantized to a small set of fixed values, leading to an (implicit) codebook given by the product of these sets. By appropriately choosing the number of dimensions and values each dimension can take, we obtain the same codebook size as in VQ. On top of such discrete representations, we can train the same models that have been trained on VQ-VAE representations. For example, autoregressive and masked transformer models for image generation, multimodal generation, and dense prediction computer vision tasks. Concretely, we employ FSQ with MaskGIT for image generation, and with UViM for depth estimation, colorization, and panoptic segmentation. Despite the much simpler design of FSQ, we obtain competitive performance in all these tasks. We emphasize that FSQ does not suffer from codebook collapse and does not need the complex machinery employed in VQ (commitment losses, codebook reseeding, code splitting, entropy penalties, etc.) to learn expressive discrete representations.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mentzer et al-2023 Finite Scalar Quantization/Mentzer et al_2023_Finite Scalar Quantization.pdf;/Users/scannea1/Zotero/storage/54SILUPQ/2309.html}
}

@inproceedings{mianiLaplacianAutoencodersLearning2022,
  title = {Laplacian {{Autoencoders}} for {{Learning Stochastic Representations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Miani, Marco and Warburg, Frederik and {Moreno-Mu{\~n}oz}, Pablo and Skafte, Nicki and Hauberg, S{\o}ren},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {21059--21072},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Miani et al-2022 Laplacian Autoencoders for Learning Stochastic Representations/Miani et al_2022_Laplacian Autoencoders for Learning Stochastic Representations.pdf}
}

@inproceedings{micheliTransformersAreSampleEfficient2022,
  title = {Transformers Are {{Sample-Efficient World Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c c}ois},
  year = {2022},
  month = sep,
  urldate = {2023-10-02},
  abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Micheli et al-2022 Transformers are Sample-Efficient World Models/Micheli et al_2022_Transformers are Sample-Efficient World Models.pdf}
}

@inproceedings{milamNew2000,
  title = {A New Computational Approach to Real-Time Trajectory Generation for Constrained Mechanical Systems},
  booktitle = {Proceedings of the 39th {{IEEE Conference}} on {{Decision}} and {{Control}}},
  author = {Milam, M. B. and Mushambi, K. and Murray, R. M.},
  year = {2000},
  month = dec,
  volume = {1},
  pages = {845--851},
  publisher = {{IEEE}},
  issn = {0191-2216},
  doi = {10.1109/CDC.2000.912875},
  abstract = {Preliminary results of a new computational approach to generate aggressive trajectories in real-time for constrained mechanical systems are presented. The algorithm is based on a combination of the nonlinear control theory, spline theory, and sequential quadratic programming. It is demonstrated that real-time trajectory generation for constrained mechanical systems is possible by mapping the problem to one of finding trajectory curves in a lower dimensional space. Performance of the algorithm is compared with existing optimal trajectory generation techniques. Numerical results are reported using the nonlinear trajectory generation software package.},
  keywords = {Adaptive control,constrained mechanical systems,control system analysis computing,Control systems,Databases,Mechanical systems,nonlinear control,nonlinear control systems,Nonlinear control systems,optimal control,Optimal control,optimisation,quadratic programming,Real time systems,real-time systems,sequential quadratic programming,spline,splines (mathematics),Stability,Target tracking,tracking,Trajectory,trajectory generation},
  file = {/Users/scannea1/Zotero/storage/NUTQUYQH/Milam et al. - 2000 - A new computational approach to real-time trajecto.pdf;/Users/scannea1/Zotero/storage/C2U83PCJ/912875.html}
}

@inproceedings{mitrovicAdaptive2010,
  title = {Adaptive {{Optimal Feedback Control}} with {{Learned Internal Dynamics Models}}},
  booktitle = {From {{Motor Learning}} to {{Interaction Learning}} in {{Robots}}},
  author = {Mitrovic, Djordje and Klanke, Stefan and Vijayakumar, S.},
  year = {2010},
  doi = {10.1007/978-3-642-05181-4_4},
  abstract = {Optimal Feedback Control (OFC) has been proposed as an attractive movement generation strategy in goal reaching tasks for anthropomorphic manipulator systems. Recent developments, such as the Iterative Linear Quadratic Gaussian (ILQG) algorithm, have focused on the case of non-linear, but still analytically available, dynamics. For realistic control systems, however, the dynamics may often be unknown, difficult to estimate, or subject to frequent systematic changes. In this chapter, we combine the ILQG framework with learning the forward dynamics for simulated arms, which exhibit large redundancies, both, in kinematics and in the actuation. We demonstrate how our approach can compensate for complex dynamic perturbations in an online fashion. The specific adaptive framework introduced lends itself to a computationally more efficient implementation of the ILQG optimisation without sacrificing control accuracy {\textendash} allowing the method to scale to large DoF systems.},
  file = {/Users/scannea1/Zotero/storage/Q9BM2T8Y/Mitrovic et al. - 2010 - Adaptive Optimal Feedback Control with Learned Int.pdf}
}

@misc{mittalORBITUnifiedSimulation2023,
  title = {{{ORBIT}}: {{A Unified Simulation Framework}} for {{Interactive Robot Learning Environments}}},
  shorttitle = {{{ORBIT}}},
  author = {Mittal, Mayank and Yu, Calvin and Yu, Qinxi and Liu, Jingzhou and Rudin, Nikita and Hoeller, David and Yuan, Jia Lin and Tehrani, Pooria Poorsarvi and Singh, Ritvik and Guo, Yunrong and Mazhar, Hammad and Mandlekar, Ajay and Babich, Buck and State, Gavriel and Hutter, Marco and Garg, Animesh},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04195},
  eprint = {2301.04195},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {We present ORBIT, a unified and modular framework for robot learning powered by NVIDIA Isaac Sim. It offers a modular design to easily and efficiently create robotic environments with photo-realistic scenes and fast and accurate rigid and deformable body simulation. With ORBIT, we provide a suite of benchmark tasks of varying difficulty -- from single-stage cabinet opening and cloth folding to multi-stage tasks such as room reorganization. To support working with diverse observations and action spaces, we include fixed-arm and mobile manipulators with different physically-based sensors and motion generators. ORBIT allows training reinforcement learning policies and collecting large demonstration datasets from hand-crafted or expert solutions in a matter of minutes by leveraging GPU-based parallelization. In summary, we offer an open-sourced framework that readily comes with 16 robotic platforms, 4 sensor modalities, 10 motion generators, more than 20 benchmark tasks, and wrappers to 4 learning libraries. With this framework, we aim to support various research areas, including representation learning, reinforcement learning, imitation learning, and task and motion planning. We hope it helps establish interdisciplinary collaborations in these communities, and its modularity makes it easily extensible for more tasks and applications in the future. For videos, documentation, and code: https://isaac-orbit.github.io/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
  note = {Comment: Project website: https://isaac-orbit.github.io/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mittal et al-2023 ORBIT/Mittal et al_2023_ORBIT.pdf;/Users/scannea1/Zotero/storage/WEETEY4N/2301.html}
}

@inproceedings{moerlandEfficient2017,
  title = {Efficient Exploration with {{Double Uncertain Value Networks}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Moerland, Thomas and Broekens, Joost and Jonker, Catholijn},
  year = {2017},
  abstract = {This paper studies directed exploration for reinforcement learning agents by tracking uncertainty about the value of each available action. We identify two sources of uncertainty that are relevant for exploration. The first originates from limited data (parametric uncertainty), while the second originates from the distribution of the returns (return uncertainty). We identify methods to learn these distributions with deep neural networks, where we estimate parametric uncertainty with Bayesian drop-out, while return uncertainty is propagated through the Bellman equation as a Gaussian distribution. Then, we identify that both can be jointly estimated in one network, which we call the Double Uncertain Value Network. The policy is directly derived from the learned distributions based on Thompson sampling. Experimental results show that both types of uncertainty may vastly improve learning in domains with a strong exploration challenge.},
  file = {/Users/scannea1/Zotero/storage/Q94JFI6T/Moerland et al. - 2017 - Efficient exploration with Double Uncertain Value .pdf}
}

@article{moerlandLearning2017,
  title = {Learning {{Multimodal Transition Dynamics}} for {{Model-Based Reinforcement Learning}}},
  author = {Moerland, Thomas M. and Broekens, Joost and Jonker, Catholijn M.},
  year = {2017},
  month = aug,
  journal = {arXiv:1705.00470},
  eprint = {1705.00470},
  abstract = {In this paper we study how to learn stochastic, multimodal transition dynamics in reinforcement learning (RL) tasks. We focus on evaluating transition function estimation, while we defer planning over this model to future work. Stochasticity is a fundamental property of many task environments. However, discriminative function approximators have difficulty estimating multimodal stochasticity. In contrast, deep generative models do capture complex high-dimensional outcome distributions. First we discuss why, amongst such models, conditional variational inference (VI) is theoretically most appealing for model-based RL. Subsequently, we compare different VI models on their ability to learn complex stochasticity on simulated functions, as well as on a typical RL gridworld with multimodal dynamics. Results show VI successfully predicts multimodal outcomes, but also robustly ignores these for deterministic parts of the transition dynamics. In summary, we show a robust method to learn multimodal transitions using function approximation, which is a key preliminary for model-based RL in stochastic domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Scaling Up Reinforcement Learning (SURL) Workshop @ European Machine Learning Conference (ECML)},
  file = {/Users/scannea1/Zotero/storage/8BGSA3IV/Moerland et al. - 2017 - Learning Multimodal Transition Dynamics for Model-.pdf;/Users/scannea1/Zotero/storage/6PAULW4H/1705.html}
}

@misc{moldovanSafeExplorationMarkov2012,
  title = {Safe {{Exploration}} in {{Markov Decision Processes}}},
  author = {Moldovan, Teodor Mihai and Abbeel, Pieter},
  year = {2012},
  month = jul,
  number = {arXiv:1205.4810},
  eprint = {1205.4810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-10-07},
  abstract = {In environments with uncertain dynamics exploration is necessary to learn how to perform well. Existing reinforcement learning algorithms provide strong exploration guarantees, but they tend to rely on an ergodicity assumption. The essence of ergodicity is that any state is eventually reachable from any other state by following a suitable policy. This assumption allows for exploration algorithms that operate by simply favoring states that have rarely been visited before. For most physical systems this assumption is impractical as the systems would break before any reasonable exploration has taken place, i.e., most physical systems don't satisfy the ergodicity assumption. In this paper we address the need for safe exploration methods in Markov decision processes. We first propose a general formulation of safety through ergodicity. We show that imposing safety by restricting attention to the resulting set of guaranteed safe policies is NP-hard. We then present an efficient algorithm for guaranteed safe, but potentially suboptimal, exploration. At the core is an optimization formulation in which the constraints restrict attention to a subset of the guaranteed safe policies and the objective favors exploration policies. Our framework is compatible with the majority of previously proposed exploration methods, which rely on an exploration bonus. Our experiments, which include a Martian terrain exploration problem, show that our method is able to explore better than classical exploration methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Moldovan_Abbeel-2012 Safe Exploration in Markov Decision Processes/Moldovan_Abbeel_2012_Safe Exploration in Markov Decision Processes.pdf;/Users/scannea1/Zotero/storage/BQ5H8I3Y/1205.html}
}

@inproceedings{mollenhoffSAMOptimalRelaxation2022,
  title = {{{SAM}} as an {{Optimal Relaxation}} of {{Bayes}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {M{\"o}llenhoff, Thomas and Khan, Mohammad Emtiyaz},
  year = {2022},
  month = sep,
  urldate = {2023-09-30},
  abstract = {Sharpness-aware minimization (SAM) and related adversarial deep-learning methods can drastically improve generalization, but their underlying mechanisms are not yet fully understood. Here, we establish SAM as a relaxation of the Bayes objective where the expected negative-loss is replaced by the optimal convex lower bound, obtained by using the so-called Fenchel biconjugate. The connection enables a new Adam-like extension of SAM to automatically obtain reasonable uncertainty estimates, while sometimes also improving its accuracy. By connecting adversarial and Bayesian methods, our work opens a new path to robustness.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Mllenhoff_Khan-2022 SAM as an Optimal Relaxation of Bayes/Mollenhoff_Khan_2022_SAM as an Optimal Relaxation of Bayes.pdf}
}

@article{mukadamContinuoustime2018,
  title = {Continuous-Time {{Gaussian}} Process Motion Planning via Probabilistic Inference},
  author = {Mukadam, Mustafa and Dong, Jing and Yan, Xinyan and Dellaert, Frank and Boots, Byron},
  year = {2018},
  month = sep,
  journal = {The International Journal of Robotics Research},
  volume = {37},
  number = {11},
  pages = {1319--1340},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364918790369},
  urldate = {2021-07-29},
  abstract = {We introduce a novel formulation of motion planning, for continuous-time trajectories, as probabilistic inference. We first show how smooth continuous-time trajectories can be represented by a small number of states using sparse Gaussian process (GP) models. We next develop an efficient gradient-based optimization algorithm that exploits this sparsity and GP interpolation. We call this algorithm the Gaussian Process Motion Planner (GPMP). We then detail how motion planning problems can be formulated as probabilistic inference on a factor graph. This forms the basis for GPMP2, a very efficient algorithm that combines GP representations of trajectories with fast, structure-exploiting inference via numerical optimization. Finally, we extend GPMP2 to an incremental algorithm, iGPMP2, that can efficiently replan when conditions change. We benchmark our algorithms against several sampling-based and trajectory optimization-based motion planning algorithms on planning problems in multiple environments. Our evaluation reveals that GPMP2 is several times faster than previous algorithms while retaining robustness. We also benchmark iGPMP2 on replanning problems, and show that it can find successful solutions in a fraction of the time required by GPMP2 to replan from scratch.},
  langid = {english},
  keywords = {factor graphs,Gaussian processes,Motion planning,probabilistic inference,trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/GB3VYF5E/Mukadam et al. - 2018 - Continuous-time Gaussian process motion planning v.pdf}
}

@inproceedings{munosSafeEfficientOffPolicy2016,
  title = {Safe and {{Efficient Off-Policy Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Munos, Remi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-05},
  abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(lambda), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyse the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q* without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(lambda), which was  an open problem since 1989. We illustrate the benefits of Retrace(lambda) on a standard suite of Atari 2600 games.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Munos et al-2016 Safe and Efficient Off-Policy Reinforcement Learning/Munos et al_2016_Safe and Efficient Off-Policy Reinforcement Learning.pdf}
}

@techreport{murrayNote2005,
  title = {A Note on the Evidence and {{Bayesian Occam}}'s Razor},
  author = {Murray, Iain and Ghahramani, Zoubin},
  year = {2005},
  abstract = {Abstract{\textemdash}In his thesis, MacKay (1991) introduced figure 1, explaining how Bayes rule provides an automatic ``Occam's razor '' effect, penalizing unnecessarily complex models. This figure has been adopted by several authors in the same schematic form. Here, after briefly reviewing necessary material, we compute a realization of the plot for a toy data modeling problem. We discuss interesting aspects of this plot and their implications for understanding model complexity. I.},
  file = {/Users/scannea1/Zotero/storage/M5MRVENH/Murray and Ghahramani - 2005 - A note on the evidence and Bayesian Occams razor.pdf;/Users/scannea1/Zotero/storage/RVURUM52/summary.html}
}

@inproceedings{nagabandiDeep2020,
  title = {Deep {{Dynamics Models}} for {{Learning Dexterous Manipulation}}},
  booktitle = {Proceedings of the {{Conference}} on {{Robot Learning}}},
  author = {Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
  year = {2020},
  month = may,
  pages = {1101--1112},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-04-29},
  abstract = {Dexterous multi-fingered hands can provide robots with the ability to flexibly perform a wide range of manipulation skills. However, many of the more complex behaviors are also notoriously difficult to control: Performing in-hand object manipulation, executing finger gaits to move objects, and exhibiting precise fine motor skills such as writing, all require finely balancing contact forces, breaking and reestablishing contacts repeatedly, and maintaining control of unactuated objects. Learning-based techniques provide the appealing possibility of acquiring these skills directly from data, but current learning approaches either require large amounts of data and produce task-specific policies, or they have not yet been shown to scale up to more complex and realistic tasks requiring fine motor skills. In this work, we demonstrate that our method of online planning with deep dynamics models (PDDM) addresses both of these limitations; we show that improvements in learned dynamics models, together with improvements in on-line model-predictive control, can indeed enable efficient and effective learning of flexible contact-rich dexterous manipulation skills {\textendash} and that too, on a 24-DoF anthropomorphic hand in the real world, using just 4 hours of purely real-world data to learn to simultaneously coordinate multiple free-floating objects. Videos can be found at https://sites.google.com/view/pddm/.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/NU3ZJ22M/Nagabandi et al. - 2020 - Deep Dynamics Models for Learning Dexterous Manipu.pdf}
}

@inproceedings{nagumoUber1942,
  title = {{\"U}ber Die {{Lage}} Der {{Integralkurven}} Gew{\"o}hnlicher {{Differentialgleichungen}}},
  booktitle = {Proceedings of the {{Physico-Mathematical Society}} of {{Japan}}. 3rd {{Series}}},
  author = {Nagumo, Mitio},
  year = {1942},
  volume = {24},
  pages = {551--559},
  doi = {10.11429/PPMSJ1919.24.0_551},
  abstract = {Semantic Scholar extracted view of "{\"U}ber die Lage der Integralkurven gew{\"o}hnlicher Differentialgleichungen" by Mitio Nagumo}
}

@inproceedings{naish-guzmanGeneralized2008,
  title = {The {{Generalized FITC Approximation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Naish-guzman}, Andrew and Holden, Sean},
  year = {2008},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-10-28},
  file = {/Users/scannea1/Zotero/storage/F98CE2B4/Naish-guzman and Holden - 2008 - The Generalized FITC Approximation.pdf}
}

@article{nakkaChanceConstrained2021,
  title = {Chance-{{Constrained Trajectory Optimization}} for {{Safe Exploration}} and {{Learning}} of {{Nonlinear Systems}}},
  author = {Nakka, Yashwanth Kumar and Liu, Anqi and Shi, Guanya and Anandkumar, Anima and Yue, Yisong and Chung, Soon-Jo},
  year = {2021},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {2},
  pages = {389--396},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3044033},
  abstract = {Learning-based control algorithms require data collection with abundant supervision for training. Safe exploration algorithms ensure the safety of this data collection process even when only partial knowledge is available. We present a new approach for optimal motion planning with safe exploration that integrates chance-constrained stochastic optimal control with dynamics learning and feedback control. We derive an iterative convex optimization algorithm that solves an Information-cost Stochastic Nonlinear Optimal Control problem (Info-SNOC). The optimization objective encodes control cost for performance and exploration cost for learning, and the safety is incorporated as distributionally robust chance constraints. The dynamics are predicted from a robust regression model that is learned from data. The Info-SNOC algorithm is used to compute a sub-optimal pool of safe motion plans that aid in exploration for learning unknown residual dynamics under safety constraints. A stable feedback controller is used to execute the motion plan and collect data for model learning. We prove the safety of rollout from our exploration method and reduction in uncertainty over epochs, thereby guaranteeing the consistency of our learning method. We validate the effectiveness of Info-SNOC by designing and implementing a pool of safe trajectories for a planar robot. We demonstrate that our approach has higher success rate in ensuring safety when compared to a deterministic trajectory optimization approach.},
  keywords = {Chance constraints,Computational modeling,Data models,Dynamics,machine learning for robot control,model learning for control,motion and path planning,Optimal control,Planning,Safety,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/IDB5STY5/Nakka et al. - 2021 - Chance-Constrained Trajectory Optimization for Saf.pdf}
}

@inproceedings{nazaretVariational2022,
  title = {Variational {{Inference}} for {{Infinitely Deep Neural Networks}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Nazaret, Achille and Blei, David},
  year = {2022},
  month = jun,
  pages = {16447--16461},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {We introduce the unbounded depth neural network (UDN), an infinitely deep probabilistic model that adapts its complexity to the training data. The UDN contains an infinite sequence of hidden layers and places an unbounded prior on a truncation L, the layer from which it produces its data. Given a dataset of observations, the posterior UDN provides a conditional distribution of both the parameters of the infinite neural network and its truncation. We develop a novel variational inference algorithm to approximate this posterior, optimizing a distribution of the neural network weights and of the truncation depth L, and without any upper limit on L. To this end, the variational family has a special structure: it models neural network weights of arbitrary depth, and it dynamically creates or removes free variational parameters as its distribution of the truncation is optimized. (Unlike heuristic approaches to model search, it is solely through gradient-based optimization that this algorithm explores the space of truncations.) We study the UDN on real and synthetic data. We find that the UDN adapts its posterior depth to the dataset complexity; it outperforms standard neural networks of similar computational complexity; and it outperforms other approaches to infinite-depth neural networks.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GK2E2BWK/Nazaret and Blei - 2022 - Variational Inference for Infinitely Deep Neural N.pdf}
}

@misc{NewFCAITeams,
  title = {({{New}}) {{FCAI Teams Overview- Weekly Template}}},
  urldate = {2023-06-07},
  file = {/Users/scannea1/Zotero/storage/ZYFLR74L/(New) FCAI Teams Overview- Weekly Template.pdf}
}

@article{nguyen-tuongModel2009,
  title = {Model Learning with Local Gaussian Process Regression},
  author = {{Nguyen-Tuong}, Duy and Seeger, Matthias and Peters, Jan},
  year = {2009},
  journal = {Advanced Robotics},
  volume = {23},
  number = {15},
  eprint = {https://doi.org/10.1163/016918609X12529286896877},
  pages = {2015--2034},
  publisher = {{Taylor \& Francis}},
  doi = {10.1163/016918609X12529286896877}
}

@inproceedings{nguyenFast2014,
  title = {Fast {{Allocation}} of {{Gaussian Process Experts}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Nguyen, Trung and Bonilla, Edwin},
  year = {2014},
  month = jan,
  pages = {145--153},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-10-26},
  abstract = {We propose a scalable nonparametric Bayesian regression model based on a mixture of Gaussian process (GP) experts  and the inducing points formalism underpinning sparse GP approximations. Each expert is augmented with a set of inducing points, and the allocation of data points to experts is defined probabilistically based on their proximity to the experts. This allocation mechanism enables a fast variational inference procedure for learning of the inducing inputs and hyperparameters of the experts. When using K experts, our method can  run K\^2 times faster and use K\^2 times less memory than popular sparse methods such as the FITC approximation. Furthermore, it is easy to parallelize and handles non-stationarity  straightforwardly. Our experiments show that on medium-sized datasets (of around 10\^4 training points) it  trains up to 5 times faster than FITC while achieving comparable accuracy. On a large dataset  of 10\^5 training points, our method significantly outperforms six  competitive baselines while requiring only a few hours of training.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3M2SHR5F/Nguyen and Bonilla - 2014 - Fast Allocation of Gaussian Process Experts.pdf}
}

@article{nguyenModel2020,
  title = {Model {{Predictive Control}} for {{Micro Aerial Vehicles}}: {{A Survey}}},
  shorttitle = {Model {{Predictive Control}} for {{Micro Aerial Vehicles}}},
  author = {Nguyen, Huan and Kamel, Mina and Alexis, Kostas and Siegwart, Roland},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.11104 [cs]},
  eprint = {2011.11104},
  primaryclass = {cs},
  urldate = {2021-03-23},
  abstract = {This paper presents a review of the design and application of model predictive control strategies for Micro Aerial Vehicles and specifically multirotor configurations such as quadrotors. The diverse set of works in the domain is organized based on the control law being optimized over linear or nonlinear dynamics, the integration of state and input constraints, possible fault-tolerant design, if reinforcement learning methods have been utilized and if the controller refers to free-flight or other tasks such as physical interaction or load transportation. A selected set of comparison results are also presented and serve to provide insight for the selection between linear and nonlinear schemes, the tuning of the prediction horizon, the importance of disturbance observer-based offset-free tracking and the intrinsic robustness of such methods to parameter uncertainty. Furthermore, an overview of recent research trends on the combined application of modern deep reinforcement learning techniques and model predictive control for multirotor vehicles is presented. Finally, this review concludes with explicit discussion regarding selected open-source software packages that deliver off-the-shelf model predictive control functionality applicable to a wide variety of Micro Aerial Vehicle configurations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/Users/scannea1/Zotero/storage/SASSEGG9/Nguyen et al. - 2020 - Model Predictive Control for Micro Aerial Vehicles.pdf;/Users/scannea1/Zotero/storage/P8F5EY7X/2011.html}
}

@article{nguyenStochastic2018,
  title = {Stochastic Variational Hierarchical Mixture of Sparse {{Gaussian}} Processes for Regression},
  author = {Nguyen, Thi Nhat Anh and Bouzerdoum, Abdesselam and Phung, Son Lam},
  year = {2018},
  month = dec,
  journal = {Machine Learning},
  volume = {107},
  number = {12},
  pages = {1947--1986},
  issn = {1573-0565},
  doi = {10.1007/s10994-018-5721-5},
  urldate = {2021-02-18},
  abstract = {In this article, we propose a scalable Gaussian process (GP) regression method that combines the advantages of both global and local GP approximations through a two-layer hierarchical model using a variational inference framework. The upper layer consists of a global sparse GP to coarsely model the entire data set, whereas the lower layer comprises a mixture of sparse GP experts which exploit local information to learn a fine-grained model. A two-step variational inference algorithm is developed to learn the global GP, the GP experts and the gating network simultaneously. Stochastic optimization can be employed to allow the application of the model to large-scale problems. Experiments on a wide range of benchmark data sets demonstrate the flexibility, scalability and predictive power of the proposed method.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/LA4BFTY4/Nguyen et al. - 2018 - Stochastic variational hierarchical mixture of spa.pdf}
}

@article{ni2024bridging,
	title={Bridging State and History Representations: Understanding Self-Predictive RL},
	author={Ni, Tianwei and Eysenbach, Benjamin and Seyedsalehi, Erfan and Ma, Michel and Gehring, Clement and Mahajan, Aditya and Bacon, Pierre-Luc},
	journal={arXiv preprint arXiv:2401.08898},
	year={2024}
}

@inproceedings{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  month = jul,
  pages = {8162--8171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-19},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/E8KR8FHU/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/scannea1/Zotero/storage/PXTVTZSY/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@article{nickischApproximations2008,
  title = {Approximations for {{Binary Gaussian Process Classification}}},
  author = {Nickisch, Hannes and Rasmussen, Carl Edward},
  year = {2008},
  journal = {Journal of Machine Learning Research},
  volume = {9},
  pages = {2035--2078},
  abstract = {We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/R7ZVMTFH/Nickisch and Rasmussen - Approximations for Binary Gaussian Process Classi.pdf}
}

@inproceedings{nikishinControlOrientedModelBasedReinforcement2022,
  title = {Control-{{Oriented Model-Based Reinforcement Learning}} with {{Implicit Differentiation}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Nikishin, Evgenii and Abachi, Romina and Agarwal, Rishabh and Bacon, Pierre-Luc},
  year = {2022},
  month = jun,
  volume = {36},
  pages = {7886--7894},
  doi = {10.1609/aaai.v36i7.20758},
  urldate = {2023-09-30},
  abstract = {The shortcomings of maximum likelihood estimation in the context of model-based reinforcement learning have been highlighted by an increasing number of papers. When the model class is misspecified or has a limited representational capacity, model parameters with high likelihood might not necessarily result in high performance of the agent on a downstream control task. To alleviate this problem, we propose an end-to-end approach for model learning which directly optimizes the expected returns using implicit differentiation. We treat a value function that satisfies the Bellman optimality operator induced by the model as an implicit function of model parameters and show how to differentiate the function. We provide theoretical and empirical evidence highlighting the benefits of our approach in the model misspecification regime compared to likelihood-based methods.},
  copyright = {Copyright (c) 2022 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Search And Optimization (SO)},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Nikishin et al-2022 Control-Oriented Model-Based Reinforcement Learning with Implicit/Nikishin et al_2022_Control-Oriented Model-Based Reinforcement Learning with Implicit.pdf}
}

@inproceedings{nikishinDeepReinforcementLearning2023,
  title = {Deep {{Reinforcement Learning}} with {{Plasticity Injection}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andr{\'e}},
  year = {2023},
  month = oct,
  eprint = {2305.15555},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.15555},
  urldate = {2023-11-02},
  abstract = {A growing body of evidence suggests that neural networks employed in deep reinforcement learning (RL) gradually lose their plasticity, the ability to learn from new data; however, the analysis and mitigation of this phenomenon is hampered by the complex relationship between plasticity, exploration, and performance in RL. This paper introduces plasticity injection, a minimalistic intervention that increases the network plasticity without changing the number of trainable parameters or biasing the predictions. The applications of this intervention are two-fold: first, as a diagnostic tool \${\textbackslash}unicode\{x2014\}\$ if injection increases the performance, we may conclude that an agent's network was losing its plasticity. This tool allows us to identify a subset of Atari environments where the lack of plasticity causes performance plateaus, motivating future studies on understanding and combating plasticity loss. Second, plasticity injection can be used to improve the computational efficiency of RL training if the agent has to re-learn from scratch due to exhausted plasticity or by growing the agent's network dynamically without compromising performance. The results on Atari show that plasticity injection attains stronger performance compared to alternative methods while being computationally efficient.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: NeurIPS 2023 camera-ready},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Nikishin et al-2023 Deep Reinforcement Learning with Plasticity Injection/Nikishin et al_2023_Deep Reinforcement Learning with Plasticity Injection.pdf;/Users/scannea1/Zotero/storage/GEYGJD6Y/2305.html}
}

@inproceedings{nikishinPrimacyBiasDeep2022,
  title = {The {{Primacy Bias}} in {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  year = {2022},
  month = jun,
  pages = {16828--16847},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Nikishin et al-2022 The Primacy Bias in Deep Reinforcement Learning/Nikishin et al_2022_The Primacy Bias in Deep Reinforcement Learning.pdf}
}

@inproceedings{NIPS2005_f499d34b,
  ids = {meedsAlternative2006},
  title = {An Alternative Infinite Mixture of Gaussian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Meeds, Edward and Osindero, Simon},
  editor = {Weiss, Y. and Sch{\"o}lkopf, B. and Platt, J.},
  year = {2006},
  volume = {18},
  publisher = {{MIT Press}},
  file = {/Users/scannea1/Zotero/storage/BQWNWLFY/Meeds and Osindero - 2006 - An Alternative Infinite Mixture Of Gaussian Proces.pdf}
}

@inproceedings{NIPS2008_f4b9ec30,
  ids = {yuanVariational2009},
  title = {Variational Mixture of Gaussian Process Experts},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yuan, Chao and Neubauer, Claus},
  editor = {Koller, D. and Schuurmans, D. and Bengio, Y. and Bottou, L.},
  year = {2009},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/scannea1/Zotero/storage/TVYKP872/Yuan and Neubauer - 2009 - Variational Mixture of Gaussian Process Experts.pdf}
}

@inproceedings{NIPS2011_7eb3c8be,
  title = {Practical Variational Inference for Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Graves, Alex},
  editor = {{Shawe-Taylor}, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K.Q.},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}}
}

@misc{nottinghamEmbodiedAgentsDream2023,
  title = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}: {{Embodied Decision Making}} Using {{Language Guided World Modelling}}},
  shorttitle = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}},
  author = {Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy},
  year = {2023},
  month = apr,
  number = {arXiv:2301.12050},
  eprint = {2301.12050},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.},
  archiveprefix = {arxiv},
  keywords = {\_tablet,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: in proceedings of ICML 23},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Nottingham et al-2023 Do Embodied Agents Dream of Pixelated Sheep/Nottingham et al_2023_Do Embodied Agents Dream of Pixelated Sheep.pdf;/Users/scannea1/Zotero/storage/68HR2IXX/2301.html}
}

@inproceedings{obando-ceronSmallBatchDeep2023a,
  title = {Small Batch Deep Reinforcement Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Obando-Ceron}, Johan and Bellemare, Marc G. and Castro, Pablo Samuel},
  year = {2023},
  month = oct,
  eprint = {2310.03882},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.03882},
  urldate = {2023-11-02},
  abstract = {In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests \{{\textbackslash}em reducing\} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  note = {Comment: Published at NeurIPS 2023},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Obando-Ceron et al-2023 Small batch deep reinforcement learning/Obando-Ceron et al_2023_Small batch deep reinforcement learning2.pdf;/Users/scannea1/Zotero/storage/F3G9UDXG/2310.html}
}

@inproceedings{okadaVariational2020,
  title = {Variational {{Inference MPC}} for {{Bayesian Model-based Reinforcement Learning}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Okada, Masashi and Taniguchi, Tadahiro},
  year = {2020},
  month = may,
  pages = {258--272},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-30},
  abstract = {In recent studies on model-based reinforcement learning (MBRL), incorporating uncertainty in forward dynamics is a state-of-the-art strategy to enhance learning performance, making MBRLs competitiv...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4R3LXT8H/Okada and Taniguchi - 2020 - Variational Inference MPC for Bayesian Model-based.pdf;/Users/scannea1/Zotero/storage/XWZNGU6G/okada20a.html}
}

@inproceedings{osawaPractical2019,
  title = {Practical {{Deep Learning}} with {{Bayesian Principles}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osawa, Kazuki and Swaroop, Siddharth and Khan, Mohammad Emtiyaz E and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E and Yokota, Rio},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-14},
  abstract = {Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on out-of-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation is available as a plug-and-play optimiser.},
  file = {/Users/scannea1/Zotero/storage/8CF9CNB4/Osawa et al. - 2019 - Practical Deep Learning with Bayesian Principles.pdf}
}

@inproceedings{osbandMoreEfficientReinforcement2013,
  title = {({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-29},
  keywords = {reading-list},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Osband et al-2013 (More) Efficient Reinforcement Learning via Posterior Sampling/Osband et al_2013_(More) Efficient Reinforcement Learning via Posterior Sampling.pdf}
}

@inproceedings{osbandWhyPosteriorSampling2017,
  title = {Why Is {{Posterior Sampling Better}} than {{Optimism}} for {{Reinforcement Learning}}?},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Osband, Ian and Roy, Benjamin Van},
  year = {2017},
  month = jul,
  pages = {2701--2710},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-11-29},
  abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an {$O$}\~~({$HSAT$}-----)O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H{\textbackslash}sqrt\{SAT\}) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of {$O$}\~~({$HSAT$}---)O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H S {\textbackslash}sqrt\{AT\}) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.},
  langid = {english},
  keywords = {reading-list},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Osband_Roy-2017 Why is Posterior Sampling Better than Optimism for Reinforcement Learning/Osband_Roy_2017_Why is Posterior Sampling Better than Optimism for Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/4A5HQH39/Osband and Roy - 2017 - Why is Posterior Sampling Better than Optimism for.pdf}
}

@inproceedings{ozairVectorQuantizedModels2021,
  title = {Vector {{Quantized Models}} for {{Planning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Ozair, Sherjil and Li, Yazhe and Razavi, Ali and Antonoglou, Ioannis and Oord, Aaron Van Den and Vinyals, Oriol},
  year = {2021},
  month = jul,
  pages = {8302--8313},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-02},
  abstract = {Recent developments in the field of model-based RL have proven successful in a range of environments, especially ones where planning is essential. However, such successes have been limited to deterministic fully-observed environments. We present a new approach that handles stochastic and partially-observable environments. Our key insight is to use discrete autoencoders to capture the multiple possible effects of an action in a stochastic environment. We use a stochastic variant of Monte Carlo tree search to plan over both the agent's actions and the discrete latent variables representing the environment's response. Our approach significantly outperforms an offline version of MuZero on a stochastic interpretation of chess where the opponent is considered part of the environment. We also show that our approach scales to DeepMind Lab, a first-person 3D environment with large visual observations and partial observability.},
  langid = {english},
  keywords = {\_tablet\_modified},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Ozair et al-2021 Vector Quantized Models for Planning/Ozair et al_2021_Vector Quantized Models for Planning.pdf;/Users/scannea1/Zotero/storage/H4NFTBSE/Ozair et al_2021_Vector Quantized Models for Planning.pdf}
}

@article{padalkarOpenXEmbodimentRobotic,
  title = {Open {{X-Embodiment}}: {{Robotic Learning Datasets}} and {{RT-X Models}}},
  author = {Padalkar, Abhishek and Pooley, Acorn and Jain, Ajinkya and Bewley, Alex and Herzog, Alex and Irpan, Alex and Khazatsky, Alexander and Rai, Anant and Singh, Anikait and Brohan, Anthony and Raffin, Antonin and Wahid, Ayzaan and {Burgess-Limerick}, Ben and Kim, Beomjoon and Scholkopf, Bernhard and Ichter, Brian and Lu, Cewu and Xu, Charles and Finn, Chelsea and Xu, Chenfeng and Chi, Cheng and Huang, Chenguang and Chan, Christine and Pan, Chuer and Fu, Chuyuan and Devin, Coline and Driess, Danny and Pathak, Deepak and Shah, Dhruv and Buchler, Dieter and Kalashnikov, Dmitry and Sadigh, Dorsa and Johns, Edward and Ceola, Federico and Xia, Fei and Stulp, Freek and Zhou, Gaoyue and Sukhatme, Gaurav S and Salhotra, Gautam and Yan, Ge and Schiavi, Giulio and Su, Hao and Fang, Hao-Shu and Shi, Haochen and Amor, Heni Ben and Christensen, Henrik I and Furuta, Hiroki and Walke, Homer and Fang, Hongjie and Mordatch, Igor and Radosavovic, Ilija and Leal, Isabel and Liang, Jacky and Kim, Jaehyung and Schneider, Jan and Hsu, Jasmine and Bohg, Jeannette and Bingham, Jeffrey and Wu, Jiajun and Wu, Jialin and Luo, Jianlan and Gu, Jiayuan and Tan, Jie and Oh, Jihoon and Malik, Jitendra and Tompson, Jonathan and Yang, Jonathan and Lim, Joseph J and Silverio, Joao and Han, Junhyek and Rao, Kanishka and Pertsch, Karl and Hausman, Karol and Go, Keegan and Gopalakrishnan, Keerthana and Goldberg, Ken and Byrne, Kendra and Oslund, Kenneth and Kawaharazuka, Kento and Zhang, Kevin and Majd, Keyvan and Rana, Krishan and Srinivasan, Krishnan and Chen, Lawrence Yunliang and Pinto, Lerrel and Tan, Liam and Ott, Lionel and Lee, Lisa and Tomizuka, Masayoshi and Du, Maximilian and Ahn, Michael and Zhang, Mingtong and Ding, Mingyu and Srirama, Mohan Kumar and Sharma, Mohit and Kim, Moo Jin and Kanazawa, Naoaki and Hansen, Nicklas and Heess, Nicolas and Joshi, Nikhil J and Suenderhauf, Niko and Wohlhart, Paul and Xu, Peng and Sermanet, Pierre and Sundaresan, Priya and Vuong, Quan and Rafailov, Rafael and Tian, Ran and Doshi, Ria and {Mart{\i}n-Mart{\i}n}, Roberto and Mendonca, Russell and Shah, Rutav and Hoque, Ryan and Julian, Ryan and Bustamante, Samuel and Kirmani, Sean and Levine, Sergey and Moore, Sherry and Bahl, Shikhar and Dass, Shivin and Song, Shuran and Xu, Sichun and Haldar, Siddhant and Adebola, Simeon and Guist, Simon and Nasiriany, Soroush and Schaal, Stefan and Welker, Stefan and Tian, Stephen and Dasari, Sudeep and Belkhale, Suneel and Osa, Takayuki and Harada, Tatsuya and Matsushima, Tatsuya and Xiao, Ted and Yu, Tianhe and Ding, Tianli and Davchev, Todor and Zhao, Tony Z and Armstrong, Travis and Darrell, Trevor and Jain, Vidhi and Vanhoucke, Vincent and Zhan, Wei and Zhou, Wenxuan and Burgard, Wolfram and Chen, Xi and Wang, Xiaolong and Zhu, Xinghao and Li, Xuanlin and Lu, Yao and Chebotar, Yevgen and Zhou, Yifan and Zhu, Yifeng and Xu, Ying and Wang, Yixuan and Bisk, Yonatan and Cho, Yoonyoung and Lee, Youngwoon and Cui, Yuchen and Wu, Yueh-hua and Tang, Yujin and Zhu, Yuke and Li, Yunzhu and Iwasawa, Yusuke and Matsuo, Yutaka and Xu, Zhuo and Cui, Zichen Jeff},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/T8CLGH5K/Padalkar et al. - Open X-Embodiment Robotic Learning Datasets and R.pdf}
}

@inproceedings{palenicekDiminishingReturnValue2022,
  title = {Diminishing {{Return}} of {{Value Expansion Methods}} in {{Model-Based Reinforcement Learning}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Palenicek, Daniel and Lutter, Michael and Carvalho, Joao and Peters, Jan},
  year = {2022},
  month = sep,
  urldate = {2023-11-05},
  abstract = {Model-based reinforcement learning is one approach to increase sample efficiency. However, the accuracy of the dynamics model and the resulting compounding error over modelled trajectories are commonly regarded as key limitations. A natural question to ask is: How much more sample efficiency can be gained by improving the learned dynamics models? Our paper empirically answers this question for the class of model-based value expansion methods in continuous control problems. Value expansion methods should benefit from increased model accuracy by enabling longer rollout horizons and better value function approximations. Our empirical study, which leverages oracle dynamics models to avoid compounding model errors, shows that (1) longer horizons increase sample efficiency, but the gain in improvement decreases with each additional expansion step, and (2) the increased model accuracy only marginally increases the sample efficiency compared to learned models with identical horizons. Therefore, longer horizons and increased model accuracy yield diminishing returns in terms of sample efficiency. These improvements in sample efficiency are particularly disappointing when compared to model-free value expansion methods. Even though they introduce no computational overhead, we find their performance to be on-par with model-based value expansion methods. Therefore, we conclude that the limitation of model-based value expansion methods is not the model accuracy of the learned models. While higher model accuracy is beneficial, our experiments show that even a perfect model will not provide an un-rivaled sample efficiency but that the bottleneck lies elsewhere.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Palenicek et al-2022 Diminishing Return of Value Expansion Methods in Model-Based Reinforcement/Palenicek et al_2022_Diminishing Return of Value Expansion Methods in Model-Based Reinforcement.pdf}
}

@misc{palenicekRevisitingModelbasedValue2022,
  title = {Revisiting {{Model-based Value Expansion}}},
  author = {Palenicek, Daniel and Lutter, Michael and Peters, Jan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14660},
  eprint = {2203.14660},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.14660},
  urldate = {2023-11-05},
  abstract = {Model-based value expansion methods promise to improve the quality of value function targets and, thereby, the effectiveness of value function learning. However, to date, these methods are being outperformed by Dyna-style algorithms with conceptually simpler 1-step value function targets. This shows that in practice, the theoretical justification of value expansion does not seem to hold. We provide a thorough empirical study to shed light on the causes of failure of value expansion methods in practice which is believed to be the compounding model error. By leveraging GPU based physics simulators, we are able to efficiently use the true dynamics for analysis inside the model-based reinforcement learning loop. Performing extensive comparisons between true and learned dynamics sheds light into this black box. This paper provides a better understanding of the actual problems in value expansion. We provide future directions of research by empirically testing the maximum theoretical performance of current approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Palenicek et al-2022 Revisiting Model-based Value Expansion/Palenicek et al_2022_Revisiting Model-based Value Expansion.pdf;/Users/scannea1/Zotero/storage/7SHALJBF/2203.html}
}

@article{panEfficient2018,
  title = {Efficient {{Reinforcement Learning}} via {{Probabilistic Trajectory Optimization}}},
  author = {Pan, Yunpeng and Boutselis, George I. and Theodorou, Evangelos A.},
  year = {2018},
  month = nov,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {29},
  number = {11},
  pages = {5459--5474},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2017.2764499},
  abstract = {We present a trajectory optimization approach to reinforcement learning in continuous state and action spaces, called probabilistic differential dynamic programming (PDDP). Our method represents systems dynamics using Gaussian processes (GPs), and performs local dynamic programming iteratively around a nominal trajectory in Gaussian belief spaces. Different from model-based policy search methods, PDDP does not require a policy parameterization and learns a time-varying control policy via successive forward-backward sweeps. A convergence analysis of the iterative scheme is given, showing that our algorithm converges to a stationary point globally under certain conditions. We show that prior model knowledge can be incorporated into the proposed framework to speed up learning, and a generalized optimization criterion based on the predicted cost distribution can be employed to enable risk-sensitive learning. We demonstrate the effectiveness and efficiency of the proposed algorithm using nontrivial tasks. Compared with a state-of-the-art GP-based policy search method, PDDP offers a superior combination of learning speed, data efficiency, and applicability.},
  keywords = {Aerospace electronics,Data models,Dynamic programming,Gaussian processes (GPs),Heuristic algorithms,optimal control,Optimal control,Predictive models,Probabilistic logic,reinforcement learning (RL),trajectory optimization,Trajectory optimization},
  file = {/Users/scannea1/Zotero/storage/UIRUIIA8/Pan et al. - 2018 - Efficient Reinforcement Learning via Probabilistic.pdf;/Users/scannea1/Zotero/storage/P62DM6XT/8306829.html}
}

@article{panovWorldModelActor2023,
  title = {A {{World Model}} for {{Actor}}{\textendash}{{Critic}} in {{Reinforcement Learning}}},
  author = {Panov, A. I. and Ugadiarov, L. A.},
  year = {2023},
  month = sep,
  journal = {Pattern Recognition and Image Analysis},
  volume = {33},
  number = {3},
  pages = {467--477},
  issn = {1555-6212},
  doi = {10.1134/S1054661823030379},
  urldate = {2023-10-02},
  abstract = {Model-based reinforcement learning is a hybrid approach that combines planning with a world model and model-free policy learning, a major advantage of which is the high sample efficiency. The world model approaches apply it either for additional policy training on model-generated trajectories or for obtaining a more accurate approximation of the state-value function that is used to estimate the action values along multistep model trajectories. This paper proposes a new approach to integrating the world model as a critic into architectures of the actor{\textendash}critic family to estimate the action values. Experiments with hybrid algorithms using a world model with look-ahead tree search as a critic on environments with a complex set of subgoals have shown that the proposed integration can accelerate policy learning under certain conditions.},
  langid = {english},
  keywords = {actor{\textendash}critic architecture,behavior planning,look-ahead tree search,reinforcement learning,world model},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Panov_Ugadiarov-2023 A World Model for ActorCritic in Reinforcement Learning/Panov_Ugadiarov_2023_A World Model for ActorCritic in Reinforcement Learning.pdf}
}

@inproceedings{panProbabilistic2014,
  title = {Probabilistic {{Differential Dynamic Programming}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pan, Yunpeng and Theodorou, Evangelos},
  year = {2014},
  volume = {27},
  pages = {1907--1915},
  abstract = {We present a data-driven, probabilistic trajectory optimization framework for systems with unknown dynamics, called Probabilistic Differential Dynamic Programming (PDDP). PDDP takes into account uncertainty explicitly for dynamics models using Gaussian processes (GPs). Based on the second-order local approximation of the value function, PDDP performs Dynamic Programming around a nominal trajectory in Gaussian belief spaces. Different from typical gradientbased policy search methods, PDDP does not require a policy parameterization and learns a locally optimal, time-varying control policy. We demonstrate the effectiveness and efficiency of the proposed algorithm using two nontrivial tasks. Compared with the classical DDP and a state-of-the-art GP-based policy search method, PDDP offers a superior combination of data-efficiency, learning speed, and applicability.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/D4XA2UIZ/Pan and Theodorou - Probabilistic Differential Dynamic Programming.pdf}
}

@inproceedings{panSample2015,
  title = {Sample {{Efficient Path Integral Control}} under {{Uncertainty}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pan, Yunpeng and Theodorou, Evangelos and Kontitsis, Michail},
  year = {2015},
  volume = {28},
  urldate = {2021-06-24},
  file = {/Users/scannea1/Zotero/storage/J6YZYW7T/Fleisher - 1988 - The Hopfield Model with Multi-Level Neurons.pdf;/Users/scannea1/Zotero/storage/RDCXT48K/81ca0262c82e712e50c580c032d99b60-Paper.html}
}

@article{papagiannisBoostingDeepReinforcement2024,
  title = {Boosting {{Deep Reinforcement Learning Agents}} with {{Generative Data Augmentation}}},
  author = {Papagiannis, Tasos and Alexandridis, Georgios and Stafylopatis, Andreas},
  year = {2024},
  month = jan,
  journal = {Applied Sciences},
  volume = {14},
  number = {1},
  pages = {330},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app14010330},
  urldate = {2024-01-15},
  abstract = {Data augmentation is a promising technique in improving exploration and convergence speed in deep reinforcement learning methodologies. In this work, we propose a data augmentation framework based on generative models for creating completely novel states and increasing diversity. For this purpose, a diffusion model is used to generate artificial states (learning the distribution of original, collected states), while an additional model is trained to predict the action executed between two consecutive states. These models are combined to create synthetic data for cases of high and low immediate rewards, which are encountered less frequently during the agent's interaction with the environment. During the training process, the synthetic samples are mixed with the actually observed data in order to speed up agent learning. The proposed methodology is tested on the Atari 2600 framework, producing realistic and diverse synthetic data which improve training in most cases. Specifically, the agent is evaluated on three heterogeneous games, achieving a reward increase of up to 31\%, although the results indicate performance variance among the different environments. The augmentation models are independent of the learning process and can be integrated to different algorithms, as well as different environments, with slight adaptations.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Arcade Learning Environment,data augmentation,deep reinforcement learning,diffusion models,generative models},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Papagiannis et al-2024 Boosting Deep Reinforcement Learning Agents with Generative Data Augmentation/Papagiannis et al_2024_Boosting Deep Reinforcement Learning Agents with Generative Data Augmentation.pdf}
}

@inproceedings{parmasPIPPS2018,
  title = {{{PIPPS}}: {{Flexible Model-Based Policy Search Robust}} to the {{Curse}} of {{Chaos}}},
  shorttitle = {{{PIPPS}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Parmas, Paavo and Rasmussen, Carl Edward and Peters, Jan and Doya, Kenji},
  year = {2018},
  month = jul,
  pages = {4065--4074},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-02-02},
  abstract = {Previously, the exploding gradient problem has been explained to be central in deep learning and model-based reinforcement learning, because it causes numerical issues and instability in optimizati...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/XGUGY4P8/Parmas et al. - 2018 - PIPPS Flexible Model-Based Policy Search Robust t.pdf;/Users/scannea1/Zotero/storage/QXP623BU/parmas18a.html}
}

@inproceedings{pathakCuriositydrivenExplorationSelfsupervised2017,
  title = {Curiosity-Driven {{Exploration}} by {{Self-supervised Prediction}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {2778--2787},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-20},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Pathak et al-2017 Curiosity-driven Exploration by Self-supervised Prediction/Pathak et al_2017_Curiosity-driven Exploration by Self-supervised Prediction.pdf}
}

@misc{perezFiLMVisualReasoning2017,
  title = {{{FiLM}}: {{Visual Reasoning}} with a {{General Conditioning Layer}}},
  shorttitle = {{{FiLM}}},
  author = {Perez, Ethan and Strub, Florian and {de Vries}, Harm and Dumoulin, Vincent and Courville, Aaron},
  year = {2017},
  month = dec,
  number = {arXiv:1709.07871},
  eprint = {1709.07871},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1709.07871},
  urldate = {2023-10-25},
  abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  note = {Comment: AAAI 2018. Code available at http://github.com/ethanjperez/film . Extends arXiv:1707.03017},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Perez et al-2017 FiLM/Perez et al_2017_FiLM.pdf;/Users/scannea1/Zotero/storage/VHB9Y6VZ/1709.html}
}

@inproceedings{pinneriSampleefficientCrossEntropyMethod2021,
  title = {Sample-Efficient {{Cross-Entropy Method}} for {{Real-time Planning}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Robot Learning}}},
  author = {Pinneri, Cristina and Sawant, Shambhuraj and Blaes, Sebastian and Achterhold, Jan and Stueckler, Joerg and Rolinek, Michal and Martius, Georg},
  year = {2021},
  month = oct,
  pages = {1049--1065},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {Trajectory optimizers for model-based reinforcement learning, such as the  Cross-Entropy Method (CEM), can yield compelling results even in high-dimensional control tasks and sparse-reward environments. However, their sampling inefficiency prevents them from being used for real-time planning and control. We propose an improved version of the CEM algorithm for fast planning, with novel additions including temporally-correlated actions and memory, requiring 2.7-22x less samples and yielding a performance increase of 1.2-10x in high-dimensional control problems.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Pinneri et al-2021 Sample-efficient Cross-Entropy Method for Real-time Planning/Pinneri et al_2021_Sample-efficient Cross-Entropy Method for Real-time Planning.pdf}
}

@inproceedings{polymenakosSafe2019,
  ids = {polymenakosSafe2019a},
  title = {Safe {{Policy Search Using Gaussian Process Models}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Autonomous Agents}} and {{MultiAgent Systems}}},
  author = {Polymenakos, Kyriakos and Abate, Alessandro and Roberts, Stephen},
  year = {2019},
  month = may,
  series = {{{AAMAS}} '19},
  pages = {1565--1573},
  publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
  address = {{Richland, SC}},
  urldate = {2021-06-18},
  abstract = {We propose a method to optimise the parameters of a policy which will be used to safely perform a given task in a data-efficient manner. We train a Gaussian process model to capture the system dynamics, based on the PILCO framework. The model has useful analytic properties, which allow closed form computation of error gradients and the probability of violating given state space constraints. Even during training, only policies that are deemed safe are implemented on the real system, minimising the risk of catastrophic failure.},
  isbn = {978-1-4503-6309-9},
  keywords = {gaussian processes,model-based reinforcement learning,safety critical systems},
  file = {/Users/scannea1/Zotero/storage/LEEH5ULN/Polymenakos et al. - 2019 - Safe Policy Search Using Gaussian Process Models.pdf}
}

@book{pontryagin1987mathematical,
  title = {Mathematical Theory of Optimal Processes},
  author = {Pontryagin, Lev Semenovich},
  year = {1987},
  publisher = {{CRC press}}
}

@article{prajnaBarrier2006,
  title = {Barrier Certificates for Nonlinear Model Validation},
  author = {Prajna, Stephen},
  year = {2006},
  month = jan,
  journal = {Automatica},
  volume = {42},
  number = {1},
  pages = {117--126},
  issn = {0005-1098},
  doi = {10.1016/j.automatica.2005.08.007},
  urldate = {2021-06-29},
  abstract = {Methods for model validation of continuous-time nonlinear systems with uncertain parameters are presented in this paper. The methods employ functions of state-parameter-time, termed barrier certificates, whose existence proves that a model and a feasible parameter set are inconsistent with some time-domain experimental data. A very large class of models can be treated within this framework; this includes differential-algebraic models, models with memoryless/dynamic uncertainties, and hybrid models. Construction of barrier certificates can be performed by convex optimization, utilizing recent results on the sum of squares decomposition of multivariate polynomials.},
  langid = {english},
  keywords = {Barrier certificates,Model validation,Nonlinear systems,Semidefinite programming relaxations},
  file = {/Users/scannea1/Zotero/storage/2GNGMD4X/Prajna - 2006 - Barrier certificates for nonlinear model validatio.pdf;/Users/scannea1/Zotero/storage/JE8FZWCV/S0005109805002839.html}
}

@inproceedings{prajnaSafety2004,
  title = {Safety {{Verification}} of {{Hybrid Systems Using Barrier Certificates}}},
  booktitle = {In {{Hybrid Systems}}: {{Computation}} and {{Control}}},
  author = {Prajna, Stephen and Jadbabaie, Ali},
  year = {2004},
  pages = {477--492},
  publisher = {{Springer}},
  abstract = {This paper presents a novel methodology for safety verification  of hybrid systems. For proving that all trajectories of a hybrid  system do not enter an unsafe region, the proposed method uses a function  of state termed a barrier certificate. The zero level set of a barrier  certificate separates the unsafe region from all possible trajectories starting  from a given set of initial conditions, hence providing an exact proof  of system safety. No explicit computation of reachable sets is required in  the construction of barrier certificates, which makes nonlinearity, uncertainty,  and constraints can be handled directly within this framework.},
  file = {/Users/scannea1/Zotero/storage/DQXKMT8U/Prajna and Jadbabaie - 2004 - Safety Verification of Hybrid Systems Using Barrie.pdf;/Users/scannea1/Zotero/storage/5F4IM4ZH/summary.html}
}

@inproceedings{preechakulDiffusionAutoencodersMeaningful2022,
  title = {Diffusion {{Autoencoders}}: {{Toward}} a {{Meaningful}} and {{Decodable Representation}}},
  shorttitle = {Diffusion {{Autoencoders}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
  year = {2022},
  pages = {10619--10629},
  urldate = {2023-11-21},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/BE7FKIMB/Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf}
}

@misc{qiaoPrimacyBiasModelbased2023,
  title = {The Primacy Bias in {{Model-based RL}}},
  author = {Qiao, Zhongjian and Lyu, Jiafei and Li, Xiu},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15017},
  eprint = {2310.15017},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.15017},
  urldate = {2023-11-02},
  abstract = {The primacy bias in deep reinforcement learning (DRL), which refers to the agent's tendency to overfit early data and lose the ability to learn from new data, can significantly decrease the performance of DRL algorithms. Previous studies have shown that employing simple techniques, such as resetting the agent's parameters, can substantially alleviate the primacy bias. However, we observe that resetting the agent's parameters harms its performance in the context of model-based reinforcement learning (MBRL). In fact, on further investigation, we find that the primacy bias in MBRL differs from that in model-free RL. In this work, we focus on investigating the primacy bias in MBRL and propose world model resetting, which works in MBRL. We apply our method to two different MBRL algorithms, MBPO and DreamerV2. We validate the effectiveness of our method on multiple continuous control tasks on MuJoCo and DeepMind Control Suite, as well as discrete control tasks on Atari 100k benchmark. The results show that world model resetting can significantly alleviate the primacy bias in model-based setting and improve algorithm's performance. We also give a guide on how to perform world model resetting effectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Qiao et al-2023 The primacy bias in Model-based RL/Qiao et al_2023_The primacy bias in Model-based RL.pdf;/Users/scannea1/Zotero/storage/3KK68YTH/2310.html}
}

@inproceedings{quinonero-candelaPropagation2003,
  title = {Propagation of Uncertainty in {{Bayesian}} Kernel Models - Application to Multiple-Step Ahead Forecasting},
  booktitle = {2003 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}.},
  author = {{Quinonero-Candela}, Joaquin and Girard, A. and Larsen, J. and Rasmussen, C.E.},
  year = {2003},
  month = apr,
  volume = {2},
  pages = {II-701},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2003.1202463},
  abstract = {The object of Bayesian modelling is predictive distribution, which, in a forecasting scenario, enables evaluation of forecasted values and their uncertainties. We focus on reliably estimating the predictive mean and variance of forecasted values using Bayesian kernel based models such as the Gaussian process and the relevance vector machine. We derive novel analytic expressions for the predictive mean and variance for Gaussian kernel shapes under the assumption of a Gaussian input distribution in the static case, and of a recursive Gaussian predictive density in iterative forecasting. The capability of the method is demonstrated for forecasting of time-series and compared to approximate methods.},
  keywords = {Analysis of variance,Bayesian methods,Covariance matrix,Equations,Intelligent networks,Kernel,Predictive models,Taylor series,Testing,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/MIKCW6A4/Candela et al. - 2003 - Propagation of uncertainty in Bayesian kernel mode.pdf;/Users/scannea1/Zotero/storage/8E99C732/1202463.html}
}

@article{quinonero-candelaUnifying2005,
  title = {A {{Unifying View}} of {{Sparse Approximate Gaussian Process Regression}}},
  author = {{Qui{\~n}onero-Candela}, Joaquin and Rasmussen, Carl Edward},
  year = {2005},
  journal = {Journal of Machine Learning Research},
  volume = {6},
  number = {65},
  pages = {1939--1959},
  urldate = {2021-02-02},
  abstract = {We provide a new unifying view, including all existing proper probabilistic   sparse approximations for Gaussian process regression. Our approach relies on   expressing the effective prior which the methods are using. This   allows new insights to be gained, and highlights the relationship between   existing methods. It also allows for a clear theoretically justified ranking   of the closeness of the known approximations to the corresponding full GPs.   Finally we point directly to designs of new better sparse approximations,   combining the best of the existing strategies, within attractive   computational constraints.},
  file = {/Users/scannea1/Zotero/storage/H6HSMVQS/Quionero-Candela and Rasmussen - 2005 - A Unifying View of Sparse Approximate Gaussian Pro.pdf}
}

@inproceedings{rafailovOfflineReinforcementLearning2021,
  title = {Offline {{Reinforcement Learning}} from {{Images}} with {{Latent Space Models}}},
  booktitle = {Proceedings of the 3rd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Rafailov, Rafael and Yu, Tianhe and Rajeswaran, Aravind and Finn, Chelsea},
  year = {2021},
  month = may,
  pages = {1154--1168},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-10},
  abstract = {Offline reinforcement learning (RL) refers to the task of learning policies from a static dataset of environment interactions. Offline RL enables extensive utilization and re-use of historical datasets, while also alleviating safety concerns associated with online exploration, thereby expanding the real-world applicability of RL. Most prior work in offline RL has focused on tasks with compact state representations. However, the ability to learn directly from rich observation spaces like images is critical for real-world applications like robotics. In this work, we build on recent advances in model-based algorithms for offline RL, and extend them to high-dimensional visual observation spaces. Model-based offline RL algorithms have achieved state of the art results in state based tasks and are minimax optimal. However, they rely crucially on the ability to quantify uncertainty in the model predictions. This is particularly challenging with image observations. To overcome this challenge, we propose to learn a latent-state dynamics model, and represent the uncertainty in the latent space. Our approach is both tractable in practice and corresponds to maximizing a lower bound of the ELBO in the unknown POMDP. Through experiments on a range of challenging image-based locomotion and robotic manipulation tasks, we find that our algorithm significantly outperforms previous offline model-free RL methods as well as state-of-the-art online visual model-based RL methods. Moreover, we also find that our approach excels on an image-based drawer closing task on a real robot using a pre-existing dataset. All results including videos can be found online at {\textbackslash}url\{https://sites.google.com/view/lompo/\}.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rafailov et al-2021 Offline Reinforcement Learning from Images with Latent Space Models/Rafailov et al_2021_Offline Reinforcement Learning from Images with Latent Space Models.pdf}
}

@inproceedings{rakellyEfficientOffPolicyMetaReinforcement2019,
  title = {Efficient {{Off-Policy Meta-Reinforcement Learning}} via {{Probabilistic Context Variables}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  year = {2019},
  month = may,
  pages = {5331--5340},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-12-01},
  abstract = {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rakelly et al-2019 Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context/Rakelly et al_2019_Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context.pdf;/Users/scannea1/Zotero/storage/4YFXQIYE/Rakelly et al. - 2019 - Efficient Off-Policy Meta-Reinforcement Learning v.pdf}
}

@inproceedings{ranaEuclideanizing2020a,
  title = {Euclideanizing {{Flows}}: {{Diffeomorphic Reduction}} for {{Learning Stable Dynamical Systems}}},
  shorttitle = {Euclideanizing {{Flows}}},
  booktitle = {Proceedings of the 2nd {{Conference}} on {{Learning}} for {{Dynamics}} and {{Control}}},
  author = {Rana, Muhammad Asif and Li, Anqi and Fox, Dieter and Boots, Byron and Ramos, Fabio and Ratliff, Nathan},
  year = {2020},
  month = jul,
  pages = {630--639},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-28},
  abstract = {Execution of complex tasks in robotics requires motions that have complex geometric structure. We present an approach which allows robots to learn such motions from a few human demonstrations. The motions are encoded as rollouts of a dynamical system on a Riemannian manifold. Additional structure is imposed which guarantees smooth convergent motions to a goal location. The aforementioned structure involves viewing motions on an observed Riemannian manifold as deformations of straight lines on a latent Euclidean space. The observed and latent spaces are related through a diffeomorphism. Thus, this paper presents an approach for learning flexible diffeomorphisms, resulting in a stable dynamical system.  The efficacy of this approach is demonstrated through validation on an established benchmark as well demonstrations collected on a real-world robotic system.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/3TZFTDTQ/Rana et al. - 2020 - Euclideanizing Flows Diffeomorphic Reduction for .pdf}
}

@book{rasmussenGaussian2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/scannea1/Zotero/storage/9KYKSDXH/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@inproceedings{rasmussenInfinite2001,
  title = {Infinite {{Mixtures}} of {{Gaussian Process Experts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmussen, Carl and Ghahramani, Zoubin},
  year = {2001},
  volume = {14},
  pages = {881--888},
  urldate = {2020-11-26},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/K55Z2K8S/Rasmussen and Ghahramani - 2001 - Infinite Mixtures of Gaussian Process Experts.pdf;/Users/scannea1/Zotero/storage/X9FGAZH2/9afefc52942cb83c7c1f14b2139b09ba-Abstract.html}
}

@inproceedings{rasmussenOccam2001,
  title = {Occam' s {{Razor}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rasmussen, Carl and Ghahramani, Zoubin},
  year = {2001},
  volume = {13},
  publisher = {{MIT Press}},
  urldate = {2021-10-13},
  file = {/Users/scannea1/Zotero/storage/SYEYLMMN/Rasmussen and Ghahramani - 2001 - Occam' s Razor.pdf}
}

@phdthesis{rawlikProbabilistic2013,
  title = {On {{Probabilistic Inference Approaches}} to {{Stochastic Optimal Control}}},
  author = {Rawlik, Konrad C},
  year = {2013},
  abstract = {While stochastic optimal control, together with associate formulations like Reinforcement Learning, provides a formal approach to, amongst other, motor control, it remains computationally challenging for most practical problems. This thesis is concerned with the study of relations between stochastic optimal control and probabilistic inference. Such dualities {\textendash} exemplified by the classical Kalman Duality between the Linear-Quadratic-Gaussian control problem and the filtering problem in Linear-Gaussian dynamical systems {\textendash} make it possible to exploit advances made within the separate fields. In this context, the emphasis in this work lies with utilisation of approximate inference methods for the control problem.},
  langid = {english},
  school = {University of Edinburgh},
  file = {/Users/scannea1/Zotero/storage/H74IMGYX/Rawlik - On Probabilistic Inference Approaches to Stochasti.pdf}
}

@inproceedings{rawlikStochastic2013,
  title = {On {{Stochastic Optimal Control}} and {{Reinforcement Learning}} by {{Approximate Inference}} ({{Extended Abstract}})},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Artificial Intelligence}}},
  author = {Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  year = {2013},
  pages = {5},
  abstract = {We present a reformulation of the stochastic optimal control problem in terms of KL divergence minimisation, not only providing a unifying perspective of previous approaches in this area, but also demonstrating that the formalism leads to novel practical approaches to the control problem. Specifically, a natural relaxation of the dual formulation gives rise to exact iterative solutions to the finite and infinite horizon stochastic optimal control problem, while direct application of Bayesian inference methods yields instances of risk sensitive control.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/XRQ3NLZ6/Rawlik et al. - On Stochastic Optimal Control and Reinforcement Le.pdf}
}

@article{reedGeneralistAgent2022,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and {Barth-maron}, Gabriel and Gim{\'e}nez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  year = {2022},
  month = nov,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2023-06-16},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Reed et al-2022 A Generalist Agent/Reed et al_2022_A Generalist Agent.pdf}
}

@article{reedGeneralistAgent2022a,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and {Barth-maron}, Gabriel and Gim{\'e}nez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  year = {2022},
  month = aug,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2023-09-30},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Reed et al-2022 A Generalist Agent/Reed et al_2022_A Generalist Agent2.pdf}
}

@inproceedings{rezaei-shoshtariContinuousMDPHomomorphisms2022,
  title = {Continuous {{MDP Homomorphisms}} and {{Homomorphic Policy Gradient}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Rezaei-Shoshtari}, Sahand and Zhao, Rosie and Panangaden, Prakash and Meger, David and Precup, Doina},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {20189--20204},
  urldate = {2023-11-02},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rezaei-Shoshtari et al-2022 Continuous MDP Homomorphisms and Homomorphic Policy Gradient/Rezaei-Shoshtari et al_2022_Continuous MDP Homomorphisms and Homomorphic Policy Gradient.pdf}
}

@inproceedings{riquelmeDeepBayesianBandits2018,
  title = {Deep {{Bayesian Bandits Showdown}}: {{An Empirical Comparison}} of {{Bayesian Deep Networks}} for {{Thompson Sampling}}},
  shorttitle = {Deep {{Bayesian Bandits Showdown}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Riquelme, Carlos and Tucker, George and Snoek, Jasper},
  year = {2018},
  month = feb,
  urldate = {2023-09-30},
  abstract = {Recent advances in deep reinforcement learning have made significant strides in performance on applications such as Go and Atari games. However, developing practical methods to balance exploration and exploitation in complex domains remains largely unsolved. Thompson Sampling and its extension to reinforcement learning provide an elegant approach to exploration that only requires access to posterior samples of the model. At the same time, advances in approximate Bayesian methods have made posterior approximation for flexible neural network models practical. Thus, it is attractive to consider approximate Bayesian neural networks in a Thompson Sampling framework. To understand the impact of using an approximate posterior on Thompson Sampling, we benchmark well-established and recently developed methods for approximate posterior sampling combined with Thompson Sampling over a series of contextual bandit problems. We found that many approaches that have been successful in the supervised learning setting underperformed in the sequential decision-making scenario. In particular, we highlight the challenge of adapting slowly converging uncertainty estimates to the online setting.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Riquelme et al-2018 Deep Bayesian Bandits Showdown/Riquelme et al_2018_Deep Bayesian Bandits Showdown.pdf}
}

@article{rockafellarConditional2001,
  title = {Conditional {{Value-at-Risk}} for {{General Loss Distributions}}},
  author = {Rockafellar, R. and Uryasev, S.},
  year = {2001},
  journal = {Journal of Banking \& Finance},
  pages = {1443--1471},
  urldate = {2022-05-02},
  abstract = {Fundamental properties of conditional value-at-risk, as a measure of risk with significant advantages over value-at-risk, are derived for loss distributions in finance that can involve discreetness. Such distributions are of particular importance in applications because of the prevalence of models based on scenarios and finite sampling. Conditional value-at-risk is able to quantify dangers beyond value-at-risk, and moreover it is coherent. It provides optimization shortcuts which, through linear programming techniques, make practical many large-scale calculations that could otherwise be out of reach. The numerical efficiency and stability of such calculations, shown in several case studies, are illustrated further with an example of index tracking.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/2837SYH3/d243a33c209f41453927fe969949288abb4c3382.html}
}

@inproceedings{rohrProbabilistic2021,
  title = {Probabilistic Robust Linear Quadratic Regulators with {{Gaussian}} Processes},
  booktitle = {Learning for {{Dynamics}} and {{Control}}},
  author = {von Rohr, Alexander and {Neumann-Brosig}, Matthias and Trimpe, Sebastian},
  year = {2021},
  month = may,
  pages = {324--335},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-18},
  abstract = {Probabilistic models such as Gaussian processes (GPs) are powerful tools to learn unknown dynamical systems from data for subsequent use in control design. While learning-based control has the pote...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/49R9IA7G/Rohr et al. - 2021 - Probabilistic robust linear quadratic regulators w.pdf;/Users/scannea1/Zotero/storage/BX9VVXBP/rohr21a.html}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis With Latent Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  urldate = {2023-11-27},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rombach et al-2022 High-Resolution Image Synthesis With Latent Diffusion Models/Rombach et al_2022_High-Resolution Image Synthesis With Latent Diffusion Models.pdf}
}

@article{rosenbrockDifferential1972,
  title = {Differential {{Dynamic Programming}}. {{By D}}.{{H}}. {{Jacobson}} and {{D}}. {{Q}}. {{Mayne}}. {{Pp}}. Viii, 208. 1970. ({{Elsevier}}.)},
  author = {Rosenbrock, H. H.},
  year = {1972},
  month = feb,
  journal = {The Mathematical Gazette},
  volume = {56},
  number = {395},
  pages = {78--78},
  publisher = {{Cambridge University Press}},
  issn = {0025-5572, 2056-6328},
  doi = {10.2307/3613752},
  urldate = {2021-06-21},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0025557200188072/resource/name/firstPage-S0025557200188072a.jpg},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/BDJAYGT9/CB6EA5B3191A305CFB1D2CDF9CC5BA1D.html}
}

@article{rossellApproximateLaplaceApproximations2021,
  title = {Approximate {{Laplace Approximations}} for {{Scalable Model Selection}}},
  author = {Rossell, David and Abril, Oriol and Bhattacharya, Anirban},
  year = {2021},
  month = sep,
  journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume = {83},
  number = {4},
  pages = {853--879},
  issn = {1369-7412},
  doi = {10.1111/rssb.12466},
  urldate = {2023-03-20},
  abstract = {We propose the approximate Laplace approximation (ALA) to evaluate integrated likelihoods, a bottleneck in Bayesian model selection. The Laplace approximation (LA) is a popular tool that speeds up such computation and equips strong model selection properties. However, when the sample size is large or one considers many models the cost of the required optimizations becomes impractical. ALA reduces the cost to that of solving a least-squares problem for each model. Further, it enables efficient computation across models such as sharing pre-computed sufficient statistics and certain operations in matrix decompositions. We prove that in generalized (possibly non-linear) models ALA achieves a strong form of model selection consistency for a suitably-defined optimal model, at the same functional rates as exact computation. We consider fixed- and high-dimensional problems, group and hierarchical constraints, and the possibility that all models are misspecified. We also obtain ALA rates for Gaussian regression under non-local priors, an important example where the LA can be costly and does not consistently estimate the integrated likelihood. Our examples include non-linear regression, logistic, Poisson and survival models. We implement the methodology in the R package mombf.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rossell et al-2021 Approximate Laplace Approximations for Scalable Model Selection/Rossell et al_2021_Approximate Laplace Approximations for Scalable Model Selection.pdf;/Users/scannea1/Zotero/storage/AILK9H9W/7056067.html}
}

@inproceedings{rossiSparse2021,
  title = {Sparse {{Gaussian Processes Revisited}}: {{Bayesian Approaches}} to {{Inducing-Variable Approximations}}},
  shorttitle = {Sparse {{Gaussian Processes Revisited}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Rossi, Simone and Heinonen, Markus and Bonilla, Edwin and Shen, Zheyang and Filippone, Maurizio},
  year = {2021},
  month = mar,
  pages = {1837--1845},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-15},
  abstract = {Variational inference techniques based on inducing variables provide an elegant framework for scalable posterior estimation in Gaussian process (GP) models. Besides enabling scalability, one of their main advantages over sparse approximations using direct marginal likelihood maximization is that they provide a robust alternative for point estimation of the inducing inputs, i.e. the location of the inducing variables. In this work we challenge the common wisdom that optimizing the inducing inputs in the variational framework yields optimal performance. We show that, by revisiting old model approximations such as the fully-independent training conditionals endowed with powerful sampling-based inference methods, treating both inducing locations and GP hyper-parameters in a Bayesian way can improve performance significantly. Based on stochastic gradient Hamiltonian Monte Carlo, we develop a fully Bayesian approach to scalable GP and deep GP models, and demonstrate its state-of-the-art performance through an extensive experimental campaign across several regression and classification problems.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/IZRAMFPA/Rossi et al. - 2021 - Sparse Gaussian Processes Revisited Bayesian Appr.pdf;/Users/scannea1/Zotero/storage/UUBVVQJT/Rossi et al. - 2021 - Sparse Gaussian Processes Revisited Bayesian Appr.pdf}
}

@article{rossPseudospectral2004,
  title = {Pseudospectral Methods for Optimal Motion Planning of Differentially Flat Systems},
  author = {Ross, I. M. and Fahroo, F.},
  year = {2004},
  month = aug,
  journal = {IEEE Transactions on Automatic Control},
  volume = {49},
  number = {8},
  pages = {1410--1413},
  issn = {1558-2523},
  doi = {10.1109/TAC.2004.832972},
  abstract = {The article presents some preliminary results on combining two new ideas from nonlinear control theory and dynamic optimization. We show that the computational framework facilitated by pseudospectral methods applies quite naturally and easily to Fliess' implicit state variable representation of dynamical systems. The optimal motion planning problem for differentially flat systems is equivalent to a classic Bolza problem of the calculus of variations. We exploit the notion that derivatives of flat outputs given in terms of Lagrange polynomials at Legendre-Gauss-Lobatto points can be quickly computed using pseudospectral differentiation matrices. Additionally, the Legendre pseudospectral method approximates integrals by Gauss-type quadrature rules. The application of this method to the two-dimensional crane model reveals how differential flatness may be readily exploited.},
  keywords = {Asymptotic stability,Automatic control,Bolza problem,Control systems,Control theory,differentially flat systems,differentiation,dynamic optimization,dynamical systems,Fliess implicit state variable representation,Gauss-type quadrature rules,integration,Lagrange polynomials,Lagrange pseudospectral methods,Legendre-Gauss-Lobatto points,Lyapunov method,matrix algebra,nonlinear control systems,nonlinear control theory,Nonlinear systems,optimal control,optimal motion planning,optimisation,Optimization methods,path planning,polynomials,pseudospectral differentiation matrices,Rivers,Time varying systems,time-varying systems,two-dimensional crane model,variational calculus,variational techniques},
  file = {/Users/scannea1/Zotero/storage/TY8D8LGT/Ross and Fahroo - 2004 - Pseudospectral methods for optimal motion planning.pdf;/Users/scannea1/Zotero/storage/BN6XJXPM/1323189.html}
}

@inproceedings{royDirectBehaviorSpecification2022,
  title = {Direct {{Behavior Specification}} via {{Constrained Reinforcement Learning}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Roy, Julien and Girgis, Roger and Romoff, Joshua and Bacon, Pierre-Luc and Pal, Chris J.},
  year = {2022},
  month = jun,
  pages = {18828--18843},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-10-13},
  abstract = {The standard formulation of Reinforcement Learning lacks a practical way of specifying what are admissible and forbidden behaviors. Most often, practitioners go about the task of behavior specification by manually engineering the reward function, a counter-intuitive process that requires several iterations and is prone to reward hacking by the agent. In this work, we argue that constrained RL, which has almost exclusively been used for safe RL, also has the potential to significantly reduce the amount of work spent for reward specification in applied RL projects. To this end, we propose to specify behavioral preferences in the CMDP framework and to use Lagrangian methods to automatically weigh each of these behavioral constraints. Specifically, we investigate how CMDPs can be adapted to solve goal-based tasks while adhering to several constraints simultaneously. We evaluate this framework on a set of continuous control tasks relevant to the application of Reinforcement Learning for NPC design in video games.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Roy et al-2022 Direct Behavior Specification via Constrained Reinforcement Learning/Roy et al_2022_Direct Behavior Specification via Constrained Reinforcement Learning.pdf}
}

@inproceedings{rudnerTractableFunctionSpaceVariational2022,
  title = {Tractable {{Function-Space Variational Inference}} in {{Bayesian Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rudner, Tim G. J. and Chen, Zonghao and Teh, Yee Whye and Gal, Yarin},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {22686--22698},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Rudner et al-2022 Tractable Function-Space Variational Inference in Bayesian Neural Networks/Rudner et al_2022_Tractable Function-Space Variational Inference in Bayesian Neural Networks.pdf}
}

@article{rybkinModelBased2021,
  title = {Model-{{Based Reinforcement Learning}} via {{Latent-Space Collocation}}},
  author = {Rybkin, Oleh and Zhu, Chuning and Nagabandi, Anusha and Daniilidis, Kostas and Mordatch, Igor and Levine, Sergey},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.13229 [cs]},
  eprint = {2106.13229},
  primaryclass = {cs},
  urldate = {2021-06-29},
  abstract = {The ability to plan into the future while utilizing only raw high-dimensional observations, such as images, can provide autonomous agents with broad capabilities. Visual model-based reinforcement learning (RL) methods that plan future actions directly have shown impressive results on tasks that require only short-horizon reasoning, however, these methods struggle on temporally extended tasks. We argue that it is easier to solve long-horizon tasks by planning sequences of states rather than just actions, as the effects of actions greatly compound over time and are harder to optimize. To achieve this, we draw on the idea of collocation, which has shown good results on long-horizon tasks in optimal control literature, and adapt it to the image-based setting by utilizing learned latent state space models. The resulting latent collocation method (LatCo) optimizes trajectories of latent states, which improves over previously proposed shooting methods for visual model-based RL on tasks with sparse rewards and long-term goals. Videos and code at https://orybkin.github.io/latco/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: International Conference on Machine Learning (ICML), 2021. Videos and code at https://orybkin.github.io/latco/},
  file = {/Users/scannea1/Zotero/storage/MDHUIW24/Rybkin et al. - 2021 - Model-Based Reinforcement Learning via Latent-Spac.pdf;/Users/scannea1/Zotero/storage/RARRLXIJ/2106.html}
}

@inproceedings{sadighSafe2016,
  ids = {sadighSafe2016a},
  title = {Safe {{Control Under Uncertainty}} with {{Probabilistic Signal Temporal Logic}}},
  booktitle = {Proceedings of {{Robotics}}: {{Science}} and {{Systems XII}}},
  author = {Sadigh, Dorsa and Kapoor, Ashish},
  year = {2016},
  month = jun,
  urldate = {2022-05-02},
  abstract = {Controller synthesis for hybrid systems that satisfy temporal specifications expressing various system properties is a challenging problem that has drawn the attention of many researchers. However, making the assumption that such temporal properties are deterministic is far from the reality. For example, many of the properties the controller has to satisfy are learned through machine [{\dots}]},
  langid = {american},
  file = {/Users/scannea1/Zotero/storage/33KXXXBE/Sadigh and Kapoor - 2016 - Safe Control under Uncertainty with Probabilistic .pdf;/Users/scannea1/Zotero/storage/JT2L6HUF/safe-control-uncertainty-probabilistic-signal-temporal-logic.html}
}

@inproceedings{salimbeniDeep2019,
  title = {Deep {{Gaussian Processes}} with {{Importance-Weighted Variational Inference}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Salimbeni, Hugh and Dutordoir, Vincent and Hensman, James and Deisenroth, Marc},
  year = {2019},
  month = may,
  pages = {5589--5598},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-05-12},
  abstract = {Deep Gaussian processes (DGPs) can model complex marginal densities as well as complex mappings. Non-Gaussian marginals are essential for modelling real-world data, and can be generated from the DG...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/M3UC5GE3/Salimbeni et al. - 2019 - Deep Gaussian Processes with Importance-Weighted V.pdf;/Users/scannea1/Zotero/storage/SN6XD5GN/salimbeni19a.html}
}

@inproceedings{salimbeniDoubly2017,
  title = {Doubly {{Stochastic Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salimbeni, Hugh and Deisenroth, Marc},
  year = {2017},
  volume = {30},
  pages = {4588--4599},
  urldate = {2020-11-26},
  langid = {english},
  keywords = {deep-gaussian-processes,gaussian-processes,variational-inference},
  file = {/Users/scannea1/Zotero/storage/6IZTGNS9/Salimbeni and Deisenroth - 2017 - Doubly Stochastic Variational Inference for Deep G.pdf;/Users/scannea1/Zotero/storage/QV5N9ZPA/8208974663db80265e9bfe7b222dcb18-Abstract.html}
}

@article{sasakiVariational2021,
  title = {Variational Policy Search Using Sparse {{Gaussian}} Process Priors for Learning Multimodal Optimal Actions},
  author = {Sasaki, Hikaru and Matsubara, Takamitsu},
  year = {2021},
  month = jun,
  journal = {Neural Networks},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2021.06.010},
  urldate = {2021-06-18},
  abstract = {Policy search reinforcement learning has been drawing much attention as a method of learning a robot control policy. In particular, policy search using such non-parametric policies as Gaussian process regression can learn optimal actions with high-dimensional and redundant sensors as input. However, previous methods implicitly assume that the optimal action becomes unique for each state. This assumption can severely limit such practical applications as robot manipulations since designing a reward function that appears in only one optimal action for complex tasks is difficult. The previous methods might have caused critical performance deterioration because the typical non-parametric policies cannot capture the optimal actions due to their unimodality. We propose novel approaches in non-parametric policy searches with multiple optimal actions and offer two different algorithms commonly based on a sparse Gaussian process prior and variational Bayesian inference. The following are the key ideas: (1) multimodality for capturing multiple optimal actions and (2) mode-seeking for capturing one optimal action by ignoring the others. First, we propose a multimodal sparse Gaussian process policy search that uses multiple overlapped GPs as a prior. Second, we propose a mode-seeking sparse Gaussian process policy search that uses the student-t distribution for a likelihood function. The effectiveness of those algorithms is demonstrated through applications to object manipulation tasks with multiple optimal actions in simulations.},
  langid = {english},
  keywords = {Gaussian processes,Mode-seeking,Multimodality,Policy search,Reinforcement learning},
  file = {/Users/scannea1/Zotero/storage/JK92RTY5/S0893608021002422.html}
}

@misc{sassoMultiSourceTransferLearning2023,
  title = {Multi-{{Source Transfer Learning}} for {{Deep Model-Based Reinforcement Learning}}},
  author = {Sasso, Remo and Sabatelli, Matthia and Wiering, Marco A.},
  year = {2023},
  month = apr,
  number = {arXiv:2205.14410},
  eprint = {2205.14410},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {A crucial challenge in reinforcement learning is to reduce the number of interactions with the environment that an agent requires to master a given task. Transfer learning proposes to address this issue by re-using knowledge from previously learned tasks. However, determining which source task qualifies as the most appropriate for knowledge extraction, as well as the choice regarding which algorithm components to transfer, represent severe obstacles to its application in reinforcement learning. The goal of this paper is to address these issues with modular multi-source transfer learning techniques. The proposed techniques automatically learn how to extract useful information from source tasks, regardless of the difference in state-action space and reward function. We support our claims with extensive and challenging cross-domain experiments for visual control.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.m},
  note = {Comment: 24 pages, 7 figures, 8 tables. arXiv admin note: text overlap with arXiv:2108.06526},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Sasso et al-2023 Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning/Sasso et al_2023_Multi-Source Transfer Learning for Deep Model-Based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/I328VBCX/2205.html}
}

@inproceedings{sassoPosteriorSamplingDeep2023,
  title = {Posterior {{Sampling}} for {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Sasso, Remo and Conserva, Michelangelo and Rauber, Paulo},
  year = {2023},
  month = jul,
  pages = {30042--30061},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored incremental planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being competitive with a state-of-the-art (model-based) reinforcement learning method, both in sample efficiency and computational efficiency.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Sasso et al-2023 Posterior Sampling for Deep Reinforcement Learning/Sasso et al_2023_Posterior Sampling for Deep Reinforcement Learning.pdf}
}

@inproceedings{scannellModeconstrainedModelbasedReinforcement2023,
  title = {Mode-Constrained {{Model-based Reinforcement Learning}} via {{Gaussian Processes}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Scannell, Aidan and Ek, Carl Henrik and Richards, Arthur},
  year = {2023},
  month = apr,
  pages = {3299--3314},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-05-13},
  abstract = {Model-based reinforcement learning (RL) algorithms do not typically consider environments with multiple dynamic modes, where it is beneficial to avoid inoperable or undesirable modes. We present a model-based RL algorithm that constrains training to a single dynamic mode with high probability. This is a difficult problem because the mode constraint is a hidden variable associated with the environment's dynamics. As such, it is 1) unknown a priori and 2) we do not observe its output from the environment, so cannot learn it with supervised learning. We present a nonparametric dynamic model which learns the mode constraint alongside the dynamic modes. Importantly, it learns latent structure that our planning scheme leverages to 1) enforce the mode constraint with high probability, and 2) escape local optima induced by the mode constraint. We validate our method by showing that it can solve a simulated quadcopter navigation task whilst providing a level of constraint satisfaction both during and after training.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Scannell et al-2023 Mode-constrained Model-based Reinforcement Learning via Gaussian Processes/Scannell et al_2023_Mode-constrained Model-based Reinforcement Learning via Gaussian Processes.pdf}
}

@inproceedings{scannellTrajectory2021,
  title = {Trajectory {{Optimisation}} in {{Learned Multimodal Dynamical Systems Via Latent-ODE Collocation}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Scannell, Aidan and Ek, Carl Henrik and Richards, Arthur},
  year = {2021},
  publisher = {{IEEE}},
  abstract = {This paper presents a two-stage method to perform trajectory optimisation in multimodal dynamical systems with unknown nonlinear stochastic transition dynamics. The method finds trajectories that remain in a preferred dynamics mode where possible and in regions of the transition dynamics model that have been observed and can be predicted confidently. The first stage leverages a Mixture of Gaussian Process Experts method to learn a predictive dynamics model from historical data. Importantly, this model learns a gating function that indicates the probability of being in a particular dynamics mode at a given state location. This gating function acts as a coordinate map for a latent Riemannian manifold on which shortest trajectories are solutions to our trajectory optimisation problem. Based on this intuition, the second stage formulates a geometric cost function, which it then implicitly minimises by projecting the trajectory optimisation onto the second-order geodesic ODE; a classic result of Riemannian geometry. A set of collocation constraints are derived that ensure trajectories are solutions to this ODE, implicitly solving the trajectory optimisation problem.},
  file = {/Users/scannea1/Zotero/storage/JLG5TV8J/Scannell et al. - 2021 - Trajectory Optimisation in Learned Multimodal Dyna.pdf}
}

@inproceedings{schneiderExploiting1996,
  title = {Exploiting {{Model Uncertainty Estimates}} for {{Safe Dynamic Control Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schneider, J.},
  year = {1996},
  volume = {9},
  pages = {1047--1053},
  abstract = {Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures during learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with dynamic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the converse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem.}
}

@article{schonSystem2011,
  title = {System Identification of Nonlinear State-Space Models},
  author = {Sch{\"o}n, Thomas B. and Wills, Adrian and Ninness, Brett},
  year = {2011},
  month = jan,
  journal = {Automatica},
  volume = {47},
  number = {1},
  pages = {39--49},
  issn = {00051098},
  doi = {10.1016/j.automatica.2010.10.013},
  urldate = {2021-09-10},
  abstract = {This paper is concerned with the parameter estimation of a general class of nonlinear dynamic systems in state-space form. More specifically, a Maximum Likelihood (ML) framework is employed and an Expectation Maximisation (EM) algorithm is derived to compute these ML estimates. The Expectation (E) step involves solving a nonlinear state estimation problem, where the smoothed estimates of the states are required. This problem lends itself perfectly to the particle smoother, which provide arbitrarily good estimates. The maximisation (M) step is solved using standard techniques from numerical optimisation theory. Simulation examples demonstrate the efficacy of our proposed solution.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/GHK4KDGX/Schn et al. - 2011 - System identification of nonlinear state-space mod.pdf}
}

@inproceedings{schreiterSafe2015,
  title = {Safe {{Exploration}} for {{Active Learning}} with {{Gaussian Processes}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Schreiter, Jens and {Nguyen-Tuong}, Duy and Eberts, Mona and Bischoff, Bastian and Markert, Heiner and Toussaint, Marc},
  year = {2015},
  pages = {133--149},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-23461-8_9},
  abstract = {In this paper, the problem of safe exploration in the active learning context is considered. Safe exploration is especially important for data sampling from technical and industrial systems, e.g. combustion engines and gas turbines, where critical and unsafe measurements need to be avoided. The objective is to learn data-based regression models from such technical systems using a limited budget of measured, i.e. labelled, points while ensuring that critical regions of the considered systems are avoided during measurements. We propose an approach for learning such models and exploring new data regions based on Gaussian processes (GP's). In particular, we employ a problem specific GP classifier to identify safe and unsafe regions, while using a differential entropy criterion for exploring relevant data regions. A theoretical analysis is shown for the proposed algorithm, where we provide an upper bound for the probability of failure. To demonstrate the efficiency and robustness of our safe exploration scheme in the active learning setting, we test the approach on a policy exploration task for the inverse pendulum hold up problem.},
  isbn = {978-3-319-23461-8},
  langid = {english},
  keywords = {Decision Boundary,Discriminative Function,Exploration Scheme,Gaussian Process,Input Space},
  file = {/Users/scannea1/Zotero/storage/8HEA83P2/Schreiter et al. - 2015 - Safe Exploration for Active Learning with Gaussian.pdf}
}

@article{schrittwieserMastering2020,
  title = {Mastering {{Atari}}, {{Go}}, {{Chess}} and {{Shogi}} by {{Planning}} with a {{Learned Model}}},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  eprint = {1911.08265},
  primaryclass = {cs, stat},
  pages = {604--609},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  urldate = {2022-06-25},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/JW8GMQEQ/Schrittwieser et al. - 2020 - Mastering Atari, Go, Chess and Shogi by Planning w.pdf;/Users/scannea1/Zotero/storage/SA8X3VBI/1911.html}
}

@inproceedings{schrittwieserOnlineOfflineReinforcement2021,
  title = {Online and {{Offline Reinforcement Learning}} by {{Planning}} with a {{Learned Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schrittwieser, Julian and Hubert, Thomas and Mandhane, Amol and Barekatain, Mohammadamin and Antonoglou, Ioannis and Silver, David},
  year = {2021},
  volume = {34},
  pages = {27580--27591},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-09-30},
  abstract = {Learning efficiently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment, and the offline case when learning from a fixed dataset. However, to date no single unified algorithm could demonstrate state-of-the-art results for both settings.In this work, we describe the Reanalyse algorithm, which uses model-based policy and value improvement operators to compute improved training targets for existing data points, allowing for efficient learning at data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn completely without environment interactions, as in the case of Offline Reinforcement Learning (Offline RL). Combining Reanalyse with the MuZero algorithm, we introduce MuZero Unplugged, a single unified algorithm for any data budget, including Offline RL. In contrast to previous work, our algorithm requires no special adaptations for the off-policy or Offline RL settings. MuZero Unplugged sets new state-of-the-art results for Atari in the standard 200 million frame online setting as well as in the RL Unplugged Offline RL benchmark.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Schrittwieser et al-2021 Online and Offline Reinforcement Learning by Planning with a Learned Model/Schrittwieser et al_2021_Online and Offline Reinforcement Learning by Planning with a Learned Model.pdf}
}

@article{schwarmChanceconstrained1999,
  title = {Chance-Constrained Model Predictive Control},
  author = {Schwarm, Alexander T. and Nikolaou, Michael},
  year = {1999},
  journal = {AIChE Journal},
  volume = {45},
  number = {8},
  pages = {1743--1752},
  issn = {1547-5905},
  doi = {10.1002/aic.690450811},
  urldate = {2022-05-02},
  abstract = {This work focuses on robustness of model-predictive control with respect to satisfaction of process output constraints. A method of improving such robustness is presented. The method relies on formulating output constraints as chance constraints using the uncertainty description of the process model. The resulting on-line optimization problem is convex. The proposed approach is illustrated through a simulation case study on a high-purity distillation column. Suggestions for further improvements are made.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/G8ZC8FQA/aic.html}
}

@inproceedings{seegerFast2003,
  title = {Fast Forward Selection to Speed up Sparse Gaussian Process Regression},
  booktitle = {Proceedings of the {{Ninth International Workshop}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Seeger, Matthias and Williams, Christopher K. I. and Lawrence, Neil D.},
  year = {2003},
  abstract = {We present a method for the sparse greedy approximation of Bayesian Gaussian process regression, featuring a novel heuristic for very fast forward selection. Our method is essentially as fast as an equivalent one which selects the ``support'' patterns at random, yet it can outperform random selection on hard curve fitting tasks. More importantly, it leads to a sufficiently stable approximation of the log marginal likelihood of the training data, which can be optimised to adjust a large number of hyperparameters automatically. We demonstrate the model selection capabilities of the algorithm in a range of experiments. In line with the development of our method, we present a simple view on sparse approximations for GP models and their underlying assumptions and show relations to other methods.},
  file = {/Users/scannea1/Zotero/storage/HUFUTERH/Seeger et al. - 2003 - Fast forward selection to speed up sparse gaussian.pdf;/Users/scannea1/Zotero/storage/F3LX47VP/summary.html}
}

@inproceedings{sekarPlanning2020,
  title = {Planning to {{Explore}} via {{Self-Supervised World Models}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  year = {2020},
  month = nov,
  pages = {8583--8592},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-05-02},
  abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1.github.io/plan2explore/},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/4RJFWDE7/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf;/Users/scannea1/Zotero/storage/79JZ8WW6/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf}
}

@inproceedings{seoMaskedWorldModels2023,
  title = {Masked {{World Models}} for {{Visual Control}}},
  booktitle = {Proceedings of {{The}} 6th {{Conference}} on {{Robot Learning}}},
  author = {Seo, Younggyo and Hafner, Danijar and Liu, Hao and Liu, Fangchen and James, Stephen and Lee, Kimin and Abbeel, Pieter},
  year = {2023},
  month = mar,
  pages = {1332--1344},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-23},
  abstract = {Visual model-based reinforcement learning (RL) has the potential to enable sample-efficient robot learning from visual observations. Yet the current approaches typically train a single model end-to-end for learning both visual representations and dynamics, making it difficult to accurately model the interaction between robots and small objects. In this work, we introduce a visual model-based RL framework that decouples visual representation learning and dynamics learning. Specifically, we train an autoencoder with convolutional layers and vision transformers (ViT) to reconstruct pixels given masked convolutional features, and learn a latent dynamics model that operates on the representations from the autoencoder. Moreover, to encode task-relevant information, we introduce an auxiliary reward prediction objective for the autoencoder. We continually update both autoencoder and dynamics model using online samples collected from environment interaction. We demonstrate that our decoupling approach achieves state-of-the-art performance on a variety of visual robotic tasks from Meta-world and RLBench, e.g., we achieve 81.7\% success rate on 50 visual robotic manipulation tasks from Meta-world, while the baseline achieves 67.9\%. Code is available on the project website: https://sites.google.com/view/mwm-rl.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/2226P336/Seo et al. - 2023 - Masked World Models for Visual Control.pdf;/Users/scannea1/Zotero/storage/HRGN26Q4/Seo et al. - 2023 - Masked World Models for Visual Control.pdf}
}

@inproceedings{seoReinforcementLearningActionFree2022,
  title = {Reinforcement {{Learning}} with {{Action-Free Pre-Training}} from {{Videos}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Seo, Younggyo and Lee, Kimin and James, Stephen L. and Abbeel, Pieter},
  year = {2022},
  month = jun,
  pages = {19561--19579},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-03},
  abstract = {Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at {\textbackslash}url\{https://github.com/younggyoseo/apv\}.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Seo et al-2022 Reinforcement Learning with Action-Free Pre-Training from Videos/Seo et al_2022_Reinforcement Learning with Action-Free Pre-Training from Videos.pdf}
}

@inproceedings{seoReinforcementLearningActionFree2022a,
  title = {Reinforcement {{Learning}} with {{Action-Free Pre-Training}} from {{Videos}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Seo, Younggyo and Lee, Kimin and James, Stephen L. and Abbeel, Pieter},
  year = {2022},
  month = jun,
  pages = {19561--19579},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-24},
  abstract = {Recent unsupervised pre-training methods have shown to be effective on language and vision domains by learning useful representations for multiple downstream tasks. In this paper, we investigate if such unsupervised pre-training methods can also be effective for vision-based reinforcement learning (RL). To this end, we introduce a framework that learns representations useful for understanding the dynamics via generative pre-training on videos. Our framework consists of two phases: we pre-train an action-free latent video prediction model, and then utilize the pre-trained representations for efficiently learning action-conditional world models on unseen environments. To incorporate additional action inputs during fine-tuning, we introduce a new architecture that stacks an action-conditional latent prediction model on top of the pre-trained action-free prediction model. Moreover, for better exploration, we propose a video-based intrinsic bonus that leverages pre-trained representations. We demonstrate that our framework significantly improves both final performances and sample-efficiency of vision-based RL in a variety of manipulation and locomotion tasks. Code is available at {\textbackslash}url\{https://github.com/younggyoseo/apv\}.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/73PNNHWY/Seo et al. - 2022 - Reinforcement Learning with Action-Free Pre-Traini.pdf}
}

@article{shahriariTaking2016,
  title = {Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}},
  shorttitle = {Taking the {{Human Out}} of the {{Loop}}},
  author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and {de Freitas}, Nando},
  year = {2016},
  month = jan,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {1},
  pages = {148--175},
  issn = {1558-2256},
  abstract = {Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
  keywords = {Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning},
  file = {/Users/scannea1/Zotero/storage/PX9FTIL2/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf;/Users/scannea1/Zotero/storage/XLYEGD6W/7352306.html}
}

@article{shumwayAPPROACH1982,
  title = {{{AN APPROACH TO TIME SERIES SMOOTHING AND FORECASTING USING THE EM ALGORITHM}}},
  author = {Shumway, R. and Stoffer, D.},
  year = {1982},
  journal = {Journal of Time Series Analysis},
  doi = {10.1111/J.1467-9892.1982.TB00349.X},
  abstract = {Abstract. An approach to smoothing and forecasting for time series with missing observations is proposed. For an underlying state-space model, the EM algorithm is used in conjunction with the conventional Kalman smoothed estimators to derive a simple recursive procedure for estimating the parameters by maximum likelihood. An example is given which involves smoothing and forecasting an economic series using the maximum likelihood estimators for the parameters.}
}

@inproceedings{shyamModelBasedActiveExploration2019,
  title = {Model-{{Based Active Exploration}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2019},
  month = may,
  pages = {5779--5788},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-09-20},
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Shyam et al-2019 Model-Based Active Exploration/Shyam et al_2019_Model-Based Active Exploration.pdf;/Users/scannea1/Zotero/storage/8Z8RVUMW/Shyam et al. - 2019 - Model-Based Active Exploration.pdf}
}

@article{Silverman1985,
  title = {Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting},
  author = {Silverman, B W},
  year = {1985},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {47},
  number = {1},
  pages = {1--21},
  doi = {10.1111/j.2517-6161.1985.tb01327.x},
  abstract = {Non-parametric regression using cubic splines is an attractive, flexible and widely-applicable approach to curve estimation. Although the basic idea was formulated many years ago, the method is not as widely known or adopted as perhaps it should be. The topics and examples discussed in this paper are intended to promote the understanding and extend the practicability of the spline smoothing methodology. Particular subjects covered include the basic principles of the method; the relation with moving average and other smoothing methods; the automatic choice of the amount of smoothing; and the use of residuals for diagnostic checking and model adaptation. The question of providing inference regions for curves-and for relevant properties of curves{\textendash}is approached via a finite-dimensional Bayesian formulation.},
  mendeley-groups = {GP/Mixture of GPs},
  keywords = {mcycle,motorcycle},
  file = {/Users/scannea1/Zotero/storage/GVBR4H9A/Silverman - 1985 - Some Aspects of the Spline Smoothing Approach to Non-Parametric Regression Curve Fitting.pdf}
}

@inproceedings{singhReinforcementLearningSoft1994,
  title = {Reinforcement {{Learning}} with {{Soft State Aggregation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Singh, Satinder and Jaakkola, Tommi and Jordan, Michael},
  year = {1994},
  volume = {7},
  publisher = {{MIT Press}},
  urldate = {2023-11-02},
  abstract = {It is  widely  accepted  that the use  of more compact representations  than lookup tables is crucial to scaling reinforcement learning (RL)  algorithms to real-world problems.  Unfortunately almost all of the  theory  of reinforcement  learning assumes  lookup table representa(cid:173) tions.  In  this  paper  we  address  the  pressing  issue  of  combining  function  approximation and RL,  and present  1)  a function approx(cid:173) imator  based  on  a  simple extension  to  state  aggregation  (a  com(cid:173) monly  used  form  of  compact  representation),  namely  soft  state  aggregation,  2)  a  theory  of convergence for  RL  with arbitrary, but  fixed,  soft  state  aggregation,  3)  a  novel  intuitive  understanding of  the effect  of state aggregation on online RL, and 4)  a new heuristic  adaptive  state aggregation algorithm that finds  improved compact  representations  by  exploiting the  non-discrete  nature  of soft  state  aggregation.  Preliminary empirical results  are  also  presented.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Singh et al-1994 Reinforcement Learning with Soft State Aggregation/Singh et al_1994_Reinforcement Learning with Soft State Aggregation.pdf}
}

@inproceedings{snelsonSparse2005a,
  title = {Sparse {{Gaussian Processes}} Using {{Pseudo-inputs}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Snelson, Edward and Ghahramani, Zoubin},
  year = {2005},
  abstract = {We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M {$\ll$} N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime.},
  file = {/Users/scannea1/Zotero/storage/J54QN2B7/Snelson and Ghahramani - Sparse Gaussian Processes using Pseudo-inputs.pdf}
}

@inproceedings{sokarDormantNeuronPhenomenon2023,
  title = {The {{Dormant Neuron Phenomenon}} in {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Sokar, Ghada and Agarwal, Rishabh and Castro, Pablo Samuel and Evci, Utku},
  year = {2023},
  month = jul,
  pages = {32145--32168},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-18},
  abstract = {In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent's network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.},
  langid = {english},
  keywords = {\_tablet},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Sokar et al-2023 The Dormant Neuron Phenomenon in Deep Reinforcement Learning/Sokar et al_2023_The Dormant Neuron Phenomenon in Deep Reinforcement Learning.pdf}
}

@book{stengelStochastic1986,
  title = {Stochastic Optimal Control: Theory and Application},
  shorttitle = {Stochastic Optimal Control},
  author = {Stengel, Robert F.},
  year = {1986},
  publisher = {{John Wiley \& Sons, Inc.}},
  isbn = {978-0-471-86462-2}
}

@inproceedings{stookeDecouplingRepresentationLearning2021,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  year = {2021},
  month = jul,
  pages = {9870--9879},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-02},
  abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at {\textbackslash}url\{https://github.com/astooke/rlpyt/tree/master/rlpyt/ul\}.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Stooke et al-2021 Decoupling Representation Learning from Reinforcement Learning/Stooke et al_2021_Decoupling Representation Learning from Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/KPQ6I9BS/Stooke et al. - 2021 - Decoupling Representation Learning from Reinforcem.pdf}
}

@inproceedings{suhFightingUncertaintyGradients2023,
  title = {Fighting {{Uncertainty}} with {{Gradients}}: {{Offline Reinforcement Learning}} via {{Diffusion Score Matching}}},
  shorttitle = {Fighting {{Uncertainty}} with {{Gradients}}},
  booktitle = {7th {{Annual Conference}} on {{Robot Learning}}},
  author = {Suh, H. J. Terry and Chou, Glen and Dai, Hongkai and Yang, Lujie and Gupta, Abhishek and Tedrake, Russ},
  year = {2023},
  month = aug,
  urldate = {2023-11-27},
  abstract = {Gradient-based methods enable efficient search capabilities in high dimensions. However, in order to apply them effectively in offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL), we require a more careful consideration of how uncertainty estimation interplays with first-order methods that attempt to minimize them. We study smoothed distance to data as an uncertainty metric, and claim that it has two beneficial properties: (i) it allows gradient-based methods that attempt to minimize uncertainty to drive iterates to data as smoothing is annealed, and (ii) it facilitates analysis of model bias with Lipschitz constants. As distance to data can be expensive to compute online, we consider settings where we need amortize this computation. Instead of learning the distance however, we propose to learn its gradients directly as an oracle for first-order optimizers. We show these gradients can be efficiently learned with score-matching techniques by leveraging the equivalence between distance to data and data likelihood. Using this insight, we propose Score-Guided Planning (SGP), a planning algorithm for offline RL that utilizes score-matching to enable first-order planning in high-dimensional problems, where zeroth-order methods were unable to scale, and ensembles were unable to overcome local minima. Website: https://sites.google.com/view/score-guided-planning/home},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Suh et al-2023 Fighting Uncertainty with Gradients/Suh et al_2023_Fighting Uncertainty with Gradients.pdf}
}

@book{sutton2018reinforcement,
  title = {Reinforcement Learning, Second Edition: {{An}} Introduction},
  author = {Sutton, R.S. and Barto, A.G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  publisher = {{MIT Press}},
  isbn = {978-0-262-35270-3}
}

@inproceedings{tarbouriechProbabilisticInferenceReinforcement2023,
  title = {Probabilistic {{Inference}} in {{Reinforcement Learning Done Right}}},
  booktitle = {Sixteenth {{European Workshop}} on {{Reinforcement Learning}}},
  author = {Tarbouriech, Jean and Lattimore, Tor and O'Donoghue, Brendan},
  year = {2023},
  month = may,
  urldate = {2023-10-07},
  abstract = {A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Tarbouriech et al-2023 Probabilistic Inference in Reinforcement Learning Done Right/Tarbouriech et al_2023_Probabilistic Inference in Reinforcement Learning Done Right.pdf}
}

@inproceedings{tassaReceding2007,
  title = {Receding {{Horizon Differential Dynamic Programming}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Tassa, Yuval and Erez, Tom and Smart, Bill},
  year = {2007},
  volume = {20},
  urldate = {2021-06-23},
  file = {/Users/scannea1/Zotero/storage/Y7BD5423/Jang et al. - 1988 - An Optimization Network for Matrix Inversion.pdf;/Users/scannea1/Zotero/storage/FKB3ZUD6/c6bff625bdb0393992c9d4db0c6bbe45-Paper.html}
}

@inproceedings{tassaSynthesis2012,
  title = {Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  year = {2012},
  month = oct,
  pages = {4906--4913},
  issn = {2153-0866},
  doi = {10.1109/IROS.2012.6386025},
  abstract = {We present an online trajectory optimization method and software platform applicable to complex humanoid robots performing challenging tasks such as getting up from an arbitrary pose on the ground and recovering from large disturbances using dexterous acrobatic maneuvers. The resulting behaviors, illustrated in the attached video, are computed only 7 {\texttimes} slower than real time, on a standard PC. The video also shows results on the acrobot problem, planar swimming and one-legged hopping. These simpler problems can already be solved in real time, without pre-computing anything.},
  keywords = {Computational modeling,Heuristic algorithms,Mathematical model,Optimization,Real-time systems,Robots,Trajectory},
  file = {/Users/scannea1/Zotero/storage/R6QNBA2X/Tassa et al. - 2012 - Synthesis and stabilization of complex behaviors t.pdf;/Users/scannea1/Zotero/storage/W7EMBPV4/6386025.html}
}

@article{tassa2018deepmind,
	title={Deepmind control suite},
	author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
	journal={arXiv preprint arXiv:1801.00690},
	year={2018}
}

@misc{tensorflow2015-whitepaper,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015},
  note = {Software available from tensorflow.org}
}

@article{theodorouGeneralized2010,
  title = {A {{Generalized Path Integral Control Approach}} to {{Reinforcement Learning}}},
  author = {Theodorou, Evangelos and Buchli, Jonas and Schaal, Stefan},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {104},
  pages = {3137--3181},
  issn = {1533-7928},
  urldate = {2021-06-25},
  file = {/Users/scannea1/Zotero/storage/V9XNWFXI/Theodorou et al. - 2010 - A Generalized Path Integral Control Approach to Re.pdf;/Users/scannea1/Zotero/storage/P82WK5SN/theodorou10a.html}
}

@inproceedings{theodorouStochastic2010,
  title = {Stochastic {{Differential Dynamic Programming}}},
  booktitle = {Proceedings of the 2010 {{American Control Conference}}},
  author = {Theodorou, Evangelos and Tassa, Yuval and Todorov, Emo},
  year = {2010},
  month = jun,
  pages = {1125--1132},
  issn = {2378-5861},
  doi = {10.1109/ACC.2010.5530971},
  abstract = {Although there has been a significant amount of work in the area of stochastic optimal control theory towards the development of new algorithms, the problem of how to control a stochastic nonlinear system remains an open research topic. Recent iterative linear quadratic optimal control methods iLQG handle control and state multiplicative noise while they are derived based on first order approximation of dynamics. On the other hand, methods such as Differential Dynamic Programming expand the dynamics up to the second order but so far they can handle nonlinear systems with additive noise. In this work we present a generalization of the classic Differential Dynamic Programming algorithm. We assume the existence of state and control multiplicative process noise, and proceed to derive the second-order expansion of the cost-to-go. We find the correction terms that arise from the stochastic assumption. Despite having quartic and cubic terms in the initial expression, we show that these vanish, leaving us with the same quadratic structure as standard DDP.},
  keywords = {Control systems,Dynamic programming,Iterative algorithms,Nonlinear control systems,Nonlinear dynamical systems,Nonlinear systems,Optimal control,Stochastic processes,Stochastic resonance,Stochastic systems},
  file = {/Users/scannea1/Zotero/storage/V3X594IF/Theodorou et al. - 2010 - Stochastic Differential Dynamic Programming.pdf;/Users/scannea1/Zotero/storage/FVYIC7HX/5530971.html}
}

@inproceedings{thomasSafeReinforcementLearning2021,
  title = {Safe {{Reinforcement Learning}} by {{Imagining}} the {{Near Future}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Thomas, Garrett and Luo, Yuping and Ma, Tengyu},
  year = {2021},
  volume = {34},
  pages = {13859--13869},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-02-22},
  abstract = {Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufficiently accurate model can avoid unsafe states.We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Thomas et al-2021 Safe Reinforcement Learning by Imagining the Near Future/Thomas et al_2021_Safe Reinforcement Learning by Imagining the Near Future.pdf}
}

@inproceedings{titsiasBayesian2010,
  title = {Bayesian {{Gaussian Process Latent Variable Model}}},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis and Lawrence, Neil D.},
  year = {2010},
  month = mar,
  pages = {844--851},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2022-03-07},
  abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VU5NQZLV/Titsias and Lawrence - 2010 - Bayesian Gaussian Process Latent Variable Model.pdf}
}

@inproceedings{titsiasVariational2009,
  title = {Variational {{Learning}} of {{Inducing Variables}} in {{Sparse Gaussian Processes}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Titsias, Michalis},
  year = {2009},
  month = apr,
  pages = {567--574},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-02-02},
  abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximat...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/DLNKJ5XI/Titsias - 2009 - Variational Learning of Inducing Variables in Spar.pdf;/Users/scannea1/Zotero/storage/LBI9US9A/titsias09a.html}
}

@inproceedings{todorovGeneralized2005,
  title = {A Generalized Iterative {{LQG}} Method for Locally-Optimal Feedback Control of Constrained Nonlinear Stochastic Systems},
  booktitle = {Proceedings of the 2005, {{American Control Conference}}, 2005.},
  author = {Todorov, E. and Li, Weiwei},
  year = {2005},
  month = jun,
  pages = {300-306 vol. 1},
  issn = {2378-5861},
  doi = {10.1109/ACC.2005.1469949},
  abstract = {We present an iterative linear-quadratic-Gaussian method for locally-optimal feedback control of nonlinear stochastic systems subject to control constraints. Previously, similar methods have been restricted to deterministic unconstrained problems with quadratic costs. The new method constructs an affine feedback control law, obtained by minimizing a novel quadratic approximation to the optimal cost-to-go function. Global convergence is guaranteed through a Levenberg-Marquardt method; convergence in the vicinity of a local minimum is quadratic. Performance is illustrated on a limited-torque inverted pendulum problem, as well as a complex biomechanical control problem involving a stochastic model of the human arm, with 10 state dimensions and 6 muscle actuators. A Matlab implementation of the new algorithm is availabe at www.cogsci.ucsd.edu//spl sim/todorov.},
  keywords = {Control systems,Convergence,Costs,Feedback control,Iterative methods,Linear feedback control systems,Mathematical model,Nonlinear control systems,Stochastic processes,Stochastic systems},
  file = {/Users/scannea1/Zotero/storage/5YFS2PIJ/Todorov and Li - 2005 - A generalized iterative LQG method for locally-opt.pdf;/Users/scannea1/Zotero/storage/XVSZ5EIE/1469949.html}
}

@inproceedings{tosiMetrics2014,
  title = {Metrics for {{Probabilistic Geometries}}},
  booktitle = {Proceedings of the 30th {{Conference}}},
  author = {Tosi, Alessandra and Hauberg, S{\o}ren and Vellido, Alfredo and Lawrence, Neil D},
  year = {2014},
  pages = {800--808},
  abstract = {We investigate the geometrical structure of probabilistic generative dimensionality reduction models using the tools of Riemannian geometry. We explicitly define a distribution over the natural metric given by the models. We provide the necessary algorithms to compute expected metric tensors where the distribution over mappings is given by a Gaussian process. We treat the corresponding latent variable model as a Riemannian manifold and we use the expectation of the metric under the Gaussian process prior to define interpolating paths and measure distance between latent points. We show how distances that respect the expected metric lead to more appropriate generation of new data.},
  langid = {english},
  keywords = {gaussian-processes,geometric-learning,gplvm},
  file = {/Users/scannea1/Zotero/storage/U65536P3/Tosi et al. - Metrics for Probabilistic Geometries.pdf}
}

@inproceedings{toussaintProbabilistic2006,
  title = {Probabilistic Inference for Solving Discrete and Continuous State {{Markov Decision Processes}}},
  booktitle = {Proceedings of the 23rd {{International Conference}} on {{Machine Learning}}},
  author = {Toussaint, Marc and Storkey, Amos},
  year = {2006},
  month = jan,
  volume = {2006},
  pages = {945--952},
  doi = {10.1145/1143844.1143963},
  abstract = {Inference in Markov Decision Processes has recently received interest as a means to in- fer goals of an observed action, policy recog- nition, and also as a tool to compute poli- cies. A particularly interesting aspect of the approach is that any existing inference tech- nique in DBNs now becomes available for an- swering behavioral questions-including those on continuous, factorial, or hierarchical state representations. Here we present an Expecta- tion Maximization algorithm for computing optimal policies. Unlike previous approaches we can show that this actually optimizes the discounted expected future return for arbi- trary reward functions and without assuming an ad hoc finite total time. The algorithm is generic in that any inference technique can be utilized in the E-step. We demonstrate this for exact inference on a discrete maze and Gaussian belief state propagation in continu- ous stochastic optimal control problems.},
  file = {/Users/scannea1/Zotero/storage/X266VYD8/Toussaint and Storkey - 2006 - Probabilistic inference for solving discrete and c.pdf}
}

@inproceedings{toussaintRobot2009,
  title = {Robot {{Trajectory Optimization}} Using {{Approximate Inference}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Toussaint, Marc},
  year = {2009},
  abstract = {The general stochastic optimal control (SOC) problem in robotics scenarios is often too complex to be solved exactly and in near real time. A classical approximate solution is to first compute an optimal (deterministic) trajectory and then solve a local linear-quadratic-gaussian (LQG) perturbation model to handle the system stochasticity. We present a new algorithm for this approach which improves upon previous algorithms like iLQG. We consider a probabilistic model for which the maximum likelihood (ML) trajectory coincides with the optimal trajectory and which, in the LQG case, reproduces the classical SOC solution. The algorithm then utilizes approximate inference methods (similar to expectation propagation) that efficiently generalize to non-LQG systems. We demonstrate the algorithm on a simulated 39-DoF humanoid robot. 1.},
  file = {/Users/scannea1/Zotero/storage/9HG93BHX/Toussaint - Robot Trajectory Optimization using Approximate In.pdf;/Users/scannea1/Zotero/storage/G6S25WB6/summary.html}
}

@misc{tranAllYouNeed2022,
  title = {All {{You Need}} Is a {{Good Functional Prior}} for {{Bayesian Deep Learning}}},
  author = {Tran, Ba-Hien and Rossi, Simone and Milios, Dimitrios and Filippone, Maurizio},
  year = {2022},
  month = apr,
  number = {arXiv:2011.12829},
  eprint = {2011.12829},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-09-29},
  abstract = {The Bayesian treatment of neural networks dictates that a prior distribution is specified over their weight and bias parameters. This poses a challenge because modern neural networks are characterized by a large number of parameters, and the choice of these priors has an uncontrolled effect on the induced functional prior, which is the distribution of the functions obtained by sampling the parameters from their prior distribution. We argue that this is a hugely limiting aspect of Bayesian deep learning, and this work tackles this limitation in a practical and effective way. Our proposal is to reason in terms of functional priors, which are easier to elicit, and to "tune" the priors of neural network parameters in a way that they reflect such functional priors. Gaussian processes offer a rigorous framework to define prior distributions over functions, and we propose a novel and robust framework to match their prior with the functional prior of neural networks based on the minimization of their Wasserstein distance. We provide vast experimental evidence that coupling these priors with scalable Markov chain Monte Carlo sampling offers systematically large performance improvements over alternative choices of priors and state-of-the-art approximate Bayesian deep learning approaches. We consider this work a considerable step in the direction of making the long-standing challenge of carrying out a fully Bayesian treatment of neural networks, including convolutional neural networks, a concrete possibility.},
  archiveprefix = {arxiv},
  keywords = {\_tablet\_modified,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Tran et al-2022 All You Need is a Good Functional Prior for Bayesian Deep Learning/Tran et al_2022_All You Need is a Good Functional Prior for Bayesian Deep Learning.pdf;/Users/scannea1/Zotero/storage/NY426YGG/2011.html}
}

@inproceedings{trappDeep2020,
  title = {Deep {{Structured Mixtures}} of {{Gaussian Processes}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Trapp, Martin and Peharz, Robert and Pernkopf, Franz and Rasmussen, Carl Edward},
  year = {2020},
  month = jun,
  pages = {2251--2261},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-10-14},
  abstract = {Gaussian Processes (GPs) are powerful non-parametric Bayesian regression models that allow exact posterior inference, but exhibit high computational and memory costs. In order to improve scalability of GPs, approximate posterior inference is frequently employed, where a prominent class of approximation techniques is based on local GP experts. However, local-expert techniques proposed so far are either not well-principled, come with limited approximation guarantees, or lead to intractable models. In this paper, we introduce deep structured mixtures of GP experts, a stochastic process model which i) allows exact posterior inference, ii) has attractive computational and memory costs, and iii) when used as GP approximation, captures predictive uncertainties consistently better than previous expert-based approximations. In a variety of experiments, we show that deep structured mixtures have a low approximation error and often perform competitive or outperform prior work.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/BK6FBB7Z/Trapp et al. - 2020 - Deep Structured Mixtures of Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/EKQQQYDQ/Trapp et al. - 2020 - Deep Structured Mixtures of Gaussian Processes.pdf}
}

@article{trespBayesian2000a,
  title = {A {{Bayesian Committee Machine}}},
  author = {Tresp, Volker},
  year = {2000},
  month = nov,
  journal = {Neural Computation},
  volume = {12},
  number = {11},
  pages = {2719--2741},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976600300014908},
  urldate = {2021-10-26},
  abstract = {The Bayesian committee machine (BCM) is a novel approach to combining estimators which were trained on different data sets. Although the BCM can be applied to the combination of any kind of estimators the main foci are Gaussian process regression and related systems such as regularization networks and smoothing splines for which the degrees of freedom increase with the number of training data. Somewhat surprisingly, we find that the performance of the BCM improves if several test points are queried at the same time and is optimal if the number of test points is at least as large as the degrees of freedom of the estimator. The BCM also provides a new solution for online learning with potential applications to data mining. We apply the BCM to systems with fixed basis functions and discuss its relationship to Gaussian process regression. Finally, we also show how the ideas behind the BCM can be applied in a non-Bayesian setting to extend the input dependent combination of estimators.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/6B6B86E3/Tresp - 2000 - A Bayesian Committee Machine.pdf}
}

@inproceedings{trespMixtures2000a,
  title = {Mixtures of {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tresp, Volker},
  year = {2000},
  volume = {13},
  pages = {654--660},
  urldate = {2020-11-26},
  abstract = {We introduce the mixture of Gaussian processes (MGP) model which is useful for applications in which the optimal bandwidth of a map is input dependent. The MGP is derived from the mixture of experts model and can also be used for modeling general conditional probability densities. We discuss how Gaussian processes - in particular in form of Gaussian process classification, the support vector machine and the MGP model--can be used for quantifying the dependencies in graphical models.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/VD6CC6WK/Tresp - 2000 - Mixtures of Gaussian Processes.pdf;/Users/scannea1/Zotero/storage/4XNN58JI/9fdb62f932adf55af2c0e09e55861964-Abstract.html}
}

@inproceedings{turchettaSafe2016,
  title = {Safe {{Exploration}} in {{Finite Markov Decision Processes}} with {{Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-05-02},
  abstract = {In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.},
  file = {/Users/scannea1/Zotero/storage/WUKK7N6W/Turchetta et al. - 2016 - Safe Exploration in Finite Markov Decision Process.pdf}
}

@inproceedings{upadhyayProbVLMProbabilisticAdapter2023,
  title = {{{ProbVLM}}: {{Probabilistic Adapter}} for {{Frozen Vison-Language Models}}},
  shorttitle = {{{ProbVLM}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Upadhyay, Uddeshya and Karthik, Shyamgopal and Mancini, Massimiliano and Akata, Zeynep},
  year = {2023},
  pages = {1899--1910},
  urldate = {2023-10-04},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Upadhyay et al-2023 ProbVLM/Upadhyay et al_2023_ProbVLM.pdf;/Users/scannea1/Zotero/storage/PVCVCYMT/Upadhyay et al. - 2023 - ProbVLM Probabilistic Adapter for Frozen Vison-La.pdf}
}

@inproceedings{ustyuzhaninovCompositional2020,
  title = {Compositional Uncertainty in Deep {{Gaussian}} Processes},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Kaiser, Markus and Bodin, Erik and Campbell, Neill D. F. and Ek, Carl Henrik},
  year = {2020},
  eprint = {1909.07698},
  urldate = {2020-12-01},
  abstract = {Gaussian processes (GPs) are nonparametric priors over functions. Fitting a GP implies computing a posterior distribution of functions consistent with the observed data. Similarly, deep Gaussian processes (DGPs) should allow us to compute a posterior distribution of compositions of multiple functions giving rise to the observations. However, exact Bayesian inference is intractable for DGPs, motivating the use of various approximations. We show that the application of simplifying mean-field assumptions across the hierarchy leads to the layers of a DGP collapsing to near-deterministic transformations. We argue that such an inference scheme is suboptimal, not taking advantage of the potential of the model to discover the compositional structure in the data. To address this issue, we examine alternative variational inference schemes allowing for dependencies across different layers and discuss their advantages and limitations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,deep-gaussian-processes,gaussian-process,Statistics - Machine Learning,variational-inference},
  note = {Comment: 17 pages},
  file = {/Users/scannea1/Zotero/storage/6RMC67MW/Ustyuzhaninov et al. - 2020 - Compositional uncertainty in deep Gaussian process.pdf;/Users/scannea1/Zotero/storage/XHN32FUW/1909.html}
}

@inproceedings{ustyuzhaninovMonotonic2020,
  title = {Monotonic {{Gaussian Process Flow}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Ustyuzhaninov, Ivan and Kazlauskaite, Ieva and Ek, Carl Henrik and Campbell, Neill D. F.},
  year = {2020},
  volume = {23},
  eprint = {1905.12930},
  urldate = {2020-12-01},
  abstract = {We propose a new framework for imposing monotonicity constraints in a Bayesian nonparametric setting based on numerical solutions of stochastic differential equations. We derive a nonparametric model of monotonic functions that allows for interpretable priors and principled quantification of hierarchical uncertainty. We demonstrate the efficacy of the proposed model by providing competitive results to other probabilistic monotonic models on a number of benchmark functions. In addition, we consider the utility of a monotonic random process as a part of a hierarchical probabilistic model; we examine the task of temporal alignment of time-series data where it is beneficial to use a monotonic random process in order to preserve the uncertainty in the temporal warpings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Proceedings of the 23nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020 (14 pages)},
  file = {/Users/scannea1/Zotero/storage/YF8MS6ER/Ustyuzhaninov et al. - 2020 - Monotonic Gaussian Process Flow.pdf;/Users/scannea1/Zotero/storage/ZSC3RQJ7/1905.html}
}

@inproceedings{vandenoordNeuralDiscreteRepresentation2017a,
  title = {Neural {{Discrete Representation Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{van den Oord}, Aaron and Vinyals, Oriol and {kavukcuoglu}, koray},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-10-09},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of posterior collapse'' -{\textemdash} where the latents are ignored when they are paired with a powerful autoregressive decoder -{\textemdash} typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/van den Oord et al-2017 Neural Discrete Representation Learning/false}
}

@article{vanderwilkFramework2020,
  title = {A {{Framework}} for {{Interdomain}} and {{Multioutput Gaussian Processes}}},
  author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, S. T. and Artemev, Artem and Adam, Vincent and Hensman, James},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.01115 [cs, stat]},
  eprint = {2003.01115},
  primaryclass = {cs, stat},
  urldate = {2021-06-15},
  abstract = {One obstacle to the use of Gaussian processes (GPs) in large-scale problems, and as a component in deep learning system, is the need for bespoke derivations and implementations for small variations in the model or inference. In order to improve the utility of GPs we need a modular system that allows rapid implementation and testing, as seen in the neural network community. We present a mathematical and software framework for scalable approximate inference in GPs, which combines interdomain approximations and multiple outputs. Our framework, implemented in GPflow, provides a unified interface for many existing multioutput models, as well as more recent convolutional structures. This simplifies the creation of deep models with GPs, and we hope that this work will encourage more interest in this approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/C89VTA94/van der Wilk et al. - 2020 - A Framework for Interdomain and Multioutput Gaussi.pdf;/Users/scannea1/Zotero/storage/EMGDFEFE/2003.html}
}

@inproceedings{vanhoofStableReinforcementLearning2016,
  title = {Stable Reinforcement Learning with Autoencoders for Tactile and Visual Data},
  booktitle = {2016 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {{van Hoof}, Herke and Chen, Nutan and Karl, Maximilian and {van der Smagt}, Patrick and Peters, Jan},
  year = {2016},
  month = oct,
  pages = {3928--3934},
  issn = {2153-0866},
  doi = {10.1109/IROS.2016.7759578},
  urldate = {2023-11-02},
  abstract = {For many tasks, tactile or visual feedback is helpful or even crucial. However, designing controllers that take such high-dimensional feedback into account is non-trivial. Therefore, robots should be able to learn tactile skills through trial and error by using reinforcement learning algorithms. The input domain for such tasks, however, might include strongly correlated or non-relevant dimensions, making it hard to specify a suitable metric on such domains. Auto-encoders specialize in finding compact representations, where defining such a metric is likely to be easier. Therefore, we propose a reinforcement learning algorithm that can learn non-linear policies in continuous state spaces, which leverages representations learned using auto-encoders. We first evaluate this method on a simulated toy-task with visual input. Then, we validate our approach on a real-robot tactile stabilization task.},
  file = {/Users/scannea1/Zotero/storage/ISL7KKXI/7759578.html}
}

@article{vasudevanGaussian2009a,
  ids = {vasudevanGaussian2009},
  title = {Gaussian Process Modeling of Large-Scale Terrain},
  author = {Vasudevan, Shrihari and Ramos, Fabio and Nettleton, Eric and {Durrant-Whyte}, Hugh},
  year = {2009},
  journal = {Journal of Field Robotics},
  volume = {26},
  number = {10},
  pages = {812--840},
  issn = {1556-4967},
  doi = {10.1002/rob.20309},
  urldate = {2021-10-26},
  abstract = {Building a model of large-scale terrain that can adequately handle uncertainty and incompleteness in a statistically sound way is a challenging problem. This work proposes the use of Gaussian processes as models of large-scale terrain. The proposed model naturally provides a multiresolution representation of space, incorporates and handles uncertainties aptly, and copes with incompleteness of sensory information. Gaussian process regression techniques are applied to estimate and interpolate (to fill gaps in occluded areas) elevation information across the field. The estimates obtained are the best linear unbiased estimates for the data under consideration. A single nonstationary (neural network) Gaussian process is shown to be powerful enough to model large and complex terrain, effectively handling issues relating to discontinuous data. A local approximation method based on a ``moving window'' methodology and implemented using k-dimensional (KD)-trees is also proposed. This enables the approach to handle extremely large data sets, thereby completely addressing its scalability issues. Experiments are performed on large-scale data sets taken from real mining applications. These data sets include sparse mine planning data, which are representative of a global positioning system{\textendash}based survey, as well as dense laser scanner data taken at different mine sites. Further, extensive statistical performance evaluation and benchmarking of the technique has been performed through cross-validation experiments. They conclude that for dense and/or flat data, the proposed approach will perform very competitively with grid-based approaches using standard interpolation techniques and triangulated irregular networks using triangle-based interpolation techniques; for sparse and/or complex data, however, it would significantly outperform them. {\textcopyright} 2009 Wiley Periodicals, Inc.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/95WPXWM3/Vasudevan et al. - 2009 - Gaussian Process Modeling of Large-Scale Terrain.pdf;/Users/scannea1/Zotero/storage/VXPH4VRG/download.html;/Users/scannea1/Zotero/storage/ZNZ2R2RM/rob.html}
}

@article{vinogradskaNumerical2020,
  title = {Numerical {{Quadrature}} for {{Probabilistic Policy Search}}},
  author = {Vinogradska, Julia and Bischoff, Bastian and Achterhold, Jan and Koller, Torsten and Peters, Jan},
  year = {2020},
  month = jan,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {42},
  number = {1},
  pages = {164--175},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2879335},
  abstract = {Learning control policies has become an appealing alternative to the derivation of control laws based on classic control theory. Model-based approaches have proven an outstanding data efficiency, especially when combined with probabilistic models to eliminate model bias. However, a major difficulty for these methods is that multi-step-ahead predictions typically become intractable for larger planning horizons and can only poorly be approximated. In this paper, we propose the use of numerical quadrature to overcome this drawback and provide significantly more accurate multi-step-ahead predictions. As a result, our approach increases data efficiency and enhances the quality of learned policies. Furthermore, policy learning is not restricted to optimizing locally around one trajectory, as numerical quadrature provides a principled approach to extend optimization to all trajectories starting in a specified starting state region. Thus, manual effort, such as choosing informative starting points for simultaneous policy optimization, is significantly decreased. Furthermore, learning is highly robust to the choice of initial policy and, thus, interaction time with the system is minimized. Empirical evaluations on simulated benchmark problems show the efficiency of the proposed approach and support our theoretical results.},
  keywords = {Computational modeling,control,Data models,Gaussian processes,Numerical models,Policy search,Predictive models,reinforcement learning,System dynamics,Uncertainty},
  file = {/Users/scannea1/Zotero/storage/32JH2IZA/Vinogradska et al. - 2020 - Numerical Quadrature for Probabilistic Policy Sear.pdf;/Users/scannea1/Zotero/storage/K82ZSX66/8520758.html}
}

@inproceedings{vinogradskaStability2016,
  title = {Stability of {{Controllers}} for {{Gaussian Process Forward Models}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Vinogradska, Julia and Bischoff, Bastian and {Nguyen-Tuong}, Duy and Romer, Anne and Schmidt, Henner and Peters, Jan},
  year = {2016},
  month = jun,
  pages = {545--554},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-06-18},
  abstract = {Learning control has become an appealing alternative to the derivation of control laws based on classic control theory. However, a major shortcoming of learning control is the lack of performance g...},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/ZXXMUJ9K/Vinogradska et al. - 2016 - Stability of Controllers for Gaussian Process Forw.pdf;/Users/scannea1/Zotero/storage/685IQFF8/vinogradska16.html}
}

@article{vonstrykDirect1992,
  title = {Direct and {{Indirect Methods}} for {{Trajectory Optimization}}},
  author = {Von Stryk, Oskar and Bulirsch, Roland},
  year = {1992},
  month = dec,
  journal = {Annals of Operations Research},
  volume = {37},
  pages = {357--373},
  doi = {10.1007/BF02071065},
  abstract = {This paper gives a brief list of commonly used direct and indirect efficient methods for the numerical solution of optimal control problems. To improve the low accuracy of the direct methods and to increase the convergence areas of the indirect methods we suggest a hybrid approach. For this a special direct collocation method is presented. In a hybrid approach this direct method can be used in combination with multiple shooting. Numerical examples illustrate the direct method and the hybrid approach.},
  file = {/Users/scannea1/Zotero/storage/D99SU5KA/Von Stryk and Bulirsch - 1992 - Direct and Indirect Methods for Trajectory Optimiz.pdf}
}

@article{wachiSafeExplorationOptimization2018,
  title = {Safe {{Exploration}} and {{Optimization}} of {{Constrained MDPs Using Gaussian Processes}}},
  author = {Wachi, Akifumi and Sui, Yanan and Yue, Yisong and Ono, Masahiro},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.12103},
  urldate = {2023-02-22},
  abstract = {We present a reinforcement learning approach to explore and optimize a safety-constrained Markov Decision Process(MDP). In this setting, the agent must maximize discounted cumulative reward while constraining the probability of entering unsafe states, defined using a safety function being within some tolerance. The safety values of all states are not known a priori, and we probabilistically model them via aGaussian Process (GP) prior. As such, properly behaving in such an environment requires balancing a three-way trade-off of exploring the safety function, exploring the reward function, and exploiting acquired knowledge to maximize reward. We propose a novel approach to balance this trade-off. Specifically, our approach explores unvisited states selectively; that is, it prioritizes the exploration of a state if visiting that state significantly improves the knowledge on the achievable cumulative reward. Our approach relies on a novel information gain criterion based on Gaussian Process representations of the reward and safety functions. We demonstrate the effectiveness of our approach on a range of experiments, including a simulation using the real Martian terrain data.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Gaussian Processes,Markov Decision Process},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Wachi et al-2018 Safe Exploration and Optimization of Constrained MDPs Using Gaussian Processes/Wachi et al_2018_Safe Exploration and Optimization of Constrained MDPs Using Gaussian Processes.pdf}
}

@inproceedings{wanEvaluating2022,
  title = {Towards {{Evaluating Adaptivity}} of {{Model-Based Reinforcement Learning Methods}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wan, Yi and {Rahimi-Kalahroudi}, Ali and Rajendran, Janarthanan and Momennejad, Ida and Chandar, Sarath and Seijen, Harm H. Van},
  year = {2022},
  month = jun,
  pages = {22536--22561},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {In recent years, a growing number of deep model-based reinforcement learning (RL) methods have been introduced. The interest in deep model-based RL is not surprising, given its many potential benefits, such as higher sample efficiency and the potential for fast adaption to changes in the environment. However, we demonstrate, using an improved version of the recently introduced Local Change Adaptation (LoCA) setup, that well-known model-based methods such as PlaNet and DreamerV2 perform poorly in their ability to adapt to local environmental changes. Combined with prior work that made a similar observation about the other popular model-based method, MuZero, a trend appears to emerge, suggesting that current deep model-based methods have serious limitations. We dive deeper into the causes of this poor performance, by identifying elements that hurt adaptive behavior and linking these to underlying techniques frequently used in deep model-based RL. We empirically validate these insights in the case of linear function approximation by demonstrating that a modified version of linear Dyna achieves effective adaptation to local changes. Furthermore, we provide detailed insights into the challenges of building an adaptive nonlinear model-based method, by experimenting with a nonlinear version of Dyna.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/MPAG7B3V/Wan et al. - 2022 - Towards Evaluating Adaptivity of Model-Based Reinf.pdf}
}

@inproceedings{wangDenoisedMDPsLearning2022,
  title = {Denoised {{MDPs}}: {{Learning World Models Better Than}} the {{World Itself}}},
  shorttitle = {Denoised {{MDPs}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Tongzhou and Du, Simon and Torralba, Antonio and Isola, Phillip and Zhang, Amy and Tian, Yuandong},
  year = {2022},
  month = jun,
  pages = {22591--22612},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-09-30},
  abstract = {The ability to separate signal from noise, and reason with clean abstractions, is critical to intelligence. With this ability, humans can efficiently perform real world tasks without considering all possible nuisance factors. How can artificial agents do the same? What kind of information can agents safely discard as noises? In this work, we categorize information out in the wild into four types based on controllability and relation with reward, and formulate useful information as that which is both controllable and reward-relevant. This framework clarifies the kinds information removed by various prior work on representation learning in reinforcement learning (RL), and leads to our proposed approach of learning a Denoised MDP that explicitly factors out certain noise distractors. Extensive experiments on variants of DeepMind Control Suite and RoboDesk demonstrate superior performance of our denoised world model over using raw observations alone, and over prior works, across policy optimization control tasks as well as the non-control task of joint position regression. Project Page: https://ssnl.github.io/denoised\_mdp/ Code: https://github.com/facebookresearch/denoised\_mdp/},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Wang et al-2022 Denoised MDPs/Wang et al_2022_Denoised MDPs.pdf}
}

@inproceedings{wangSafe2018,
  title = {Safe {{Learning}} of {{Quadrotor Dynamics Using Barrier Certificates}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Wang, Li and Theodorou, Evangelos A. and Egerstedt, Magnus},
  year = {2018},
  month = may,
  pages = {2460--2465},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2018.8460471},
  abstract = {To effectively control complex dynamical systems, accurate nonlinear models are typically needed. However, these models are not always known. In this paper, we present a data-driven approach based on Gaussian processes that learns models of quadrotors operating in partially unknown environments. What makes this challenging is that if the learning process is not carefully controlled, the system will go unstable, i.e., the quadcopter will crash. To this end, barrier certificates are employed for safe learning. The barrier certificates establish a non-conservative forward invariant safe region, in which high probability safety guarantees are provided based on the statistics of the Gaussian Process. A learning controller is designed to efficiently explore those uncertain states and expand the barrier certified safe region based on an adaptive sampling scheme. Simulation results are provided to demonstrate the effectiveness of the proposed approach.},
  keywords = {Adaptation models,Computational modeling,Control systems,Gaussian processes,Lyapunov methods,Safety,System dynamics},
  file = {/Users/scannea1/Zotero/storage/SA29II8X/Wang et al. - 2018 - Safe Learning of Quadrotor Dynamics Using Barrier .pdf;/Users/scannea1/Zotero/storage/UC3UH27R/8460471.html}
}

@inproceedings{watsonAdvancing2021,
  title = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}: {{Exploration}}, {{Covariance Control}} and {{Adaptive Risk}}},
  shorttitle = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}},
  booktitle = {American {{Control Conference}} ({{ACC}})},
  author = {Watson, Joe and Peters, Jan},
  year = {2021},
  eprint = {2103.06319},
  urldate = {2021-08-04},
  abstract = {Discrete-time stochastic optimal control remains a challenging problem for general, nonlinear systems under significant uncertainty, with practical solvers typically relying on the certainty equivalence assumption, replanning and/or extensive regularization. Control as inference is an approach that frames stochastic control as an equivalent inference problem, and has demonstrated desirable qualities over existing methods, namely in exploration and regularization. We look specifically at the input inference for control (i2c) algorithm, and derive three key characteristics that enable advanced trajectory optimization: An `expert' linear Gaussian controller that combines the benefits of open-loop optima and closed-loop variance reduction when optimizing for nonlinear systems, inherent adaptive risk sensitivity from the inference formulation, and covariance control functionality with only a minor algorithmic adjustment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  note = {Comment: American Control Conference (ACC) 2021},
  file = {/Users/scannea1/Zotero/storage/WLS2UV6J/Watson and Peters - 2021 - Advancing Trajectory Optimization with Approximate.pdf;/Users/scannea1/Zotero/storage/HBV63NMC/2103.html}
}

@inproceedings{watsonStochastic2020,
  title = {Stochastic {{Optimal Control}} as {{Approximate Input Inference}}},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Watson, Joe and Abdulsamad, Hany and Peters, Jan},
  year = {2020},
  month = may,
  pages = {697--716},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-08-03},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/AQK5B45X/Watson et al. - 2020 - Stochastic Optimal Control as Approximate Input In.pdf}
}

@article{watsonStochastic2021,
  title = {Stochastic {{Control}} through {{Approximate Bayesian Input Inference}}},
  author = {Watson, Joe and Abdulsamad, Hany and Findeisen, Rolf and Peters, Jan},
  year = {2021},
  month = may,
  journal = {arXiv:2105.07693 [cs, eess]},
  eprint = {2105.07693},
  primaryclass = {cs, eess},
  urldate = {2021-08-04},
  abstract = {Optimal control under uncertainty is a prevailing challenge in control, due to the difficulty in producing tractable solutions for the stochastic optimization problem. By framing the control problem as one of input estimation, advanced approximate inference techniques can be used to handle the statistical approximations in a principled and practical manner. Analyzing the Gaussian setting, we present a solver capable of several stochastic control methods, and was found to be superior to popular baselines on nonlinear simulated tasks. We draw connections that relate this inference formulation to previous approaches for stochastic optimal control, and outline several advantages that this inference view brings due to its statistical nature.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  note = {Comment: Submitted to Transactions on Automatic Control Special Issue: Learning and Control. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible},
  file = {/Users/scannea1/Zotero/storage/HSJRT7F9/Watson et al. - 2021 - Stochastic Control through Approximate Bayesian In.pdf;/Users/scannea1/Zotero/storage/ATILHJPE/2105.html}
}

@inproceedings{watterEmbedControlLocally2015,
  title = {Embed to {{Control}}: {{A Locally Linear Latent Dynamics Model}} for {{Control}} from {{Raw Images}}},
  shorttitle = {Embed to {{Control}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Watter, Manuel and Springenberg, Jost and Boedecker, Joschka and Riedmiller, Martin},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-02},
  abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Watter et al-2015 Embed to Control/Watter et al_2015_Embed to Control.pdf}
}

@inproceedings{weiDiffusionModelsMasked2023,
  title = {Diffusion {{Models}} as {{Masked Autoencoders}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Wei, Chen and Mangalam, Karttikeya and Huang, Po-Yao and Li, Yanghao and Fan, Haoqi and Xu, Hu and Wang, Huiyu and Xie, Cihang and Yuille, Alan and Feichtenhofer, Christoph},
  year = {2023},
  pages = {16284--16294},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/YA9PTJNP/Wei et al. - 2023 - Diffusion Models as Masked Autoencoders.pdf}
}

@inproceedings{wilkinsonSparse2021,
  title = {Sparse {{Algorithms}} for {{Markovian Gaussian Processes}}},
  booktitle = {{{AISTATS}}},
  author = {Wilkinson, William J. and Solin, A. and Adam, Vincent},
  year = {2021},
  abstract = {Approximate Bayesian inference methods that scale to very large datasets are crucial in leveraging probabilistic models for real-world time series. Sparse Markovian Gaussian processes combine the use of inducing variables with efficient Kalman filter-like recursions, resulting in algorithms whose computational and memory requirements scale linearly in the number of inducing points, whilst also enabling parallel parameter updates and stochastic optimisation. Under this paradigm, we derive a general site-based approach to approximate inference, whereby we approximate the non-Gaussian likelihood with local Gaussian terms, called sites. Our approach results in a suite of novel sparse extensions to algorithms from both the machine learning and signal processing literature, including variational inference, expectation propagation, and the classical nonlinear Kalman smoothers. The derived methods are suited to large time series, and we also demonstrate their applicability to spatio-temporal data, where the model has separate inducing points in both time and space.},
  file = {/Users/scannea1/Zotero/storage/VJTCCV38/Wilkinson et al. - 2021 - Sparse Algorithms for Markovian Gaussian Processes.pdf}
}

@misc{williamsAdvancing,
  title = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}: {{Exploration}}, {{Covariance Control}} and {{Adaptive Risk}}},
  shorttitle = {Advancing {{Trajectory Optimization}} with {{Approximate Inference}}},
  author = {Williams, Jon},
  journal = {Max Planck Institute for Intelligent Systems},
  urldate = {2021-08-04},
  abstract = {Our goal is to understand the principles of Perception, Action and Learning in autonomous systems that successfully interact with complex environments and to use this understanding to design future systems.},
  howpublished = {https://is.mpg.de},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/RT8WP95Y/watpet21.html}
}

@inproceedings{williamsInformation2017,
  ids = {williamsInformation2017a},
  title = {Information Theoretic {{MPC}} for Model-Based Reinforcement Learning},
  booktitle = {2017 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Williams, Grady and Wagener, Nolan and Goldfain, Brian and Drews, Paul and Rehg, James M. and Boots, Byron and Theodorou, Evangelos A.},
  year = {2017},
  month = may,
  pages = {1714--1721},
  doi = {10.1109/ICRA.2017.7989202},
  abstract = {We introduce an information theoretic model predictive control (MPC) algorithm capable of handling complex cost criteria and general nonlinear dynamics. The generality of the approach makes it possible to use multi-layer neural networks as dynamics models, which we incorporate into our MPC algorithm in order to solve model-based reinforcement learning tasks. We test the algorithm in simulation on a cart-pole swing up and quadrotor navigation task, as well as on actual hardware in an aggressive driving task. Empirical results demonstrate that the algorithm is capable of achieving a high level of performance and does so only utilizing data collected from the system.},
  keywords = {Cost function,Heuristic algorithms,Learning (artificial intelligence),Optimal control,Robots,Trajectory},
  file = {/Users/scannea1/Zotero/storage/4T95X4BV/Williams et al. - 2017 - Information theoretic MPC for model-based reinforc.pdf;/Users/scannea1/Zotero/storage/H3WL5XS4/Williams et al. - 2017 - Information theoretic MPC for model-based reinforc.pdf;/Users/scannea1/Zotero/storage/DGP8VC62/7989202.html;/Users/scannea1/Zotero/storage/GYW9MRB7/7989202.html}
}

@article{williamsInformationTheoretic2018,
  title = {Information-{{Theoretic Model Predictive Control}}: {{Theory}} and {{Applications}} to {{Autonomous Driving}}},
  shorttitle = {Information-{{Theoretic Model Predictive Control}}},
  author = {Williams, Grady and Drews, Paul and Goldfain, Brian and Rehg, James M. and Theodorou, Evangelos A.},
  year = {2018},
  month = dec,
  journal = {IEEE Transactions on Robotics},
  volume = {34},
  number = {6},
  pages = {1603--1622},
  issn = {1941-0468},
  doi = {10.1109/TRO.2018.2865891},
  abstract = {We present an information-theoretic approach to stochastic optimal control problems that can be used to derive general sampling-based optimization schemes. This new mathematical method is used to develop a sampling-based model predictive control algorithm. We apply this information-theoretic model predictive control scheme to the task of aggressive autonomous driving around a dirt test track, and compare its performance with a model predictive control version of the cross-entropy method.},
  keywords = {Autonomous vehicles,Monte Carlo methods,Monte-Carlo methods,nonlinear control systems,Nonlinear control systems,optimal control,Optimal control,parallel algorithms,Parallel processing,Stochastic processes},
  file = {/Users/scannea1/Zotero/storage/CNPYIW9A/Williams et al. - 2018 - Information-Theoretic Model Predictive Control Th.pdf;/Users/scannea1/Zotero/storage/HWUPYAJG/8558663.html}
}

@article{williamsModel2017,
  title = {Model {{Predictive Path Integral Control}}: {{From Theory}} to {{Parallel Computation}}},
  shorttitle = {Model {{Predictive Path Integral Control}}},
  author = {Williams, Grady and Aldrich, Andrew and Theodorou, Evangelos A.},
  year = {2017},
  journal = {Journal of Guidance, Control, and Dynamics},
  volume = {40},
  number = {2},
  pages = {344--357},
  publisher = {{American Institute of Aeronautics and Astronautics}},
  doi = {10.2514/1.G001921},
  urldate = {2021-06-24},
  abstract = {In this paper, a model predictive path integral control algorithm based on a generalized importance sampling scheme is developed and parallel optimization via sampling is performed using a graphics processing unit. The proposed generalized importance sampling scheme allows for changes in the drift and diffusion terms of stochastic diffusion processes and plays a significant role in the performance of the model predictive control algorithm. The proposed algorithm is compared in simulation with a model predictive control version of differential dynamic programming on nonlinear systems. Finally, the proposed algorithm is applied on multiple vehicles for the task of navigating through a cluttered environment. The current simulations illustrate the efficiency and robustness of the proposed approach and demonstrate the advantages of computational frameworks that incorporate concepts from statistical physics, control theory, and parallelization against more traditional approaches of optimal control theory.},
  file = {/Users/scannea1/Zotero/storage/UADARRDZ/Williams et al. - 2017 - Model Predictive Path Integral Control From Theor.pdf;/Users/scannea1/Zotero/storage/9C2RDKAE/1.html}
}

@misc{wilsonBayesian2022,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2022},
  month = mar,
  number = {arXiv:2002.08791},
  eprint = {2002.08791},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2022-07-22},
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 31 pages, 19 figures},
  file = {/Users/scannea1/Zotero/storage/L5NKD3KU/Wilson and Izmailov - 2022 - Bayesian Deep Learning and a Probabilistic Perspec.pdf;/Users/scannea1/Zotero/storage/CRZ3DE6F/2002.html}
}

@inproceedings{wilsonEfficiently2020,
  title = {Efficiently {{Sampling Functions}} from {{Gaussian Process Posteriors}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2020},
  volume = {37},
  eprint = {2002.09309},
  urldate = {2020-11-26},
  abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/scannea1/Zotero/storage/LDX7GNL2/Wilson et al. - 2020 - Efficiently Sampling Functions from Gaussian Proce.pdf;/Users/scannea1/Zotero/storage/WCQHCJV4/2002.html}
}

@article{wilsonPathwise2021,
  ids = {wilsonPathwise},
  title = {Pathwise {{Conditioning}} of {{Gaussian Processes}}},
  author = {Wilson, James T. and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  year = {2021},
  journal = {Journal of Machine Learning Research},
  volume = {22},
  number = {105},
  pages = {1--47},
  urldate = {2021-06-25},
  abstract = {As Gaussian processes are used to answer increasingly complex questions, analytic solutions become scarcer and scarcer. Monte Carlo methods act as a convenient bridge for connecting intractable mathematical expressions with actionable estimates via sampling. Conventional approaches for simulating Gaussian process posteriors view samples as draws from marginal distributions of process values at finite sets of input locations. This distribution-centric characterization leads to generative strategies that scale cubically in the size of the desired random vector. These methods are prohibitively expensive in cases where we would, ideally, like to draw high-dimensional vectors or even continuous sample paths. In this work, we investigate a different line of reasoning: rather than focusing on distributions, we articulate Gaussian conditionals at the level of random variables. We show how this pathwise interpretation of conditioning gives rise to a general family of approximations that lend themselves to efficiently sampling Gaussian process posteriors. Starting from first principles, we derive these methods and analyze the approximation errors they introduce. We, then, ground these results by exploring the practical implications of pathwise conditioning in various applied settings, such as global optimization and reinforcement learning.},
  file = {/Users/scannea1/Zotero/storage/EQYCWMQY/Wilson et al. - 2021 - Pathwise Conditioning of Gaussian Processes.pdf}
}

@article{Woodcock2009FormalMP,
  title = {Formal Methods: {{Practice}} and Experience},
  author = {Woodcock, Jim and Larsen, Peter Gorm and Bicarregui, Juan and Fitzgerald, John S.},
  year = {2009},
  journal = {Acm Computing Surveys},
  volume = {41},
  pages = {19:1-19:36}
}

@misc{xieLifelongRoboticReinforcement2022,
  title = {Lifelong {{Robotic Reinforcement Learning}} by {{Retaining Experiences}}},
  author = {Xie, Annie and Finn, Chelsea},
  year = {2022},
  month = apr,
  number = {arXiv:2109.09180},
  eprint = {2109.09180},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.09180},
  urldate = {2023-09-30},
  abstract = {Multi-task learning ideally allows robots to acquire a diverse repertoire of useful skills. However, many multi-task reinforcement learning efforts assume the robot can collect data from all tasks at all times. In reality, the tasks that the robot learns arrive sequentially, depending on the user and the robot's current environment. In this work, we study a practical sequential multi-task RL problem that is motivated by the practical constraints of physical robotic systems, and derive an approach that effectively leverages the data and policies learned for previous tasks to cumulatively grow the robot's skill-set. In a series of simulated robotic manipulation experiments, our approach requires less than half the samples than learning each task from scratch, while avoiding impractical round-robin data collection. On a Franka Emika Panda robot arm, our approach incrementally learns ten challenging tasks, including bottle capping and block insertion.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Supplementary website at https://sites.google.com/view/retain-experience/},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Xie_Finn-2022 Lifelong Robotic Reinforcement Learning by Retaining Experiences/Xie_Finn_2022_Lifelong Robotic Reinforcement Learning by Retaining Experiences.pdf;/Users/scannea1/Zotero/storage/URBEMJF7/2109.html}
}

@misc{xuDrMMasteringVisual2023,
  title = {{{DrM}}: {{Mastering Visual Reinforcement Learning}} through {{Dormant Ratio Minimization}}},
  shorttitle = {{{DrM}}},
  author = {Xu, Guowei and Zheng, Ruijie and Liang, Yongyuan and Wang, Xiyao and Yuan, Zhecheng and Ji, Tianying and Luo, Yu and Liu, Xiaoyu and Yuan, Jiaxin and Hua, Pu and Li, Shuzhen and Ze, Yanjie and Daum{\'e} III, Hal and Huang, Furong and Xu, Huazhe},
  year = {2023},
  month = oct,
  number = {arXiv:2310.19668},
  eprint = {2310.19668},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.19668},
  urldate = {2023-11-06},
  abstract = {Visual reinforcement learning (RL) has shown promise in continuous control tasks. Despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. In this paper, we identify a major shortcoming in existing visual RL methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. Expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. To quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the RL agent's network. Empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. Leveraging the aforementioned insights, we introduce DrM, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. Experiments demonstrate that DrM achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including DeepMind Control Suite, MetaWorld, and Adroit. Most importantly, DrM is the first model-free algorithm that consistently solves tasks in both the Dog and Manipulator domains from the DeepMind Control Suite as well as three dexterous hand manipulation tasks without demonstrations in Adroit, all based on pixel observations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Xu et al-2023 DrM/Xu et al_2023_DrM.pdf;/Users/scannea1/Zotero/storage/EDRXZJNF/2310.html}
}

@inproceedings{yangDiffusionModelRepresentation2023,
  title = {Diffusion {{Model}} as {{Representation Learner}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Yang, Xingyi and Wang, Xinchao},
  year = {2023},
  pages = {18938--18949},
  urldate = {2023-11-21},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/D9V8T7I5/Yang and Wang - 2023 - Diffusion Model as Representation Learner.pdf}
}

@misc{yangFoundationModelsDecision2023,
  title = {Foundation {{Models}} for {{Decision Making}}: {{Problems}}, {{Methods}}, and {{Opportunities}}},
  shorttitle = {Foundation {{Models}} for {{Decision Making}}},
  author = {Yang, Sherry and Nachum, Ofir and Du, Yilun and Wei, Jason and Abbeel, Pieter and Schuurmans, Dale},
  year = {2023},
  month = mar,
  journal = {arXiv.org},
  urldate = {2023-10-09},
  abstract = {Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.},
  howpublished = {https://arxiv.org/abs/2303.04129v1},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yang et al-2023 Foundation Models for Decision Making/Yang et al_2023_Foundation Models for Decision Making.pdf}
}

@inproceedings{yaratsImageAugmentationAll2020,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
  year = {2020},
  month = oct,
  urldate = {2023-11-03},
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to transform input examples, as well as regularizing the value function and policy. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Hafner et al., 2019; Lee et al., 2019; Hafner et al., 2018) methods and recently proposed contrastive learning (Srinivas et al., 2020). Our approach, which we dub DrQ: Data-regularized Q, can be combined with any model-free reinforcement learning algorithm. We further demonstrate this by applying it to DQN and significantly improve its data-efficiency on the Atari 100k benchmark.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yarats et al-2020 Image Augmentation Is All You Need/Yarats et al_2020_Image Augmentation Is All You Need.pdf}
}

@inproceedings{yaratsImprovingSampleEfficiency2021,
  title = {Improving {{Sample Efficiency}} in {{Model-Free Reinforcement Learning}} from {{Images}}},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Yarats, Denis and Zhang, Amy and Kostrikov, Ilya and Amos, Brandon and Pineau, Joelle and Fergus, Rob},
  year = {2021},
  month = may,
  volume = {35},
  pages = {10674--10681},
  doi = {10.1609/aaai.v35i12.17276},
  urldate = {2023-11-02},
  abstract = {Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance. Prior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning.   However, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and  identify variational autoencoders, used by previous investigations, as the cause of the divergence.    Following these findings, we propose effective techniques to improve training stability.  This results in a simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Reinforcement Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yarats et al-2021 Improving Sample Efficiency in Model-Free Reinforcement Learning from Images/Yarats et al_2021_Improving Sample Efficiency in Model-Free Reinforcement Learning from Images.pdf}
}

@inproceedings{yaratsMasteringVisualContinuous2021,
  title = {Mastering {{Visual Continuous Control}}: {{Improved Data-Augmented Reinforcement Learning}}},
  shorttitle = {Mastering {{Visual Continuous Control}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  year = {2021},
  month = oct,
  urldate = {2023-10-13},
  abstract = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2 's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yarats et al-2021 Mastering Visual Continuous Control/Yarats et al_2021_Mastering Visual Continuous Control.pdf}
}

@misc{yeMastering2021,
  title = {Mastering {{Atari Games}} with {{Limited Data}}},
  author = {Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  year = {2021},
  month = dec,
  number = {arXiv:2111.00210},
  eprint = {2111.00210},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2022-06-25},
  abstract = {Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3\% mean human performance and 109.0\% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  note = {Comment: Published at NeurIPS 2021; Homepage: https://yewr.github.io/projects/efficientzero/},
  file = {/Users/scannea1/Zotero/storage/VUXEVCAX/Ye et al. - 2021 - Mastering Atari Games with Limited Data.pdf;/Users/scannea1/Zotero/storage/H8KF25TH/2111.html}
}

@inproceedings{yildizContinuoustime2021,
  title = {Continuous-Time {{Model-based Reinforcement Learning}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Yildiz, Cagatay and Heinonen, Markus and L{\"a}hdesm{\"a}ki, Harri},
  year = {2021},
  month = jul,
  pages = {12009--12018},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-14},
  abstract = {Model-based reinforcement learning (MBRL) approaches rely on discrete-time state transition models whereas physical systems and the vast majority of control tasks operate in continuous-time. To avoid time-discretization approximation of the underlying process, we propose a continuous-time MBRL framework based on a novel actor-critic method. Our approach also infers the unknown state evolution differentials with Bayesian neural ordinary differential equations (ODE) to account for epistemic uncertainty. We implement and test our method on a new ODE-RL suite that explicitly solves continuous-time control systems. Our experiments illustrate that the model is robust against irregular and noisy data, and can solve classic control problems in a sample-efficient manner.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/A3KC3JAW/Yildiz et al. - 2021 - Continuous-time Model-based Reinforcement Learning.pdf;/Users/scannea1/Zotero/storage/M9J5CRBP/Yildiz et al. - 2021 - Continuous-time Model-based Reinforcement Learning.pdf}
}

@inproceedings{Yong1999StochasticCH,
  title = {Stochastic Controls: {{Hamiltonian}} Systems and {{HJB}} Equations},
  author = {Yong, Jiongmin and Zhou, Xun Yu},
  year = {1999}
}

@article{yongStochastic1999,
  title = {Stochastic {{Controls}}: {{Hamiltonian Systems}} and {{HJB Equations}}},
  shorttitle = {Stochastic {{Controls}}},
  author = {Yong, J. and Zhou, X.},
  year = {1999},
  journal = {undefined},
  urldate = {2022-04-29},
  abstract = {1. Basic Stochastic Calculus.- 1. Probability.- 1.1. Probability spaces.- 1.2. Random variables.- 1.3. Conditional expectation.- 1.4. Convergence of probabilities.- 2. Stochastic Processes.- 2.1. General considerations.- 2.2. Brownian motions.- 3. Stopping Times.- 4. Martingales.- 5. Ito\&\#39;s Integral.- 5.1. Nondifferentiability of Brownian motion.- 5.2. Definition of Ito\&\#39;s integral and basic properties.- 5.3. Ito\&\#39;s formula.- 5.4. Martingale representation theorems.- 6. Stochastic Differential Equations.- 6.1. Strong solutions.- 6.2. Weak solutions.- 6.3. Linear SDEs.- 6.4. Other types of SDEs.- 2. Stochastic Optimal Control Problems.- 1. Introduction.- 2. Deterministic Cases Revisited.- 3. Examples of Stochastic Control Problems.- 3.1. Production planning.- 3.2. Investment vs. consumption.- 3.3. Reinsurance and dividend management.- 3.4. Technology diffusion.- 3.5. Queueing systems in heavy traffic.- 4. Formulations of Stochastic Optimal Control Problems.- 4.1. Strong formulation.- 4.2. Weak formulation.- 5. Existence of Optimal Controls.- 5.1. A deterministic result.- 5.2. Existence under strong formulation.- 5.3. Existence under weak formulation.- 6. Reachable Sets of Stochastic Control Systems.- 6.1. Nonconvexity of the reachable sets.- 6.2. Noncloseness of the reachable sets.- 7. Other Stochastic Control Models.- 7.1. Random duration.- 7.2. Optimal stopping.- 7.3. Singular and impulse controls.- 7.4. Risk-sensitive controls.- 7.5. Ergodic controls.- 7.6. Partially observable systems.- 8. Historical Remarks.- 3. Maximum Principle and Stochastic Hamiltonian Systems.- 1. Introduction.- 2. The Deterministic Case Revisited.- 3. Statement of the Stochastic Maximum Principle.- 3.1. Adjoint equations.- 3.2. The maximum principle and stochastic Hamiltonian systems.- 3.3. A worked-out example.- 4. A Proof of the Maximum Principle.- 4.1. A moment estimate.- 4.2. Taylor expansions.- 4.3. Duality analysis and completion of the proof.- 5. Sufficient Conditions of Optimality.- 6. Problems with State Constraints.- 6.1. Formulation of the problem and the maximum principle.- 6.2. Some preliminary lemmas.- 6.3. A proof of Theorem 6.1.- 7. Historical Remarks.- 4. Dynamic Programming and HJB Equations.- 1. Introduction.- 2. The Deterministic Case Revisited.- 3. The Stochastic Principle of Optimality and the HJB Equation.- 3.1. A stochastic framework for dynamic programming.- 3.2. Principle of optimality.- 3.3. The HJB equation.- 4. Other Properties of the Value Function.- 4.1. Continuous dependence on parameters.- 4.2. Semiconcavity.- 5. Viscosity Solutions.- 5.1. Definitions.- 5.2. Some properties.- 6. Uniqueness of Viscosity Solutions.- 6.1. A uniqueness theorem.- 6.2. Proofs of Lemmas 6.6 and 6.7.- 7. Historical Remarks.- 5. The Relationship Between the Maximum Principle and Dynamic Programming.- 1. Introduction.- 2. Classical Hamilton-Jacobi Theory.- 3. Relationship for Deterministic Systems.- 3.1. Adjoint variable and value function: Smooth case.- 3.2. Economic interpretation.- 3.3. Methods of characteristics and the Feynman-Kac formula.- 3.4. Adjoint variable and value function: Nonsmooth case.- 3.5. Verification theorems.- 4. Relationship for Stochastic Systems.- 4.1. Smooth case.- 4.2. Nonsmooth case: Differentials in the spatial variable.- 4.3. Nonsmooth case: Differentials in the time variable.- 5. Stochastic Verification Theorems.- 5.1. Smooth case.- 5.2. Nonsmooth case.- 6. Optimal Feedback Controls.- 7. Historical Remarks.- 6. Linear Quadratic Optimal Control Problems.- 1. Introduction.- 2. The Deterministic LQ Problems Revisited.- 2.1. Formulation.- 2.2. A minimization problem of a quadratic functional.- 2.3. A linear Hamiltonian system.- 2.4. The Riccati equation and feedback optimal control.- 3. Formulation of Stochastic LQ Problems.- 3.1. Statement of the problems.- 3.2. Examples.- 4. Finiteness and Solvability.- 5. A Necessary Condition and a Hamiltonian System.- 6. Stochastic Riccati Equations.- 7. Global Solvability of Stochastic Riccati Equations.- 7.1. Existence: The standard case.- 7.2. Existence: The case C = 0, S = 0, and Q, G ?0.- 7.3. Existence: The one-dimensional case.- 8. A Mean-variance Portfolio Selection Problem.- 9. Historical Remarks.- 7. Backward Stochastic Differential Equations.- 1. Introduction.- 2. Linear Backward Stochastic Differential Equations.- 3. Nonlinear Backward Stochastic Differential Equations.- 3.1. BSDEs in finite deterministic durations: Method of contraction mapping.- 3.2. BSDEs in random durations: Method of continuation.- 4. Feynman-Kac-Type Formulae.- 4.1. Representation via SDEs.- 4.2. Representation via BSDEs.- 5. Forward-Backward Stochastic Differential Equations.- 5.1. General formulation and nonsolvability.- 5.2. The four-step scheme, a heuristic derivation.- 5.3. Several solvable classes of FBSDEs.- 6. Option Pricing Problems.- 6.1. European call options and the Black--Scholes formula.- 6.2. Other options.- 7. Historical Remarks.- References.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/JC3ZG78J/f60223dee8815324a4b9df9d1825e7c33fc099f3.html}
}

@article{yuActive2021,
  title = {Active {{Learning}} in {{Gaussian Process State Space Model}}},
  author = {Yu, Hon Sum Alec and Yao, Dingling and Zimmer, Christoph and Toussaint, Marc and {Nguyen-Tuong}, Duy},
  year = {2021},
  month = jul,
  journal = {arXiv:2108.00819 [cs, stat]},
  eprint = {2108.00819},
  primaryclass = {cs, stat},
  urldate = {2021-08-06},
  abstract = {We investigate active learning in Gaussian Process state-space models (GPSSM). Our problem is to actively steer the system through latent states by determining its inputs such that the underlying dynamics can be optimally learned by a GPSSM. In order that the most informative inputs are selected, we employ mutual information as our active learning criterion. In particular, we present two approaches for the approximation of mutual information for the GPSSM given latent states. The proposed approaches are evaluated in several physical systems where we actively learn the underlying non-linear dynamics represented by the state-space model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2021},
  file = {/Users/scannea1/Zotero/storage/GQRXJZJA/Yu et al. - 2021 - Active Learning in Gaussian Process State Space Mo.pdf;/Users/scannea1/Zotero/storage/XS273K4X/2108.html}
}

@inproceedings{yuanPreTrainedImageEncoder2022,
  title = {Pre-{{Trained Image Encoder}} for {{Generalizable Visual Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yuan, Zhecheng and Xue, Zhengrong and Yuan, Bo and Wang, Xueqian and Wu, Yi and Gao, Yang and Xu, Huazhe},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {13022--13037},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yuan et al-2022 Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning/Yuan et al_2022_Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning.pdf}
}

@inproceedings{yuImplicit2019,
  title = {Implicit {{Posterior Variational Inference}} for {{Deep Gaussian Processes}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {YU, Haibin and Chen, Yizhou and Low, Bryan Kian Hsiang and Jaillet, Patrick and Dai, Zhongxiang},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-10-15},
  file = {/Users/scannea1/Zotero/storage/W7EQKCL5/YU et al. - 2019 - Implicit Posterior Variational Inference for Deep .pdf}
}

@article{yukselTwenty2012,
  title = {Twenty {{Years}} of {{Mixture}} of {{Experts}}},
  author = {Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  year = {2012},
  month = aug,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {23},
  number = {8},
  pages = {1177--1193},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2012.2200299},
  abstract = {In this paper, we provide a comprehensive survey of the mixture of experts (ME). We discuss the fundamental models for regression and classification and also their training with the expectation-maximization algorithm. We follow the discussion with improvements to the ME model and focus particularly on the mixtures of Gaussian process experts. We provide a review of the literature for other training methods, such as the alternative localized ME training, and cover the variational learning of ME in detail. In addition, we describe the model selection literature which encompasses finding the optimum number of experts, as well as the depth of the tree. We present the advances in ME in the classification area and present some issues concerning the classification model. We list the statistical properties of ME, discuss how the model has been modified over the years, compare ME to some popular algorithms, and list several applications. We conclude our survey with future directions and provide a list of publicly available datasets and a list of publicly available software that implement ME. Finally, we provide examples for regression and classification. We believe that the study described in this paper will provide quick access to the relevant literature for researchers and practitioners who would like to improve or use ME, and that it will stimulate further studies in ME.},
  keywords = {Applications,Bayesian,Bayesian methods,classification,comparison,Data models,Decision trees,Gaussian processes,Hidden Markov models,hierarchical mixture of experts (HME),mixture of Gaussian process experts,regression,Regression analysis,statistical properties,Support vector machines,survey,variational},
  file = {/Users/scannea1/Zotero/storage/DBIQM3DW/Yuksel et al. - 2012 - Twenty Years of Mixture of Experts.pdf}
}

@inproceedings{yuMaskbasedLatentReconstruction2022,
  title = {Mask-Based {{Latent Reconstruction}} for {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Tao and Zhang, Zhizheng and Lan, Cuiling and Lu, Yan and Chen, Zhibo},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {25117--25131},
  urldate = {2023-11-24},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/J5FH3M3Y/Yu et al. - 2022 - Mask-based Latent Reconstruction for Reinforcement.pdf}
}

@inproceedings{yuMOPOModelbasedOffline2020,
  title = {{{MOPO}}: {{Model-based Offline Policy Optimization}}},
  shorttitle = {{{MOPO}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  year = {2020},
  volume = {33},
  pages = {14129--14142},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-12},
  abstract = {Offline reinforcement learning (RL) refers to the problem of learning policies entirely from a batch of previously collected data. This problem setting is compelling, because it offers the promise of utilizing large, diverse, previously collected datasets to acquire policies without any costly or dangerous active exploration, but it is also exceptionally difficult, due to the distributional shift between the offline training data and the learned policy. While there has been significant progress in model-free offline RL, the most successful prior methods constrain the policy to the support of the data, precluding generalization to new states. In this paper, we observe that an existing model-based RL algorithm on its own already produces significant gains in the offline setting, as compared to model-free approaches, despite not being designed for this setting. However, although many standard model-based RL methods already estimate the uncertainty of their model, they do not by themselves provide a mechanism to avoid the issues associated with distributional shift in the offline setting. We therefore propose to modify existing model-based RL methods to address these issues by casting offline model-based RL into a penalized MDP framework. We theoretically show that, by using this penalized MDP, we are maximizing a lower bound of the return in the true MDP. Based on our theoretical results, we propose a new model-based offline RL algorithm that applies the variance of a Lipschitz-regularized model as a penalty to the reward function. We find that this algorithm outperforms both standard model-based RL methods and existing state-of-the-art model-free offline RL approaches on existing offline RL benchmarks, as well as two challenging continuous control tasks that require generalizing from data collected for a different task.},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Yu et al-2020 MOPO/Yu et al_2020_MOPO.pdf}
}

@inproceedings{zhangConservativeDualPolicy2022,
  title = {Conservative {{Dual Policy Optimization}} for {{Efficient Model-Based Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Shenao},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {25450--25463},
  urldate = {2023-09-30},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Zhang-2022 Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement/Zhang_2022_Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement.pdf}
}

@misc{zhangHowFinetuneModel2023,
  title = {How to {{Fine-tune}} the {{Model}}: {{Unified Model Shift}} and {{Model Bias Policy Optimization}}},
  shorttitle = {How to {{Fine-tune}} the {{Model}}},
  author = {Zhang, Hai and Yu, Hang and Zhao, Junqiao and Zhang, Di and Huang, Chang and Zhou, Hongtu and Zhang, Xiao and Ye, Chen},
  year = {2023},
  month = oct,
  number = {arXiv:2309.12671},
  eprint = {2309.12671},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2309.12671},
  urldate = {2023-10-26},
  abstract = {Designing and deriving effective model-based reinforcement learning (MBRL) algorithms with a performance improvement guarantee is challenging, mainly attributed to the high coupling between model learning and policy optimization. Many prior methods that rely on return discrepancy to guide model learning ignore the impacts of model shift, which can lead to performance deterioration due to excessive model updates. Other methods use performance difference bound to explicitly consider model shift. However, these methods rely on a fixed threshold to constrain model shift, resulting in a heavy dependence on the threshold and a lack of adaptability during the training process. In this paper, we theoretically derive an optimization objective that can unify model shift and model bias and then formulate a fine-tuning process. This process adaptively adjusts the model updates to get a performance improvement guarantee while avoiding model overfitting. Based on these, we develop a straightforward algorithm USB-PO (Unified model Shift and model Bias Policy Optimization). Empirical results show that USB-PO achieves state-of-the-art performance on several challenging benchmark tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Zhang et al-2023 How to Fine-tune the Model/Zhang et al_2023_How to Fine-tune the Model.pdf;/Users/scannea1/Zotero/storage/K56E7NDM/2309.html}
}

@inproceedings{zhangLearningInvariantRepresentations2020,
  title = {Learning {{Invariant Representations}} for {{Reinforcement Learning}} without {{Reconstruction}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Amy and McAllister, Rowan Thomas and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  year = {2020},
  month = oct,
  urldate = {2023-11-02},
  abstract = {We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Zhang et al-2020 Learning Invariant Representations for Reinforcement Learning without/Zhang et al_2020_Learning Invariant Representations for Reinforcement Learning without.pdf}
}

@inproceedings{zhangNoisy2018,
  title = {Noisy {{Natural Gradient}} as {{Variational Inference}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  year = {2018},
  month = jul,
  pages = {5852--5861},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-07-15},
  abstract = {Variational Bayesian neural nets combine the flexibility of deep learning with Bayesian uncertainty estimation. Unfortunately, there is a tradeoff between cheap but simple variational families (e.g.~fully factorized) or expensive and complicated inference procedures. We show that natural gradient ascent with adaptive weight noise implicitly fits a variational posterior to maximize the evidence lower bound (ELBO). This insight allows us to train full-covariance, fully factorized, or matrix-variate Gaussian variational posteriors using noisy versions of natural gradient, Adam, and K-FAC, respectively, making it possible to scale up to modern-size ConvNets. On standard regression benchmarks, our noisy K-FAC algorithm makes better predictions and matches Hamiltonian Monte Carlo's predictive variances better than existing methods. Its improved uncertainty estimates lead to more efficient exploration in active learning, and intrinsic motivation for reinforcement learning.},
  langid = {english},
  file = {/Users/scannea1/Zotero/storage/MX87JMRW/Zhang et al. - 2018 - Noisy Natural Gradient as Variational Inference.pdf;/Users/scannea1/Zotero/storage/XWEARQZV/Zhang et al. - 2018 - Noisy Natural Gradient as Variational Inference.pdf}
}

@inproceedings{zhangUnsupervisedRepresentationLearning2022,
  title = {Unsupervised {{Representation Learning}} from {{Pre-trained Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zijian and Zhao, Zhou and Lin, Zhijie},
  year = {2022},
  month = dec,
  volume = {35},
  pages = {22117--22130},
  urldate = {2023-11-27},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Zhang et al-2022 Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic/Zhang et al_2022_Unsupervised Representation Learning from Pre-trained Diffusion Probabilistic.pdf}
}

@inproceedings{zhaoSimplifiedTemporalConsistency2023,
  title = {Simplified {{Temporal Consistency Reinforcement Learning}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Yi and Zhao, Wenshuai and Boney, Rinu and Kannala, Juho and Pajarinen, Joni},
  year = {2023},
  month = jul,
  pages = {42227--42246},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-10-10},
  abstract = {Reinforcement learning (RL) is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. To improve sample efficiency, recent work focuses on model-based RL which interleaves model learning with planning. Recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. In this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance RL. This applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free RL. In experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1{\texttimes}{\texttimes}{\textbackslash}times faster to train compared to ensemble-based methods. With model-free RL without planning, especially on high-dimensional tasks, such as the Deepmind Control Suite Humanoid and Dog tasks, our approach outperforms model-free methods by a large margin and matches model-based methods' sample efficiency while training 2.4{\texttimes}{\texttimes}{\textbackslash}times faster.},
  langid = {english},
  file = {/Users/scannea1/Library/Mobile Documents/com~apple~CloudDocs/zotero/Zhao et al-2023 Simplified Temporal Consistency Reinforcement Learning/Zhao et al_2023_Simplified Temporal Consistency Reinforcement Learning.pdf}
}

@inproceedings{zhouVector2014,
  title = {Vector Field Following for Quadrotors Using Differential Flatness},
  booktitle = {2014 {{IEEE International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Zhou, Dingjiang and Schwager, Mac},
  year = {2014},
  month = may,
  pages = {6567--6572},
  issn = {1050-4729},
  doi = {10.1109/ICRA.2014.6907828},
  abstract = {This paper proposes a differential flatness-based method for maneuvering a quadrotor so that its position follows a specified velocity vector field. Existing planning and control algorithms often give a 2D or 3D velocity vector field to be followed by a robot. However, quadrotors have complex nonlinear dynamics that make vector field following difficult, especially in aggressive maneuvering regimes. This paper exploits the differential flatness property of a quadrotor's dynamics to control its position along a given vector field. Differential flatness allows for the analytical derivation of control inputs in order to control the 12D dynamical state of the quadrotor such that the 2D or 3D position of the quadrotor follows the flow specified by a given vector field. The method is derived mathematically, and demonstrated in numerical simulations and in experiments with a quadrotor robot for three different vector fields.},
  keywords = {Collision avoidance,Heuristic algorithms,Navigation,Robots,Spirals,Trajectory,Vectors},
  file = {/Users/scannea1/Zotero/storage/83EI6RHU/Zhou and Schwager - 2014 - Vector field following for quadrotors using differ.pdf;/Users/scannea1/Zotero/storage/YPFEFDI6/6907828.html}
}

@phdthesis{ziebartModeling2010,
  title = {Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy},
  author = {Ziebart, Brian D.},
  year = {2010},
  abstract = {Predicting human behavior from a small amount of training examples is a challenging machine learning problem. In this thesis, we introduce the principle of maximum causal entropy, a general technique for applying information theory to decision-theoretic, game-theoretic, and control settings where relevant information is sequentially revealed over time. This approach guarantees decision-theoretic performance by matching purposeful measures of behavior (Abbeel \& Ng, 2004), and/or enforces game-theoretic rationality constraints (Aumann, 1974), while otherwise being as uncertain as possible, which minimizes worst-case predictive log-loss (Grunwald \& Dawid, 2003).  We derive probabilistic models for decision, control, and multi-player game settings using this approach. We then develop corresponding algorithms for efficient inference that include relaxations of the Bellman equation (Bellman, 1957), and simple learning algorithms based on convex optimization. We apply the models and algorithms to a number of behavior prediction tasks. Specifically, we present empirical evaluations of the approach in the domains of vehicle route preference modeling using over 100,000 miles of collected taxi driving data, pedestrian motion modeling from weeks of indoor movement data, and robust prediction of game play in stochastic multi-player games.},
  file = {/Users/scannea1/Zotero/storage/IS248UI6/Ziebart - 2010 - Modeling purposeful adaptive behavior with the pri.pdf}
}

@inproceedings{igl2018deep,
  title={Deep variational reinforcement learning for POMDPs},
  author={Igl, Maximilian and Zintgraf, Luisa and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={2117--2126},
  year={2018},
  organization={PMLR}
}

@article{hafner2019dream,
  title={Dream to control: Learning behaviors by latent imagination},
  author={Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1912.01603},
  year={2019}
}

@inproceedings{finn2017deep,
  title={Deep visual foresight for planning robot motion},
  author={Finn, Chelsea and Levine, Sergey},
  booktitle={2017 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2786--2793},
  year={2017},
  organization={IEEE}
}

@article{ebert2018visual,
  title={Visual foresight: Model-based deep reinforcement learning for vision-based robotic control},
  author={Ebert, Frederik and Finn, Chelsea and Dasari, Sudeep and Xie, Annie and Lee, Alex and Levine, Sergey},
  journal={arXiv preprint arXiv:1812.00568},
  year={2018}
}

@article{kipf2019contrastive,
  title={Contrastive learning of structured world models},
  author={Kipf, Thomas and Van der Pol, Elise and Welling, Max},
  journal={arXiv preprint arXiv:1911.12247},
  year={2019}
}

@article{schwarzer2020data,
  title={Data-efficient reinforcement learning with self-predictive representations},
  author={Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R Devon and Courville, Aaron and Bachman, Philip},
  journal={arXiv preprint arXiv:2007.05929},
  year={2020}
}

@article{mcinroe2021learning,
  title={Learning temporally-consistent representations for data-efficient reinforcement learning},
  author={McInroe, Trevor and Sch{\"a}fer, Lukas and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2110.04935},
  year={2021}
}

@article{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{lutter2021learning,
  title={Learning dynamics models for model predictive agents},
  author={Lutter, Michael and Hasenclever, Leonard and Byravan, Arunkumar and Dulac-Arnold, Gabriel and Trochim, Piotr and Heess, Nicolas and Merel, Josh and Tassa, Yuval},
  journal={arXiv preprint arXiv:2109.14311},
  year={2021}
}

@article{janner2019trust,
  title={When to trust your model: Model-based policy optimization},
  author={Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
 }
  
  @article{clavera2020model,
  title={Model-augmented actor-critic: Backpropagating through paths},
  author={Clavera, Ignasi and Fu, Violet and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2005.08068},
  year={2020}
}

@inproceedings{silver2017predictron,
  title={The predictron: End-to-end learning and planning},
  author={Silver, David and Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and others},
  booktitle={International Conference on Machine Learning},
  pages={3191--3199},
  year={2017},
  organization={PMLR}
}

@article{oh2017value,
  title={Value prediction network},
  author={Oh, Junhyuk and Singh, Satinder and Lee, Honglak},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{tamar2016value,
  title={Value iteration networks},
  author={Tamar, Aviv and Wu, Yi and Thomas, Garrett and Levine, Sergey and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{ye2021mastering,
  title={Mastering atari games with limited data},
  author={Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25476--25488},
  year={2021}
}

@article{kostrikov2020image,
  title={Image augmentation is all you need: Regularizing deep reinforcement learning from pixels},
  author={Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  journal={arXiv preprint arXiv:2004.13649},
  year={2020}
}

@article{rubinstein1997optimization,
  title={Optimization of computer simulation models with rare events},
  author={Rubinstein, Reuven Y},
  journal={European Journal of Operational Research},
  volume={99},
  number={1},
  pages={89--112},
  year={1997},
  publisher={Elsevier}
}

@article{williams2015model,
  title={Model predictive path integral control using covariance variable importance sampling},
  author={Williams, Grady and Aldrich, Andrew and Theodorou, Evangelos},
  journal={arXiv preprint arXiv:1509.01149},
  year={2015}
}

@article{yarats2021mastering,
  title={Mastering visual continuous control: Improved data-augmented reinforcement learning},
  author={Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  journal={arXiv preprint arXiv:2107.09645},
  year={2021}
}

@article{lancaster2023modem,
  title={MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation},
  author={Lancaster, Patrick and Hansen, Nicklas and Rajeswaran, Aravind and Kumar, Vikash},
  journal={arXiv preprint arXiv:2309.14236},
  year={2023}
}

@article{yuan2022euclid,
  title={Euclid: Towards efficient unsupervised reinforcement learning with multi-choice dynamics model},
  author={Yuan, Yifu and Hao, Jianye and Ni, Fei and Mu, Yao and Zheng, Yan and Hu, Yujing and Liu, Jinyi and Chen, Yingfeng and Fan, Changjie},
  journal={arXiv preprint arXiv:2210.00498},
  year={2022}
}

@article{yang2023movie,
  title={Movie: Visual model-based policy adaptation for view generalization},
  author={Yang, Sizhe and Ze, Yanjie and Xu, Huazhe},
  journal={arXiv preprint arXiv:2307.00972},
  year={2023}
}

@article{feng2023finetuning,
  title={Finetuning offline world models in the real world},
  author={Feng, Yunhai and Hansen, Nicklas and Xiong, Ziyan and Rajagopalan, Chandramouli and Wang, Xiaolong},
  journal={arXiv preprint arXiv:2310.16029},
  year={2023}
}

@article{chitnis2023iql,
  title={IQL-TD-MPC: Implicit Q-Learning for Hierarchical Model Predictive Control},
  author={Chitnis, Rohan and Xu, Yingchen and Hashemi, Bobak and Lehnert, Lucas and Dogan, Urun and Zhu, Zheqing and Delalleau, Olivier},
  journal={arXiv preprint arXiv:2306.00867},
  year={2023}
}

@article{zhu2023repo,
  title={Repo: Resilient model-based reinforcement learning by regularizing posterior predictability},
  author={Zhu, Chuning and Simchowitz, Max and Gadipudi, Siri and Gupta, Abhishek},
  journal={arXiv preprint arXiv:2309.00082},
  year={2023}
}

@article{zhang2018natural,
  title={Natural environment benchmarks for reinforcement learning},
  author={Zhang, Amy and Wu, Yuxin and Pineau, Joelle},
  journal={arXiv preprint arXiv:1811.06032},
  year={2018}
}

@article{zintgraf2021varibad,
  title={Varibad: Variational bayes-adaptive deep rl via meta-learning},
  author={Zintgraf, Luisa and Schulze, Sebastian and Lu, Cong and Feng, Leo and Igl, Maximilian and Shiarlis, Kyriacos and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={13198--13236},
  year={2021},
  publisher={JMLRORG}
}

@article{subramanian2022approximate,
  title={Approximate information state for approximate planning and reinforcement learning in partially observed systems},
  author={Subramanian, Jayakumar and Sinha, Amit and Seraj, Raihan and Mahajan, Aditya},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={483--565},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{tang2023understanding,
  title={Understanding self-predictive learning for reinforcement learning},
  author={Tang, Yunhao and Guo, Zhaohan Daniel and Richemond, Pierre Harvey and Pires, Bernardo Avila and Chandak, Yash and Munos, R{\'e}mi and Rowland, Mark and Azar, Mohammad Gheshlaghi and Le Lan, Charline and Lyle, Clare and others},
  booktitle={International Conference on Machine Learning},
  pages={33632--33656},
  year={2023},
  organization={PMLR}
}

@article{morad2023popgym,
  title={POPGym: Benchmarking Partially Observable Reinforcement Learning},
  author={Morad, Steven and Kortvelesy, Ryan and Bettini, Matteo and Liwicki, Stephan and Prorok, Amanda},
  journal={arXiv preprint arXiv:2303.01859},
  year={2023}
}

@article{ghugare2022simplifying,
  title={Simplifying model-based rl: learning representations, latent-space models, and policies with one objective},
  author={Ghugare, Raj and Bharadhwaj, Homanga and Eysenbach, Benjamin and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:2209.08466},
  year={2022}
}

@article{tomar2021learning,
  title={Learning Representations for Pixel-based Control: What Matters and Why?},
  author={Tomar, Manan and Mishra, Utkarsh A and Zhang, Amy and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2111.07775},
  year={2021}
}